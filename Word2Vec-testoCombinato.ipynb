{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### carica dataset LDO 20-21 ############################ \n",
    "\n",
    "dataLDO2020 = pd.read_excel('/home/a.renda/to_move/LDO/filtrato_per_keyword/20-21_LDO_26K/LDO_20200101_20210101 pulito.ods', engine='odf')\n",
    "dataLDO2021=pd.read_excel('/home/a.renda/to_move/LDO/filtrato_per_keyword/20-21_LDO_26K/LDO_20210101_20220101 pulito.ods', engine='odf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra le righe che non contengono numeri (escludendo anche i NaN)\n",
    "dataLDO2021 = dataLDO2021[dataLDO2021['nosologico'].astype(str).str.contains(r'\\d')]\n",
    "#rimosse 212 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nColumns LDO 20-21:\", dataLDO2020.columns)\n",
    "print(\"\\nColumns LDO 21-22:\", dataLDO2021.columns)\n",
    "print(\"\\nShape LDO 20-21:\", dataLDO2020.shape)\n",
    "print(\"\\nShape LDO 21-22:\", dataLDO2021.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### carica database filtrato ################\n",
    "\n",
    "databaseFiltrato=pd.read_csv('/home/a.renda/to_move/LDO/labeled/20-21_341/DatabaseFiltrato.csv', sep=';')\n",
    "print(databaseFiltrato.shape)\n",
    "print(databaseFiltrato.columns) # la prima colonna è solo un contatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra le righe che non contengono numeri (escludendo anche i NaN)\n",
    "databaseFiltrato = databaseFiltrato[databaseFiltrato['nosologico'].astype(str).str.contains(r'\\d')]\n",
    "\n",
    "#tolte 5 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ filtra ldo 2020 ######################\n",
    "\n",
    "# Converti la colonna 'nosologico' del secondo dataset in int\n",
    "databaseFiltrato['nosologico'] = pd.to_numeric(databaseFiltrato['nosologico'], errors='coerce')\n",
    "\n",
    "# Trova i nosologici comuni\n",
    "comuni2020 = dataLDO2020['nosologico'].isin(databaseFiltrato['nosologico'])\n",
    "\n",
    "# Filtra il primo dataset\n",
    "dataset_filtrato2020 = dataLDO2020[comuni2020]\n",
    "print(dataset_filtrato2020.columns)\n",
    "print(dataset_filtrato2020.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ filtra ldo 2021 ######################\n",
    "\n",
    "# Trova i nosologici comuni\n",
    "comuni2021 = dataLDO2021['nosologico'].isin(databaseFiltrato['nosologico'])\n",
    "\n",
    "# Filtra il primo dataset\n",
    "dataset_filtrato2021 = dataLDO2021[comuni2021]\n",
    "print(dataset_filtrato2021.columns)\n",
    "print(dataset_filtrato2021.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## merge ldo2020 e ldo 2021 filtrati ###################################\n",
    "\n",
    "merged_dataset = pd.concat([dataset_filtrato2020, dataset_filtrato2021], ignore_index=True) # non ci sono duplicati tra i due dataset \n",
    "\n",
    "# Risultato finale\n",
    "print(\"\\nColumns merged dataset:\",merged_dataset.columns)\n",
    "print(\"\\nShape merged dataset:\",merged_dataset.shape)\n",
    "print(merged_dataset['testo'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## carica dataset con nosologici positivi #####################\n",
    "\n",
    "Positivi= pd.read_excel('/home/a.renda/to_move/LDO/labeled/20-21_341/NosologiciPositivi_341.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### aggiungere la colonna positivi #########################################\n",
    "\n",
    "# Convertire la colonna 'Nosologico' in numerico nel dataset NosologiciPositivi\n",
    "nosologici_positivi = pd.to_numeric(Positivi['NosologiciPositivi'], errors='coerce').dropna()\n",
    "\n",
    "# Creare la colonna 'positivi' nel DataFrame merged_dataset\n",
    "merged_dataset['positivi'] = merged_dataset['nosologico'].isin(nosologici_positivi).astype(int)\n",
    "\n",
    "\n",
    "# Contare quanti 1 e quanti 0 ci sono nella colonna 'positivi'\n",
    "count_positivi = merged_dataset['positivi'].value_counts()\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"\\nConteggio dei valori nella colonna 'positivi':\")\n",
    "print(f\"Numero di 1 (positivi): {count_positivi.get(1, 0)}\")\n",
    "print(f\"Numero di 0 (non positivi): {count_positivi.get(0, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unire parole e numeri in 'reparto' rimuovendo lo spazio e sostituendo con un trattino\n",
    "merged_dataset['reparto'] = merged_dataset['reparto'].str.replace(r'(\\w) (\\d)', r'\\1-\\2', regex=True)\n",
    "\n",
    "# Visualizza i risultati\n",
    "print(merged_dataset['reparto'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### pulisci il testo: risoluzione di errori di codifica, sostituzione caratteri speciali #######################\n",
    "import ftfy\n",
    "\n",
    "# Applica ftfy.fix_text() a tutte le colonne di testo nel dataset, gestendo i valori non testuali\n",
    "for col in merged_dataset.select_dtypes(include='object').columns:\n",
    "    merged_dataset[col] = merged_dataset[col].apply(lambda x: ftfy.fix_text(x) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Crea testo combinato con tutte le colonne di testo #####################################\n",
    "\n",
    "# Seleziona le colonne di testo specificate\n",
    "colonne_testo = ['testo', 'motivo_ricovero', 'anamnesi', 'esameobiettivo',\n",
    "                  'terapiafarmaingresso', 'decorso', 'laboratorio', 'interventi',\n",
    "                  'followup', 'terapie2', 'terapie3', 'esami', 'reparto']\n",
    "\n",
    "# Crea un testo combinato ignorando i NaN\n",
    "merged_dataset['testo_combinato'] = merged_dataset[colonne_testo].apply(\n",
    "    lambda row: ' '.join(\n",
    "        [str(row[col]) for col in colonne_testo if not pd.isna(row[col])]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Stampa le prime righe per vedere il risultato\n",
    "print(merged_dataset['testo_combinato'].head())\n",
    "print(merged_dataset['testo_combinato'][0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### MEDIA NUMERO DI PAROLE NELLE COLONNE ###########################\n",
    "\n",
    "# Funzione per calcolare il numero di parole\n",
    "def count_words(text):\n",
    "    if isinstance(text, str):  # Verifica se il testo è una stringa\n",
    "        return len(text.split())  # Conta le parole\n",
    "    elif isinstance(text, (int, float)):  # Se è un numero\n",
    "        return 1  # Considera il numero come una parola\n",
    "    return 0  # Restituisce 0 se non è una stringa o un numero\n",
    "\n",
    "\n",
    "# Calcola la lunghezza media in parole per ciascuna colonna di testo\n",
    "for colonna in colonne_testo:\n",
    "    word_counts = merged_dataset[colonna].apply(count_words)  # Conta le parole in ogni stringa\n",
    "    average_word_count = word_counts.mean()  # Calcola la lunghezza media\n",
    "    print(f\"Lunghezza media della colonna '{colonna}' in parole: {average_word_count:.2f}\")\n",
    "\n",
    "# Calcola la lunghezza media in parole della colonna di testo combinato\n",
    "word_counts_combined = merged_dataset['testo_combinato'].apply(count_words)  # Conta le parole in ogni stringa\n",
    "average_word_count_combined = word_counts_combined.mean()  # Calcola la lunghezza media\n",
    "print(f\"Lunghezza media della colonna 'testo_combinato' in parole: {average_word_count_combined:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# ISTOGRAMMA LUNGHEZZA TESTO COMBINATO  ############################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcola la lunghezza del testo combinato\n",
    "word_counts_combined = merged_dataset['testo_combinato'].apply(count_words)\n",
    "\n",
    "# Calcola i valori minimi e massimi\n",
    "lunghezza_min = word_counts_combined.min()\n",
    "lunghezza_max = word_counts_combined.max()\n",
    "\n",
    "# Definisce il numero di bin\n",
    "num_bin = 10\n",
    "\n",
    "# Calcola la larghezza delle bin\n",
    "bin_width = (lunghezza_max - lunghezza_min) / num_bin\n",
    "\n",
    "# Crea una lista di bin\n",
    "bins = [lunghezza_min + i * bin_width for i in range(num_bin + 1)]\n",
    "\n",
    "# Crea la figura più larga\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Calcola l'istogramma e i valori delle altezze (n) e dei bin\n",
    "n, bins, patches = plt.hist(\n",
    "    word_counts_combined,  # Usa i conteggi delle parole già calcolati\n",
    "    bins=bins,\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# Applica la scala logaritmica sull'asse y\n",
    "plt.yscale('log')\n",
    "\n",
    "# Calcola e aggiungi la linea della lunghezza media del testo combinato\n",
    "average_word_count_combined = word_counts_combined.mean()  # Questo ora è un valore scalare\n",
    "plt.axvline(x=average_word_count_combined, color='red', linestyle='--', label='Lunghezza media testo combinato')\n",
    "\n",
    "# Aggiunge il titolo e le etichette degli assi\n",
    "plt.title('Distribuzione della lunghezza dei testi su 10 intervalli di parole')\n",
    "plt.xlabel('Intervalli di parole')\n",
    "plt.ylabel('Numero di testi')\n",
    "\n",
    "# Mostra le etichette delle bin\n",
    "plt.xticks(bins)\n",
    "\n",
    "# Aggiunge una label con il valore sopra ogni barra\n",
    "for i in range(len(n)):\n",
    "    plt.text(bins[i] + bin_width / 2, n[i], str(int(n[i])), ha='center', va='bottom')\n",
    "\n",
    "# Mostra la legenda\n",
    "plt.legend()\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.show()\n",
    "\n",
    "# Stampa i valori minimi e massimi del testo combinato\n",
    "print(f\"Lunghezza massima testo_combinato (in parole): {lunghezza_max}\")\n",
    "print(f\"Lunghezza minima testo_combinato (in parole): {lunghezza_min}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# DIMENSIONE VOCABOLARIO ###########################\n",
    "\n",
    "# Calcola il vocabolario dalla colonna 'testo_combinato_preprocessed'\n",
    "vocab_set = set()\n",
    "for testo in merged_dataset['testo_combinato']:\n",
    "    if isinstance(testo, str):  # Assicurati che il testo non sia NaN\n",
    "        vocab_set.update(testo.split())\n",
    "\n",
    "# Calcola la dimensione del vocabolario\n",
    "vocabolario_dimensione = len(vocab_set)\n",
    "print(f\"Dimensione del vocabolario: {vocabolario_dimensione}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### PRE PROCESSING con parole composte ###########################\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Carica il modello SpaCy per l'italiano\n",
    "nlp = spacy.load(\"it_core_news_lg\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Funzione per pulire i token (inclusi numeri, punti e virgole)\n",
    "def clean_token(token):\n",
    "    \n",
    "    # Mantieni numeri, lettere, rimuovendo altri caratteri speciali\n",
    "    #cleaned_token = re.sub(r'[^a-zA-Z0-9àèéìòùÀÈÉÌÒÙ]', '', token)\n",
    "\n",
    "    # Sostituisce tutti i caratteri non alfanumerici (inclusi simboli, punteggiatura, ecc.) con uno spazio.\n",
    "    #cleaned_token = re.sub(r'[^a-zA-Z0-9àèéìòùÀÈÉÌÒÙ]', ' ', token) \n",
    "    #cleaned_token = re.sub(r'[^a-zA-Z0-9àèéìòùÀÈÉÌÒÙ,]', ' ', token) # questo per conservare i numeri con la virgola\n",
    "    \n",
    "    # Sostituisce tutti i caratteri non alfanumerici (inclusi simboli, punteggiatura, numeri, ecc.) con uno spazio.\n",
    "    cleaned_token = re.sub(r'[^a-zA-ZàèéìòùÀÈÉÌÒÙ]', ' ', token)\n",
    "    #cleaned_token = re.sub(r'\\s+', ' ', cleaned_token).strip()  # Normalizza gli spazi multipli\n",
    "    \n",
    "    return cleaned_token\n",
    "\n",
    "# Funzione per tokenizzare e preprocessare il testo\n",
    "def preprocess_text(row):\n",
    "    if not isinstance(row, str):  # Verifica che il dato sia una stringa valida\n",
    "        return \"\"  # Restituisce una stringa vuota se non valido\n",
    "    \n",
    "    # Rimuove date nel formato 'dd/mm/yyyy' e 'dd/mm/yy'\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', '', row)\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}', '', row)  # Rimuove numeri separati da /\n",
    "    \n",
    "    # Gestisci i punti tra le parole (ad esempio \"v.allegato\" diventa \"v allegato\")\n",
    "    #row = re.sub(r'(\\w)\\.(\\w)', r'\\1 \\2', row)  # Aggiunge uno spazio tra parole separate da punto\n",
    "\n",
    "    # Gestisci numeri decimali con virgola (esempio \"1,2\" non deve diventare \"12\")\n",
    "    #row = re.sub(r'(\\d),(\\d)', r'\\1,\\2', row)  # Mantieni la virgola nei numeri decimali\n",
    "    \n",
    "    # Aggiunge spazi tra numeri e lettere (es. \"800duloxetina\" -> \"800 duloxetina\")\n",
    "    row = re.sub(r'(\\d+)([a-zA-Z]+)', r'\\1 \\2', row)\n",
    "    row = re.sub(r'([a-zA-Z]+)(\\d+)', r'\\1 \\2', row)\n",
    "\n",
    "    # Tokenizza il testo con SpaCy\n",
    "    doc = nlp(row)\n",
    "    \n",
    "    # Filtra stopwords, punteggiatura e token senza embedding\n",
    "    tokens_puliti = [\n",
    "        clean_token(token.lemma_.lower())  # Lemmatizza e pulisce il token\n",
    "        for token in doc\n",
    "        if not token.is_punct and not token.is_stop #and token.has_vector  #(Solo token con vettori validi)\n",
    "    ]\n",
    "    \n",
    "    # Rimuove eventuali stringhe vuote dai token puliti\n",
    "    tokens_puliti = [token for token in tokens_puliti if token]\n",
    "    \n",
    "    # Ricombina i token in una stringa\n",
    "    return \" \".join(tokens_puliti)\n",
    "\n",
    "# Applica il preprocessing alla colonna del dataset\n",
    "merged_dataset['testo_combinato_preprocessed'] = merged_dataset['testo_combinato'].apply(preprocess_text)\n",
    "\n",
    "# Visualizza i risultati per le prime righe\n",
    "print(merged_dataset[['testo_combinato', 'testo_combinato_preprocessed']].head(10))\n",
    "\n",
    "# Controlla se ci sono valori vuoti nel preprocessing\n",
    "print(\"Numero di righe preprocessate vuote:\", merged_dataset['testo_combinato_preprocessed'].str.strip().eq(\"\").sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### PRE PROCESSING SEPARANDO PAROLE COMPOSTE ###########################\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Carica il modello SpaCy per l'italiano\n",
    "nlp = spacy.load(\"it_core_news_lg\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Funzione per pulire i token (inclusi numeri, punti e virgole)\n",
    "def clean_token(token):\n",
    "    \n",
    "    # Sostituisce tutti i caratteri non alfanumerici (inclusi simboli, punteggiatura, numeri, ecc.) con uno spazio.\n",
    "    cleaned_token = re.sub(r'[^a-zA-ZàèéìòùÀÈÉÌÒÙ]', ' ', token)\n",
    "    cleaned_token = re.sub(r'\\s+', ' ', cleaned_token).strip()  # Normalizza gli spazi multipli\n",
    "    \n",
    "    return cleaned_token\n",
    "\n",
    "# Funzione per tokenizzare e preprocessare il testo\n",
    "def preprocess_text(row):\n",
    "    if not isinstance(row, str):  # Verifica che il dato sia una stringa valida\n",
    "        return \"\"  # Restituisce una stringa vuota se non valido\n",
    "    \n",
    "    # Rimuove date nel formato 'dd/mm/yyyy' e 'dd/mm/yy'\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', '', row)\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}', '', row)  # Rimuove numeri separati da /\n",
    "       \n",
    "    # Aggiunge spazi tra numeri e lettere (es. \"800duloxetina\" -> \"800 duloxetina\")\n",
    "    row = re.sub(r'(\\d+)([a-zA-Z]+)', r'\\1 \\2', row)\n",
    "    row = re.sub(r'([a-zA-Z]+)(\\d+)', r'\\1 \\2', row)\n",
    "\n",
    "    # Aggiunge spazi tra parole composte tipo \"vediAllegato\" -> \"vedi Allegato\"\n",
    "    row = re.sub(r'([a-zàèéìòù])([A-ZÀÈÉÌÒÙ])', r'\\1 \\2', row)\n",
    "\n",
    "    # normalizza  spazi (se ci sono più spazi consecutivi, vengono ridotti a uno solo) e rimuove  spazi all'inizio e alla fine della stringa\n",
    "    row = re.sub(r'[^\\w\\s]', ' ', row)  # Rimuove caratteri non alfanumerici e parentesi\n",
    "\n",
    "    # Tokenizza il testo con SpaCy\n",
    "    doc = nlp(row)\n",
    "    \n",
    "    # Filtra stopwords, punteggiatura e token senza embedding\n",
    "    tokens_puliti = [\n",
    "        clean_token(token.lemma_.lower())  # lemmatizzazione e pulisce il token\n",
    "        for token in doc\n",
    "        if not token.is_punct and not token.is_stop and len(token.text) > 1 #and token.text.isalpha() #per rimuovere token formati da una singola lettera\n",
    "    ]\n",
    "    \n",
    "    # Applica un filtro finale per rimuovere manualmente le lettere singole\n",
    "    tokens_puliti = [token for token in tokens_puliti if len(token) > 1]  # Assicura che tutte le parole siano > 1 carattere\n",
    "    \n",
    "    # Ricombina i token in una stringa\n",
    "    return \" \".join(tokens_puliti)\n",
    "\n",
    "# Applica il preprocessing alla colonna del dataset\n",
    "merged_dataset['testo_combinato_preprocessed'] = merged_dataset['testo_combinato'].apply(preprocess_text)\n",
    "\n",
    "# Visualizza i risultati per le prime righe\n",
    "print(merged_dataset[['testo_combinato', 'testo_combinato_preprocessed']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### MEDIA NUMERO DI PAROLE TESTO PRE PROCESSATO ###########################\n",
    "\n",
    "# Funzione per calcolare il numero di parole\n",
    "def count_words(text):\n",
    "    if isinstance(text, str):  # Verifica se il testo è una stringa\n",
    "        return len(text.split())  # Conta le parole\n",
    "    elif isinstance(text, (int, float)):  # Se è un numero\n",
    "        return 1  # Considera il numero come una parola\n",
    "    return 0  # Restituisce 0 se non è una stringa o un numero\n",
    "\n",
    "# Calcola la lunghezza media in parole della colonna di testo combinato\n",
    "word_counts_combined = merged_dataset['testo_combinato_preprocessed'].apply(count_words)  # Conta le parole in ogni stringa\n",
    "average_word_count_combined = word_counts_combined.mean()  # Calcola la lunghezza media\n",
    "print(f\"Lunghezza media della colonna 'testo_combinato' preprocessato in parole: {average_word_count_combined:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ ISTOGRAMMA LUNGHEZZA TESTO COMBINATO PRE-PROCESSATO #############################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcola la lunghezza del testo combinato\n",
    "word_counts_combined = merged_dataset['testo_combinato_preprocessed'].apply(count_words)\n",
    "\n",
    "# Calcola i valori minimi e massimi\n",
    "lunghezza_min = word_counts_combined.min()\n",
    "lunghezza_max = word_counts_combined.max()\n",
    "\n",
    "# Definisce il numero di bin\n",
    "num_bin = 10\n",
    "\n",
    "# Calcola la larghezza delle bin\n",
    "bin_width = (lunghezza_max - lunghezza_min) / num_bin\n",
    "\n",
    "# Crea una lista di bin\n",
    "bins = [lunghezza_min + i * bin_width for i in range(num_bin + 1)]\n",
    "\n",
    "# Crea la figura più larga\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Calcola l'istogramma e i valori delle altezze (n) e dei bin\n",
    "n, bins, patches = plt.hist(\n",
    "    word_counts_combined,  # Usa i conteggi delle parole già calcolati\n",
    "    bins=bins,\n",
    "    edgecolor='black'\n",
    ")\n",
    "\n",
    "# Applica la scala logaritmica sull'asse y\n",
    "plt.yscale('log')\n",
    "\n",
    "# Calcola e aggiungi la linea della lunghezza media del testo combinato\n",
    "average_word_count_combined = word_counts_combined.mean()  # Questo ora è un valore scalare\n",
    "plt.axvline(x=average_word_count_combined, color='red', linestyle='--', label='Lunghezza media testo combinato preprocessato')\n",
    "\n",
    "# Aggiunge il titolo e le etichette degli assi\n",
    "plt.title('Distribuzione della lunghezza dei testi su 10 intervalli di parole')\n",
    "plt.xlabel('Intervalli di parole')\n",
    "plt.ylabel('Numero di testi')\n",
    "\n",
    "# Mostra le etichette delle bin\n",
    "plt.xticks(bins)\n",
    "\n",
    "# Aggiunge una label con il valore sopra ogni barra\n",
    "for i in range(len(n)):\n",
    "    plt.text(bins[i] + bin_width / 2, n[i], str(int(n[i])), ha='center', va='bottom')\n",
    "\n",
    "# Mostra la legenda\n",
    "plt.legend()\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.show()\n",
    "\n",
    "# Stampa i valori minimi e massimi del testo combinato\n",
    "print(f\"Lunghezza massima testo_combinato preprocessato (in parole): {lunghezza_max}\")\n",
    "print(f\"Lunghezza minima testo_combinato preprocessato (in parole): {lunghezza_min}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_dataset['testo_combinato'][2])\n",
    "print(merged_dataset['testo_combinato_preprocessed'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### DIMENSIONE VOCABOLARIO PRE-PROCESSATO ######################\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Calcola il vocabolario dalla colonna 'testo_combinato_preprocessed'\n",
    "vocab_set = set()\n",
    "for testo in merged_dataset['testo_combinato_preprocessed']:\n",
    "    if isinstance(testo, str):  # Assicurati che il testo non sia NaN\n",
    "        vocab_set.update(testo.split())\n",
    "\n",
    "# Calcola la dimensione del vocabolario\n",
    "vocabolario_dimensione = len(vocab_set)\n",
    "\n",
    "# Confronto tra dimensione del vocabolario e numero di features\n",
    "print(\"Dimensione del vocabolario:\", vocabolario_dimensione)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## CLASSIFIERS #################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Vettorizzazione\n",
    "#vectorizer = TfidfVectorizer(max_features=5000)\n",
    "#vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Definisci una lista di classificatori che vuoi provare\n",
    "classifiers = {\n",
    "    #'RandomForest': RandomForestClassifier(n_jobs=-1, max_depth=10, max_features=0.1, random_state=42),\n",
    "    #'LogisticRegression': LogisticRegression(random_state=42, max_iter=2000),\n",
    "    #'SVM': SVC(probability=True,random_state=42),\n",
    "    #'KNeighbors': KNeighborsClassifier(),\n",
    "    #'DecisionTree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    #'extremeGradientBoosting': XGBClassifier( learning_rate=0.1, random_state=42, n_jobs=-1, max_depth=6),\n",
    "    #'DecisionTree': DecisionTreeClassifier(max_depth=20, random_state=42),\n",
    "    #'GradientBoosting': GradientBoostingClassifier(learning_rate=0.1, min_samples_split=10, random_state=42),\n",
    "    #'LSTM': LSTMWrapper(embedding_layer=embedding_layer, hidden_dim=128, output_dim=1)  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)           # Per verificare la versione di PyTorch\n",
    "print(torch.version.cuda)          # Per confermare la versione di CUDA usata da PyTorch\n",
    "print(torch.cuda.is_available())   # Dovrebbe restituire True se PyTorch riconosce correttamente la GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy[cuda113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Assicurati che spaCy utilizzi la GPU\n",
    "spacy.require_gpu()\n",
    "\n",
    "print(\"spaCy sta utilizzando la GPU!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## CROSS VALIDATION ########################\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def eval_cross_validation(pipeline, X, y, skf):\n",
    "    list_reports = []\n",
    "    list_auc = []  # Lista per raccogliere i valori di AUC\n",
    "    list_num_features = []  # Lista per raccogliere il numero di features per ogni fold\n",
    "\n",
    "    # Metriche aggregate per ogni classe\n",
    "    precision_sum = {}\n",
    "    recall_sum = {}\n",
    "    f1_sum = {}\n",
    "    support_sum = {}  # Per il calcolo del supporto medio\n",
    "\n",
    "    # Liste per calcolare deviazioni standard\n",
    "    precision_values = {}\n",
    "    recall_values = {}\n",
    "    f1_values = {}\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Addestra il pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Previsioni e probabilità\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        y_pred_prob = pipeline.predict_proba(X_val)[:, 1]  # Probabilità della classe positiva\n",
    "\n",
    "        # Classification report per il fold\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        list_reports.append(report)\n",
    "\n",
    "        # AUC per il fold\n",
    "        auc = roc_auc_score(y_val, y_pred_prob)\n",
    "        list_auc.append(auc)\n",
    "\n",
    "        # Numero di features (fisso per embedding precalcolati)\n",
    "        num_features = X.shape[1]\n",
    "        list_num_features.append(num_features)\n",
    "\n",
    "        # Somma le metriche per ogni classe\n",
    "        for label, metrics in report.items():\n",
    "            if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "                precision_sum[label] = precision_sum.get(label, 0) + metrics[\"precision\"]\n",
    "                recall_sum[label] = recall_sum.get(label, 0) + metrics[\"recall\"]\n",
    "                f1_sum[label] = f1_sum.get(label, 0) + metrics[\"f1-score\"]\n",
    "                support_sum[label] = support_sum.get(label, 0) + metrics[\"support\"]\n",
    "\n",
    "                # Aggiungi i valori per la deviazione standard\n",
    "                precision_values[label] = precision_values.get(label, []) + [metrics[\"precision\"]]\n",
    "                recall_values[label] = recall_values.get(label, []) + [metrics[\"recall\"]]\n",
    "                f1_values[label] = f1_values.get(label, []) + [metrics[\"f1-score\"]]\n",
    "\n",
    "    # Calcola le medie delle metriche\n",
    "    num_folds = skf.get_n_splits()\n",
    "    precision_avg = {label: precision_sum[label] / num_folds for label in precision_sum}\n",
    "    recall_avg = {label: recall_sum[label] / num_folds for label in recall_sum}\n",
    "    f1_avg_per_class = {label: f1_sum[label] / num_folds for label in f1_sum}\n",
    "    support_avg = {label: support_sum[label] / num_folds for label in support_sum}\n",
    "\n",
    "    # Calcola le deviazioni standard\n",
    "    precision_std = {label: np.std(precision_values[label]) for label in precision_values}\n",
    "    recall_std = {label: np.std(recall_values[label]) for label in recall_values}\n",
    "    f1_std = {label: np.std(f1_values[label]) for label in f1_values}\n",
    "\n",
    "    # Media di AUC e numero di features\n",
    "    auc_avg = np.mean(list_auc)\n",
    "    num_features_avg = np.mean(list_num_features)\n",
    "\n",
    "    # Crea un DataFrame riassuntivo\n",
    "    df_avg = pd.DataFrame({\n",
    "        \"Precision\": precision_avg,\n",
    "        \"Recall\": recall_avg,\n",
    "        \"F1-Score\": f1_avg_per_class,\n",
    "        \"Support Avg\": support_avg,  # Supporto medio\n",
    "        \"Precision Std\": precision_std,\n",
    "        \"Recall Std\": recall_std,\n",
    "        \"F1-Score Std\": f1_std,\n",
    "    })\n",
    "\n",
    "    return df_avg, auc_avg, f1_avg_per_class, num_features_avg\n",
    "\n",
    "\n",
    "# Configura StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download it_core_news_lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### WORD2VEC con embedding precalcolati spacy (senza vettore di zeri + counter oov) #################\n",
    "\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Funzione per calcolare gli embedding e gestire parole valide/OOV uniche\n",
    "def compute_embeddings_parallel(texts, nosologici, nlp, batch_size=1000, n_process=-1, max_oov_display=500):\n",
    "    \"\"\"\n",
    "    Calcola gli embedding spaCy in parallelo e rileva le parole valide e OOV uniche.\n",
    "    - texts: lista di testi.\n",
    "    - nosologici: lista di identificativi associati ai testi.\n",
    "    - batch_size: numero di documenti per batch.\n",
    "    - n_process: (-1 per utilizzare tutti i core disponibili).\n",
    "    - max_oov_display: numero massimo di parole OOV da stampare.\n",
    "    \"\"\"\n",
    "    print(\"Calcolo degli embedding in corso...\")\n",
    "\n",
    "    # Inizializza gli insiemi e contatori\n",
    "    oov_words_counter = Counter()  # Contatore per parole OOV\n",
    "    valid_words_unique = set()  # Parole valide uniche\n",
    "    oov_per_doc = []  # Numero di parole OOV per documento\n",
    "    oov_details = []  # Dettagli OOV per documento (numero di OOV e testo originale, nosologico)\n",
    "\n",
    "    # Utilizzo del pipe per elaborare i testi\n",
    "    docs = nlp.pipe(texts, batch_size=batch_size, n_process=n_process)  # Elabora in batch\n",
    "    embeddings = []\n",
    "\n",
    "    for idx, doc in enumerate(docs):\n",
    "        doc_embedding = []\n",
    "        doc_oov = set()  # Parole OOV uniche per questo documento\n",
    "\n",
    "        for token in doc:\n",
    "            if not token.has_vector or np.all(token.vector == 0):  # Se il token non ha un embedding valido\n",
    "                if token.text.strip():  # Escludi stringhe vuote o costituite solo da spazi\n",
    "                    oov_words_counter[token.text] += 1  # Incrementa il conteggio per questa parola OOV\n",
    "                    doc_oov.add(token.text)  # Aggiungi la parola OOV al set del documento\n",
    "            else:\n",
    "                valid_words_unique.add(token.text)  # Parola valida\n",
    "                doc_embedding.append(token.vector)  # Aggiungi il vettore valido\n",
    "\n",
    "        # Calcola l'embedding medio per il documento (ignorando le parole OOV)\n",
    "        if doc_embedding:  # Se ci sono vettori validi\n",
    "            embeddings.append(np.mean(doc_embedding, axis=0))\n",
    "        else:  # Documento senza parole valide (tutte OOV)\n",
    "            embeddings.append(None)  # Oppure puoi aggiungere un vettore di fallback (es. np.zeros(nlp.vocab.vectors_length))\n",
    "\n",
    "        # Aggiungi il numero di OOV uniche per documento alla lista\n",
    "        oov_per_doc.append(len(doc_oov))\n",
    "\n",
    "        # Salva i dettagli OOV per questo documento\n",
    "        oov_details.append((len(doc_oov), texts[idx], nosologici[idx]))\n",
    "\n",
    "    # Calcola statistiche sulle OOV\n",
    "    total_unique_words = len(valid_words_unique.union(oov_words_counter.keys()))\n",
    "    oov_percentage_unique = (len(oov_words_counter.keys()) / total_unique_words) * 100\n",
    "\n",
    "    print(f\"Totale parole valide uniche: {len(valid_words_unique)}\")\n",
    "    print(f\"Totale parole OOV uniche: {len(oov_words_counter.keys())}\")\n",
    "    print(f\"Totale parole uniche nel dataset (OOV + valide): {total_unique_words}\")\n",
    "    print(f\"Percentuale di parole OOV rispetto al totale di parole uniche: {oov_percentage_unique:.2f}%\")\n",
    "\n",
    "    # Stampa le prime 50 parole OOV uniche ordinate per frequenza\n",
    "    print(\"\\nPrime 50 parole OOV uniche ordinate per frequenza:\")\n",
    "    for i, (word, count) in enumerate(oov_words_counter.most_common(max_oov_display)):\n",
    "        print(f\"{i+1}. {word}: {count}\")\n",
    "\n",
    "    # Ordinamento dei documenti in base al numero di OOV (decrescente)\n",
    "    sorted_oov_details = sorted(oov_details, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Stampa delle righe della colonna ordinate in base alle OOV\n",
    "    print(\"\\nPrime 10 righe con il maggior numero di OOV:\")\n",
    "    for i, (num_oov, text, nosologico) in enumerate(sorted_oov_details[:10]):  # Mostra i primi 10 documenti\n",
    "        print(f\"{i+1}. Nosologico: {nosologico} | Numero di OOV: {num_oov} | Testo: {text}\")\n",
    "\n",
    "    print(\"Calcolo degli embedding completato.\")\n",
    "\n",
    "    # Grafico della distribuzione del numero di parole OOV per documento\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(oov_per_doc, bins=50, color='lightblue', edgecolor='black')\n",
    "    plt.title('Distribuzione del numero di parole OOV uniche per documento')\n",
    "    plt.xlabel('Numero di parole OOV uniche per documento')\n",
    "    plt.ylabel('Numero di documenti')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return np.array(embeddings)  # Restituisce una matrice di embedding di forma (n_righe, 300) dove 300 è la dimensione di ogni embedding\n",
    "\n",
    "# Carica il dataset\n",
    "X_texts = merged_dataset['testo_combinato_preprocessed']\n",
    "y = merged_dataset['positivi']\n",
    "nosologici = merged_dataset['nosologico'].tolist()  # Lista degli identificativi (colonna 'nosologico')\n",
    "\n",
    "# Calcolo e salvataggio degli embedding\n",
    "X_embeddings = compute_embeddings_parallel(X_texts, nosologici, nlp)\n",
    "np.save('X_embeddings.npy', X_embeddings)  # Salva gli embedding su disco\n",
    "\n",
    "# Per ricaricare gli embedding in futuro\n",
    "# X_embeddings = np.load('X_embeddings.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### dimensione vocabolario modello spacy ###########################\n",
    "vocab_size = len(nlp.vocab)\n",
    "print(f\"Dimensione del vocabolario del modello spaCy: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################### RETI NEURALI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## CROSS VALIDATION ########################\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def eval_cross_validation(pipeline, X, y, skf):\n",
    "    list_reports = []\n",
    "    list_auc = []  # Lista per raccogliere i valori di AUC\n",
    "    list_num_features = []  # Lista per raccogliere il numero di features per ogni fold\n",
    "\n",
    "    # Metriche aggregate per ogni classe\n",
    "    precision_sum = {}\n",
    "    recall_sum = {}\n",
    "    f1_sum = {}\n",
    "    support_sum = {}  # Per il calcolo del supporto medio\n",
    "\n",
    "    # Liste per calcolare deviazioni standard\n",
    "    precision_values = {}\n",
    "    recall_values = {}\n",
    "    f1_values = {}\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Addestra il pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Previsioni e probabilità\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        y_pred_prob = pipeline.predict_proba(X_val)[:, 1]  # Probabilità della classe positiva\n",
    "\n",
    "        # Classification report per il fold\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        list_reports.append(report)\n",
    "\n",
    "        # AUC per il fold\n",
    "        auc = roc_auc_score(y_val, y_pred_prob)\n",
    "        list_auc.append(auc)\n",
    "\n",
    "        # Numero di features (fisso per embedding precalcolati)\n",
    "        num_features = X.shape[1]\n",
    "        list_num_features.append(num_features)\n",
    "\n",
    "        # Somma le metriche per ogni classe\n",
    "        for label, metrics in report.items():\n",
    "            if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "                precision_sum[label] = precision_sum.get(label, 0) + metrics[\"precision\"]\n",
    "                recall_sum[label] = recall_sum.get(label, 0) + metrics[\"recall\"]\n",
    "                f1_sum[label] = f1_sum.get(label, 0) + metrics[\"f1-score\"]\n",
    "                support_sum[label] = support_sum.get(label, 0) + metrics[\"support\"]\n",
    "\n",
    "                # Aggiungi i valori per la deviazione standard\n",
    "                precision_values[label] = precision_values.get(label, []) + [metrics[\"precision\"]]\n",
    "                recall_values[label] = recall_values.get(label, []) + [metrics[\"recall\"]]\n",
    "                f1_values[label] = f1_values.get(label, []) + [metrics[\"f1-score\"]]\n",
    "\n",
    "    # Calcola le medie delle metriche\n",
    "    num_folds = skf.get_n_splits()\n",
    "    precision_avg = {label: precision_sum[label] / num_folds for label in precision_sum}\n",
    "    recall_avg = {label: recall_sum[label] / num_folds for label in recall_sum}\n",
    "    f1_avg_per_class = {label: f1_sum[label] / num_folds for label in f1_sum}\n",
    "    support_avg = {label: support_sum[label] / num_folds for label in support_sum}\n",
    "\n",
    "    # Calcola le deviazioni standard\n",
    "    precision_std = {label: np.std(precision_values[label]) for label in precision_values}\n",
    "    recall_std = {label: np.std(recall_values[label]) for label in recall_values}\n",
    "    f1_std = {label: np.std(f1_values[label]) for label in f1_values}\n",
    "\n",
    "    # Media di AUC e numero di features\n",
    "    auc_avg = np.mean(list_auc)\n",
    "    num_features_avg = np.mean(list_num_features)\n",
    "\n",
    "    # Crea un DataFrame riassuntivo\n",
    "    df_avg = pd.DataFrame({\n",
    "        \"Precision\": precision_avg,\n",
    "        \"Recall\": recall_avg,\n",
    "        \"F1-Score\": f1_avg_per_class,\n",
    "        \"Support Avg\": support_avg,  # Supporto medio\n",
    "        \"Precision Std\": precision_std,\n",
    "        \"Recall Std\": recall_std,\n",
    "        \"F1-Score Std\": f1_std,\n",
    "    })\n",
    "\n",
    "    return df_avg, auc_avg, f1_avg_per_class, num_features_avg\n",
    "\n",
    "\n",
    "# Configura StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### convertire embedding spacy in un layer trainable #####################\n",
    "#Per rendere trainabili gli embedding generati da SpaCy:\n",
    "#1.Estrai gli embedding generati da SpaCy per ogni parola e crea una matrice di embedding.\n",
    "#2.Aggiungi uno spazio per i token OOV, inizializzati casualmente.\n",
    "#3.Utilizza un layer di embedding trainabile in PyTorch per gestire il backpropagation.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Dimensione degli embedding (dipende dal modello SpaCy, ad es. 300 per `it_core_news_md`)\n",
    "embedding_dim = 300\n",
    "\n",
    "# Costruzione della matrice di embedding\n",
    "vocab = {word: idx for idx, word in enumerate(nlp.vocab.strings)}\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "\n",
    "# Riempi la matrice con i vettori SpaCy\n",
    "for word, idx in vocab.items():\n",
    "    vector = nlp.vocab[word].vector\n",
    "    if vector.any():  # Solo se il vettore è disponibile\n",
    "        embedding_matrix[idx] = vector\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.uniform(0, 1, embedding_dim)  # Random per OOV\n",
    "\n",
    "# Converti in tensore PyTorch\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Layer di embedding trainabile\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # Non congelato\n",
    "\n",
    "\n",
    "#Trasformare il testo in sequenze di numeri: ogni parola viene sostituita con il suo indice nella matrice di embedding globale.\n",
    "#Preparare i dati per la rete neurale: le reti neurali non elaborano direttamente il testo; richiedono input numerici con una lunghezza uniforme (grazie al padding o troncamento).\n",
    "def tokenize_and_index_spacy(text, vocab, max_len):\n",
    "    tokens = [vocab.get(word, len(vocab) - 1) for word in text.split()]  # Usa l'indice dell'OOV per le parole non trovate\n",
    "    if len(tokens) < max_len:\n",
    "        tokens += [0] * (max_len - len(tokens))  # Padding\n",
    "    return tokens[:max_len]  # Trunca le sequenze troppo lunghe\n",
    "\n",
    "# Applicare la tokenizzazione e il padding ai testi\n",
    "max_len = 50  # Lunghezza massima delle sequenze\n",
    "X_indices = [tokenize_and_index_spacy(text, vocab, max_len) for text in X_texts]\n",
    "X_indices = torch.tensor(X_indices, dtype=torch.long)\n",
    "#y = torch.tensor(y.tolist(), dtype=torch.float32)\n",
    "# Se y è un tensore unidimensionale, cambiamo la sua forma a (26192, 1)\n",
    "#y = y.view(-1, 1)\n",
    "\n",
    "\n",
    "# Definiamo una rete neurale a valle del layer di embedding per il task di classificazione\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        _, (hidden, _) = self.lstm(embedded)  # hidden: [1, batch_size, hidden_dim]\n",
    "        output = self.fc(hidden.squeeze(0))  # [batch_size, output_dim]\n",
    "        return output\n",
    "\n",
    "#class CNNClassifier(nn.Module):\n",
    "    #def __init__(self, embedding_layer, num_filters, filter_sizes, output_dim):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embedding_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), output_dim)\n",
    "\n",
    "    #def forward(self, x):\n",
    "        embedded = self.embedding(x).unsqueeze(1)  # [batch_size, 1, seq_len, embedding_dim]\n",
    "        conved = [torch.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [torch.max(conv, dim=2)[0] for conv in conved]\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        output = self.fc(cat)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y))  # Dovrebbe essere un tensore PyTorch, tipo <class 'torch.Tensor'>\n",
    "print(y.shape)  # Dovrebbe avere la forma (batch_size, 1) per la classificazione binaria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class LSTMWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, embedding_layer, hidden_dim=128, output_dim=1, batch_size=64, epochs=5, lr=0.001):\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.model = LSTMClassifier(self.embedding_layer, self.hidden_dim, self.output_dim)  \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Creazione del modello LSTM\n",
    "        if self.model is None:  # Se il modello non è stato ancora creato\n",
    "            self.model = LSTMClassifier(self.embedding_layer, self.hidden_dim, self.output_dim)  # Crealo una sola volta\n",
    "        \n",
    "        # Ottimizzatore e loss function\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Convertiamo i dati di input in tensori\n",
    "        X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "        y_tensor = torch.tensor(y.tolist(), dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        # Creazione del DataLoader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        train_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # Addestramento\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Convertiamo i dati di input in tensori\n",
    "        X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "        \n",
    "        # Predizione\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            preds = torch.round(torch.sigmoid(outputs)).squeeze().numpy()\n",
    "        \n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### NO SMOTE ###################################\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Creazione del modello LSTM nella pipeline\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # Non congelato\n",
    "\n",
    "lstm_clf = LSTMWrapper(embedding_layer=embedding_layer, hidden_dim=128, output_dim=1, batch_size=64, epochs=5, lr=0.001)\n",
    "\n",
    "# Usa gli embedding di SpaCy\n",
    "X = X_indices\n",
    "y = torch.tensor(y.tolist(), dtype=torch.float32)\n",
    "\n",
    "#X = X.numpy()  # Tensore -> NumPy array\n",
    "#y = y.numpy()  # Tensore -> NumPy array\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "\n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    #pipeline = Pipeline([('classificazione', clf)])\n",
    "    pipeline = Pipeline([('classificazione', lstm_clf)])\n",
    "\n",
    "    # Usa gli embedding precalcolati\n",
    "    #X = X_embeddings\n",
    "  \n",
    "    # Fit della pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Calcola le metriche usando la funzione \n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### NO SMOTE #######################################\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "\n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline(('classificazione', clf))\n",
    "\n",
    "    # Usa gli embedding precalcolati\n",
    "    X = X_embeddings\n",
    "\n",
    "    # Fit della pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Calcola le metriche usando la funzione (modifica per adattare alla tua funzione eval_cross_validation)\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### SMOTE #######################################\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "\n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([('smote', SMOTE(random_state=42)),('classificazione', clf)])\n",
    "\n",
    "    # Usa gli embedding precalcolati\n",
    "    X = X_embeddings\n",
    "\n",
    "    # Fit della pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Calcola le metriche usando la funzione (modifica per adattare alla tua funzione eval_cross_validation)\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### SMOTE + UNDERSAMLING #######################################\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "\n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42, sampling_strategy=0.6)),  # Applica SMOTE fino al 60% della classe maggioritaria\n",
    "    ('undersample', RandomUnderSampler(sampling_strategy=1.0, random_state=42)),\n",
    "    ('classificazione', clf)])\n",
    "\n",
    "    # Usa gli embedding precalcolati\n",
    "    X = X_embeddings\n",
    "\n",
    "    # Fit della pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Calcola le metriche usando la funzione (modifica per adattare alla tua funzione eval_cross_validation)\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### convertire embedding spacy in un layer trainable #####################\n",
    "#Per rendere trainabili gli embedding generati da SpaCy:\n",
    "#1.Estrai gli embedding generati da SpaCy per ogni parola e crea una matrice di embedding.\n",
    "#2.Aggiungi uno spazio per i token OOV, inizializzati casualmente.\n",
    "#3.Utilizza un layer di embedding trainabile in PyTorch per gestire il backpropagation.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Dimensione degli embedding (dipende dal modello SpaCy, ad es. 300 per `it_core_news_md`)\n",
    "embedding_dim = 300\n",
    "\n",
    "# Costruzione della matrice di embedding\n",
    "vocab = {word: idx for idx, word in enumerate(nlp.vocab.strings)}\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "\n",
    "# Riempi la matrice con i vettori SpaCy\n",
    "for word, idx in vocab.items():\n",
    "    vector = nlp.vocab[word].vector\n",
    "    if vector.any():  # Solo se il vettore è disponibile\n",
    "        embedding_matrix[idx] = vector\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.uniform(0, 1, embedding_dim)  # Random per OOV\n",
    "\n",
    "# Converti in tensore PyTorch\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Layer di embedding trainabile\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # Non congelato\n",
    "\n",
    "\n",
    "#Trasformare il testo in sequenze di numeri: ogni parola viene sostituita con il suo indice nella matrice di embedding globale.\n",
    "#Preparare i dati per la rete neurale: le reti neurali non elaborano direttamente il testo; richiedono input numerici con una lunghezza uniforme (grazie al padding o troncamento).\n",
    "def tokenize_and_index_spacy(text, vocab, max_len):\n",
    "    tokens = [vocab.get(word, len(vocab) - 1) for word in text.split()]  # Usa l'indice dell'OOV per le parole non trovate\n",
    "    if len(tokens) < max_len:\n",
    "        tokens += [0] * (max_len - len(tokens))  # Padding\n",
    "    return tokens[:max_len]  # Trunca le sequenze troppo lunghe\n",
    "\n",
    "# Applicare la tokenizzazione e il padding ai testi\n",
    "max_len = 50  # Lunghezza massima delle sequenze\n",
    "X_indices = [tokenize_and_index_spacy(text, vocab, max_len) for text in X_texts]\n",
    "X_indices = torch.tensor(X_indices, dtype=torch.long)\n",
    "y = torch.tensor(y.tolist(), dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Definiamo una rete neurale a valle del layer di embedding per il task di classificazione\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_layer, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        _, (hidden, _) = self.lstm(embedded)  # hidden: [1, batch_size, hidden_dim]\n",
    "        output = self.fc(hidden.squeeze(0))  # [batch_size, output_dim]\n",
    "        return output\n",
    "\n",
    "#class CNNClassifier(nn.Module):\n",
    "    #def __init__(self, embedding_layer, num_filters, filter_sizes, output_dim):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embedding_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), output_dim)\n",
    "\n",
    "    #def forward(self, x):\n",
    "        embedded = self.embedding(x).unsqueeze(1)  # [batch_size, 1, seq_len, embedding_dim]\n",
    "        conved = [torch.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [torch.max(conv, dim=2)[0] for conv in conved]\n",
    "        cat = torch.cat(pooled, dim=1)\n",
    "        output = self.fc(cat)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Istanzia il modello\n",
    "hidden_dim = 128\n",
    "output_dim = 1  # Classificazione binaria\n",
    "model = LSTMClassifier(embedding_layer, hidden_dim, output_dim)\n",
    "\n",
    "# Ottimizzatore e funzione di perdita\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(X_indices).squeeze(1)\n",
    "    loss = criterion(predictions, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
