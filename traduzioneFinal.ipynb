{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### carica dataset LDO 20-21 ############################ \n",
    "\n",
    "dataLDO2020 = pd.read_excel('/home/a.renda/to_move/LDO/filtrato_per_keyword/20-21_LDO_26K/LDO_20200101_20210101 pulito.ods', engine='odf')\n",
    "dataLDO2021=pd.read_excel('/home/a.renda/to_move/LDO/filtrato_per_keyword/20-21_LDO_26K/LDO_20210101_20220101 pulito.ods', engine='odf')\n",
    "\n",
    "# Filtra le righe che non contengono numeri (escludendo anche i NaN)\n",
    "dataLDO2021 = dataLDO2021[dataLDO2021['nosologico'].astype(str).str.contains(r'\\d')]\n",
    "#rimosse 212 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### carica database filtrato ################\n",
    "\n",
    "databaseFiltrato=pd.read_csv('/home/a.renda/to_move/LDO/labeled/20-21_341/DatabaseFiltrato.csv', sep=';')\n",
    "print(databaseFiltrato.shape)\n",
    "print(databaseFiltrato.columns) # la prima colonna è solo un contatore\n",
    "\n",
    "# Filtra le righe che non contengono numeri (escludendo anche i NaN)\n",
    "databaseFiltrato = databaseFiltrato[databaseFiltrato['nosologico'].astype(str).str.contains(r'\\d')]\n",
    "\n",
    "#tolte 5 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ filtra ldo 2020 ######################\n",
    "\n",
    "# Converti la colonna 'nosologico' del secondo dataset in int\n",
    "databaseFiltrato['nosologico'] = pd.to_numeric(databaseFiltrato['nosologico'], errors='coerce')\n",
    "\n",
    "# Trova i nosologici comuni\n",
    "comuni2020 = dataLDO2020['nosologico'].isin(databaseFiltrato['nosologico'])\n",
    "\n",
    "# Filtra il primo dataset\n",
    "dataset_filtrato2020 = dataLDO2020[comuni2020]\n",
    "print(dataset_filtrato2020.columns)\n",
    "print(dataset_filtrato2020.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ filtra ldo 2021 ######################\n",
    "\n",
    "# Trova i nosologici comuni\n",
    "comuni2021 = dataLDO2021['nosologico'].isin(databaseFiltrato['nosologico'])\n",
    "\n",
    "# Filtra il primo dataset\n",
    "dataset_filtrato2021 = dataLDO2021[comuni2021]\n",
    "print(dataset_filtrato2021.columns)\n",
    "print(dataset_filtrato2021.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## merge ldo2020 e ldo 2021 filtrati ###################################\n",
    "\n",
    "merged_dataset = pd.concat([dataset_filtrato2020, dataset_filtrato2021], ignore_index=True) # non ci sono duplicati tra i due dataset \n",
    "\n",
    "# Risultato finale\n",
    "print(\"\\nColumns merged dataset:\",merged_dataset.columns)\n",
    "print(\"\\nShape merged dataset:\",merged_dataset.shape)\n",
    "print(merged_dataset['testo'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## carica dataset con nosologici positivi #####################\n",
    "\n",
    "Positivi= pd.read_excel('/home/a.renda/to_move/LDO/labeled/20-21_341/NosologiciPositivi_341.xlsx')\n",
    "\n",
    "################################### aggiungere la colonna positivi #########################################\n",
    "\n",
    "# Convertire la colonna 'Nosologico' in numerico nel dataset NosologiciPositivi\n",
    "nosologici_positivi = pd.to_numeric(Positivi['NosologiciPositivi'], errors='coerce').dropna()\n",
    "\n",
    "# Creare la colonna 'positivi' nel DataFrame merged_dataset\n",
    "merged_dataset['positivi'] = merged_dataset['nosologico'].isin(nosologici_positivi).astype(int)\n",
    "\n",
    "\n",
    "# Contare quanti 1 e quanti 0 ci sono nella colonna 'positivi'\n",
    "count_positivi = merged_dataset['positivi'].value_counts()\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"\\nConteggio dei valori nella colonna 'positivi':\")\n",
    "print(f\"Numero di 1 (positivi): {count_positivi.get(1, 0)}\")\n",
    "print(f\"Numero di 0 (non positivi): {count_positivi.get(0, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unire parole e numeri in 'reparto' rimuovendo lo spazio e sostituendo con un trattino\n",
    "merged_dataset['reparto'] = merged_dataset['reparto'].str.replace(r'(\\w) (\\d)', r'\\1-\\2', regex=True)\n",
    "\n",
    "# Visualizza i risultati\n",
    "print(merged_dataset['reparto'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### pulisci il testo: risoluzione di errori di codifica, sostituzione caratteri speciali #######################\n",
    "\n",
    "import ftfy\n",
    "\n",
    "# Applica ftfy.fix_text() a tutte le colonne di testo nel dataset, gestendo i valori non testuali\n",
    "for col in merged_dataset.select_dtypes(include='object').columns:\n",
    "    merged_dataset[col] = merged_dataset[col].apply(lambda x: ftfy.fix_text(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ traduzione ################################\n",
    "\n",
    "import spacy\n",
    "from transformers import MBart50Tokenizer, MBartForConditionalGeneration\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Inizializza spaCy e il modello linguistico italiano\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "# Configurazione modello e tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Parametri\n",
    "src_lang = \"it_IT\"\n",
    "tgt_lang = \"en_XX\"\n",
    "tokenizer.src_lang = src_lang\n",
    "max_length = 512  # Numero massimo di token per segmento\n",
    "\n",
    "# Funzione per contare i token\n",
    "def conta_token(text, tokenizer):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "# Funzione per segmentare il testo in frasi con spaCy\n",
    "def segmenta_testo(testo):\n",
    "    if not isinstance(testo, str):  # Gestisce i valori non stringa\n",
    "        return []\n",
    "    doc = nlp(testo)\n",
    "    frasi = [sent.text.strip() for sent in doc.sents]\n",
    "    return frasi\n",
    "\n",
    "# Funzione per creare segmenti che rispettano il limite massimo di token\n",
    "def crea_segmenti(frammenti, max_length, tokenizer):\n",
    "    segmenti = []\n",
    "    segmento_corrente = []\n",
    "    token_correnti = 0\n",
    "\n",
    "    for frase in frammenti:\n",
    "        # Conta i token della frase\n",
    "        num_token = conta_token(frase, tokenizer)\n",
    "\n",
    "        # Se aggiungere la frase supera il limite, salva il segmento corrente\n",
    "        if token_correnti + num_token > max_length:\n",
    "            segmenti.append(\" \".join(segmento_corrente))\n",
    "            segmento_corrente = [frase]\n",
    "            token_correnti = num_token\n",
    "        else:\n",
    "            segmento_corrente.append(frase)\n",
    "            token_correnti += num_token\n",
    "\n",
    "    # Aggiungi l'ultimo segmento\n",
    "    if segmento_corrente:\n",
    "        segmenti.append(\" \".join(segmento_corrente))\n",
    "\n",
    "    return segmenti\n",
    "\n",
    "# Funzione per tradurre un segmento\n",
    "def traduci_segmento(segment_text, tokenizer, model, src_lang, tgt_lang, device):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    encoded_input = tokenizer(segment_text, return_tensors=\"pt\", truncation=True, max_length=max_length).to(device)\n",
    "    translated_tokens = model.generate(\n",
    "        **encoded_input,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang],\n",
    "        max_length=max_length\n",
    "    )\n",
    "    return tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Funzione per tradurre un testo lungo\n",
    "def traduci_testo_corretto(testo, max_length, tokenizer, model, src_lang, tgt_lang, device):\n",
    "    # Segmenta il testo in frasi usando spaCy\n",
    "    frasi = segmenta_testo(testo)\n",
    "\n",
    "    # Crea segmenti rispettando il limite massimo di token\n",
    "    segmenti = crea_segmenti(frasi, max_length, tokenizer)\n",
    "\n",
    "    # Traduci ogni segmento\n",
    "    traduzioni = []\n",
    "    for idx, segmento in enumerate(segmenti):\n",
    "        traduzione_segmento = traduci_segmento(segmento, tokenizer, model, src_lang, tgt_lang, device)\n",
    "        traduzioni.append(traduzione_segmento)\n",
    "\n",
    "    # Combina le traduzioni\n",
    "    traduzione_finale = \" \".join(traduzioni)\n",
    "    return traduzione_finale\n",
    "\n",
    "# Funzione per tradurre tutte le righe del dataset per le 13 colonne\n",
    "def traduci_riga(row, colonne_da_tradurre, max_length, tokenizer, model, src_lang, tgt_lang, device):\n",
    "    result = {}\n",
    "    for colonna in colonne_da_tradurre:\n",
    "        testo = row[colonna]  # Estrai il testo dalla colonna\n",
    "        \n",
    "        # Verifica che il testo sia una stringa prima di procedere con la traduzione\n",
    "        if isinstance(testo, str):\n",
    "            traduzione = traduci_testo_corretto(testo, max_length, tokenizer, model, src_lang, tgt_lang, device)\n",
    "            result[f\"{colonna}_tradotto\"] = traduzione\n",
    "        else:\n",
    "            # Se non è una stringa, puoi convertire il dato in stringa\n",
    "            result[f\"{colonna}_tradotto\"] = str(testo)  # Converte il valore non stringa in stringa\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Funzione per tradurre l'intero dataset e salvare progressivamente nel workspace\n",
    "def traduci_dataset_e_salva(dataset, colonne_da_tradurre, workspace_file, tokenizer, model, src_lang, tgt_lang, device, batch_size=30):\n",
    "    \"\"\"\n",
    "    Traduce un intero dataset riga per riga e salva progressivamente nel workspace.\n",
    "    \"\"\"\n",
    "    # Controlla se esiste già un workspace\n",
    "    if os.path.exists(workspace_file):\n",
    "        # Carica i progressi dal workspace\n",
    "        translated_dataset = pd.read_pickle(workspace_file)\n",
    "        start_idx = len(translated_dataset)\n",
    "        print(f\"Rilevato workspace con {start_idx} righe tradotte.\")\n",
    "    else:\n",
    "        # Se non esiste, crea un nuovo dataset tradotto vuoto\n",
    "        translated_dataset = pd.DataFrame(columns=dataset.columns)\n",
    "        start_idx = 0\n",
    "\n",
    "    # Traduce il dataset riga per riga\n",
    "    for start in tqdm(range(start_idx, len(dataset), batch_size), desc=\"Traduzione batch\"):\n",
    "        end = min(start + batch_size, len(dataset))  # Fissa il limite massimo per l'ultimo batch\n",
    "        batch = dataset.iloc[start:end]\n",
    "\n",
    "        # Accumula le righe tradotte\n",
    "        translated_rows = []\n",
    "        for _, row in batch.iterrows():\n",
    "            translated_row = row.copy()\n",
    "            # Traduce per ogni colonna da tradurre\n",
    "            for colonna in colonne_da_tradurre:\n",
    "                # Verifica se la colonna è presente nella riga e se contiene una stringa\n",
    "                if colonna in row and isinstance(row[colonna], str):\n",
    "                    testo = row[colonna]\n",
    "                    traduzione = traduci_testo_corretto(testo, max_length, tokenizer, model, src_lang, tgt_lang, device)\n",
    "                    translated_row[f\"{colonna}_tradotto\"] = traduzione\n",
    "                else:\n",
    "                    # Se non è una stringa converti\n",
    "                    translated_row[f\"{colonna}_tradotto\"] = str(row[colonna]) \n",
    "            # Aggiungi la riga tradotta alla lista\n",
    "            translated_rows.append(translated_row)\n",
    "\n",
    "        # Aggiungi le nuove righe tradotte al dataset tradotto\n",
    "        translated_dataset = pd.concat([translated_dataset, pd.DataFrame(translated_rows)], ignore_index=True)\n",
    "\n",
    "        # Salva il workspace ogni batch\n",
    "        translated_dataset.to_pickle(workspace_file)\n",
    "        print(f\"Salvato workspace aggiornato con {len(translated_dataset)} righe.\")\n",
    "\n",
    "    return translated_dataset\n",
    "\n",
    "# Lista delle colonne da preprocessare\n",
    "colonne_da_tradurre = ['testo', 'motivo_ricovero', 'anamnesi', 'esameobiettivo', \n",
    "                            'terapiafarmaingresso', 'decorso', 'laboratorio', 'interventi', \n",
    "                            'followup', 'terapie2', 'terapie3', 'esami', 'reparto']\n",
    "\n",
    "workspace_file = \"workspaceFinal.pkl\"  # File workspace\n",
    "output_file = \"translated_datasetFinal.csv\"  # File tradotto finale\n",
    "\n",
    "# Esegui la traduzione e salva progressivamente\n",
    "translated_dataset = traduci_dataset_e_salva(merged_dataset, \n",
    "                                             colonne_da_tradurre, \n",
    "                                             workspace_file, \n",
    "                                             tokenizer, model, \n",
    "                                             src_lang, tgt_lang, \n",
    "                                             device, batch_size=30)\n",
    "\n",
    "# Salva il dataset tradotto finale in CSV\n",
    "translated_dataset.to_csv(output_file, index=False)\n",
    "print(f\"Traduzione completata. Risultati finali salvati in: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_dataset['anamnesi'][58])\n",
    "print(translated_dataset['anamnesi_tradotto'][58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translated_dataset.shape)\n",
    "print(translated_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per ogni riga, stampa ciascuna colonna tradotta \n",
    "for index, row in translated_dataset.iterrows():\n",
    "    print(f\"Riga {index + 1}:\")\n",
    "    for colonna in colonne_da_tradurre:\n",
    "        colonna_tradotta = f\"{colonna}_tradotto\"\n",
    "        testo_originale = row[colonna] if colonna in row else None\n",
    "        testo_tradotto = row[colonna_tradotta] if colonna_tradotta in row else None\n",
    "        print(f\"  Colonna originale '{colonna}': {testo_originale}\")\n",
    "        print(f\"  Colonna tradotta '{colonna_tradotta}': {testo_tradotto}\")\n",
    "    print(\"-\" * 80)  # Separatore tra righe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## preprocessing traduzione ############################\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Carica il modello SpaCy per l'inglese\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Lista delle colonne su cui applicare il preprocessing\n",
    "colonne_da_preprocessare = ['testo_tradotto', 'motivo_ricovero_tradotto', 'anamnesi_tradotto', \n",
    "                             'esameobiettivo_tradotto', 'terapiafarmaingresso_tradotto', \n",
    "                             'decorso_tradotto', 'laboratorio_tradotto', 'interventi_tradotto', \n",
    "                             'followup_tradotto', 'terapie2_tradotto', 'terapie3_tradotto', \n",
    "                             'esami_tradotto', 'reparto_tradotto']\n",
    "\n",
    "# Funzione per preprocessare una riga di testo\n",
    "def preprocess_text(row):\n",
    "    \"\"\"\n",
    "    Preprocessa una stringa di testo applicando le seguenti trasformazioni:\n",
    "    1. Rimozione di date e numeri speciali.\n",
    "    2. Aggiunta di spazi tra numeri e lettere.\n",
    "    3. Normalizzazione di parole composte (camelCase).\n",
    "    4. Rimozione di punteggiatura e caratteri speciali.\n",
    "    5. Tokenizzazione, lemmatizzazione e rimozione di stopwords con SpaCy.\n",
    "    \"\"\"\n",
    "    if not isinstance(row, str) or not row:  # Aggiungi controllo per valori non stringa o vuoti (oppure not row.strip())\n",
    "        return \"\"  # Restituisce una stringa vuota se non valido\n",
    "\n",
    "    # Rimuove parole esatte \"μl\", \"μg\" e simili\n",
    "    #row = re.sub(r'\\b[μu]\\w*\\b', '', row)\n",
    "\n",
    "    # Rimuove sequenze numeriche separate da underscore o trattini (es. \"6_2_5_1\" o \"6-2-5-1\")\n",
    "    #row = re.sub(r'\\b\\d+([_\\-]\\d+)+\\b', '', row)\n",
    "\n",
    "    # Rimuove date nel formato 'dd/mm/yyyy' e 'dd/mm/yy'\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', '', row)\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}', '', row)  # Rimuove numeri separati da /\n",
    "\n",
    "    # Aggiunge spazi tra numeri e lettere (es. \"800paracetamol\" -> \"800 paracetamol\")\n",
    "    row = re.sub(r'(\\d+)([a-zA-Z]+)', r'\\1 \\2', row)\n",
    "    row = re.sub(r'([a-zA-Z]+)(\\d+)', r'\\1 \\2', row)\n",
    "\n",
    "    # Normalizza parole composte tipo \"seeAttachment\" -> \"see Attachment\"\n",
    "    row = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', row)\n",
    "\n",
    "    # Rimuove punteggiatura e normalizza gli spazi multipli\n",
    "    row = re.sub(r'[^\\w\\s]', ' ', row)  # Rimuove caratteri non alfanumerici (compresa punteggiatura)\n",
    "    row = re.sub(r'\\s+', ' ', row).strip()  # Normalizza spazi multipli\n",
    "\n",
    "    # Rimuove i numeri\n",
    "    row = re.sub(r'\\b\\d+\\b', '', row)  # Rimuove numeri isolati\n",
    "\n",
    "    # Tokenizza il testo con SpaCy\n",
    "    doc = nlp(row)\n",
    "\n",
    "    # Filtra stopwords, punteggiatura e token non validi\n",
    "    tokens_puliti = [\n",
    "        token.lemma_.lower()  # Lemmatizzazione e lowercase\n",
    "        for token in doc\n",
    "        if not token.is_punct and not token.is_stop and len(token.text) > 1\n",
    "    ]\n",
    "\n",
    "    # Applica un filtro finale per rimuovere parole troppo corte\n",
    "    tokens_puliti = [token for token in tokens_puliti if len(token) > 1]\n",
    "\n",
    "    # Ricombina i token in una stringa\n",
    "    return \" \".join(tokens_puliti)\n",
    "\n",
    "\n",
    "# Preprocessing riga per riga\n",
    "for colonna in colonne_da_preprocessare:\n",
    "    print(f\"Processando colonna: {colonna}\")\n",
    "    preprocessed_rows = []\n",
    "    for idx, row in translated_dataset[colonna].items():\n",
    "        if not isinstance(row, str):\n",
    "            preprocessed_rows.append(\"\")\n",
    "        else:\n",
    "            try:\n",
    "                preprocessed_rows.append(preprocess_text(row))\n",
    "            except Exception as e:\n",
    "                print(f\"Errore alla riga {idx} nella colonna {colonna}: {e}\")\n",
    "                preprocessed_rows.append(\"\")\n",
    "    translated_dataset[colonna] = preprocessed_rows\n",
    "\n",
    "# Output del dataset preprocessato\n",
    "print(\"\\nDataset preprocessato:\")\n",
    "print(translated_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ DIMENSIONE VOCABOLARIO ############################\n",
    "# Calcola il vocabolario\n",
    "vocab_set = set()\n",
    "# Itera sulle prime 100 righe delle colonne preprocessate\n",
    "for colonna in colonne_da_preprocessare:\n",
    "    for testo in translated_dataset[colonna]:  # Limita a 100 righe\n",
    "        if isinstance(testo, str):  # Assicurati che il testo non sia NaN\n",
    "            vocab_set.update(testo.split())\n",
    "\n",
    "# Calcola la dimensione del vocabolario\n",
    "vocabolario_dimensione = len(vocab_set)\n",
    "print(\"Dimensione del vocabolario:\", vocabolario_dimensione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## CLASSIFIERS #################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Definisci una lista di classificatori che vuoi provare\n",
    "classifiers = {\n",
    "    #'RandomForest': RandomForestClassifier(n_jobs=-1, max_depth=10, max_features=0.1, random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=5000),\n",
    "    #'SVM': SVC(probability=True,random_state=42),\n",
    "    #'KNeighbors': KNeighborsClassifier(),\n",
    "    #'DecisionTree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    #'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    #'extremeGradientBoosting': XGBClassifier(learning_rate=0.1, random_state=42, n_jobs=-1, max_depth=6),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### CROSS VALIDATION #########################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "\n",
    "\n",
    "# Funzione modificata per includere la media delle features\n",
    "def eval_cross_validation(pipeline, X, y, skf):\n",
    "    list_reports = []\n",
    "    list_f1 = []\n",
    "    list_auc = []  # Lista per raccogliere i valori di AUC mediati\n",
    "    list_num_features = []  # Lista per raccogliere il numero di features per ogni fold\n",
    "    \n",
    "    # Crea una lista per raccogliere precision, recall e f1-score mediati\n",
    "    precision_sum = {}\n",
    "    recall_sum = {}\n",
    "    f1_sum = {}\n",
    "    \n",
    "    # Liste per la deviazione standard\n",
    "    precision_values = {}\n",
    "    recall_values = {}\n",
    "    f1_values = {}\n",
    "\n",
    "    # Inizializza il supporto per ogni classe\n",
    "    unique_labels = np.unique(y)  # Usa np.unique per gli array NumPy\n",
    "    support_sum = {str(label): 0 for label in unique_labels}  # Assicurati che le etichette siano stringhe\n",
    "\n",
    "    for train, val in skf.split(X, y):\n",
    "        X_tr = X.iloc[train]  # differenza con 'baseline1'\n",
    "        y_tr = y[train]  # Modifica per lavorare con l'array NumPy\n",
    "        X_val = X.iloc[val]\n",
    "        y_val = y[val]  # Modifica per lavorare con l'array NumPy\n",
    "\n",
    "        # Addestra il pipeline sul training set\n",
    "        pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "        # Previsioni sul validation set\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        y_pred_prob = pipeline.predict_proba(X_val)[:, 1]  # Probabilità della classe positiva\n",
    "\n",
    "        # Crea il classification report come dizionario\n",
    "        cr = classification_report(y_val, y_pred, output_dict=True)\n",
    "\n",
    "        # Aggiungi il report alla lista\n",
    "        list_reports.append(cr)\n",
    "\n",
    "        # Estrai il F1-score\n",
    "        list_f1.append(cr['weighted avg']['f1-score'])\n",
    "\n",
    "        # Calcola l'AUC per questo fold\n",
    "        auc_score = roc_auc_score(y_val, y_pred_prob)\n",
    "        list_auc.append(auc_score)\n",
    "\n",
    "        # Raccogli il numero di features per questo fold\n",
    "        X_word2vec = pipeline.named_steps['multi_word2vec'].transform(X_val)  # Modificato per estrarre la trasformazione\n",
    "        list_num_features.append(X_word2vec.shape[1])  # Numero di features per questo fold\n",
    "\n",
    "        # Somma le metriche per ogni classe\n",
    "        for label, metrics in cr.items():\n",
    "            if label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                precision_sum[label] = precision_sum.get(label, 0) + metrics['precision']\n",
    "                recall_sum[label] = recall_sum.get(label, 0) + metrics['recall']\n",
    "                f1_sum[label] = f1_sum.get(label, 0) + metrics['f1-score']\n",
    "\n",
    "                # Aggiungi valori per la deviazione standard\n",
    "                precision_values[label] = precision_values.get(label, []) + [metrics['precision']]\n",
    "                recall_values[label] = recall_values.get(label, []) + [metrics['recall']]\n",
    "                f1_values[label] = f1_values.get(label, []) + [metrics['f1-score']]\n",
    "\n",
    "                # Somma il supporto per questa fold\n",
    "                support_sum[str(label)] += metrics['support']  # Usa str(label) per garantire la corrispondenza\n",
    "\n",
    "    # Calcola la media dell'AUC\n",
    "    auc_avg = np.mean(list_auc)\n",
    "\n",
    "    # Calcola la media del numero di features\n",
    "    num_features_avg = np.mean(list_num_features)\n",
    "\n",
    "    # Calcola la media delle metriche per ogni classe\n",
    "    num_folds = skf.get_n_splits()\n",
    "    precision_avg = {label: precision_sum[label] / num_folds for label in precision_sum}\n",
    "    recall_avg = {label: recall_sum[label] / num_folds for label in recall_sum}\n",
    "    f1_avg_per_class = {label: f1_sum[label] / num_folds for label in f1_sum}\n",
    "\n",
    "    # Calcola la deviazione standard per ogni metrica\n",
    "    precision_std = {label: np.std(precision_values[label]) for label in precision_values}\n",
    "    recall_std = {label: np.std(recall_values[label]) for label in recall_values}\n",
    "    f1_std = {label: np.std(f1_values[label]) for label in f1_values}\n",
    "\n",
    "    # Calcola il supporto medio per ciascuna classe\n",
    "    support_avg = {label: support_sum[label] / num_folds for label in support_sum}\n",
    "\n",
    "    # Crea un DataFrame per visualizzare le metriche\n",
    "    df_avg = pd.DataFrame({\n",
    "        'Precision': precision_avg,\n",
    "        'Recall': recall_avg,\n",
    "        'F1-Score': f1_avg_per_class,\n",
    "        'Precision Std': precision_std,\n",
    "        'Recall Std': recall_std,\n",
    "        'F1-Score Std': f1_std,\n",
    "        'Support': support_avg,  # Supporto medio\n",
    "        'Avg Features': num_features_avg,  # Media delle features\n",
    "    })  # Trasponi per avere le classi come righe\n",
    "\n",
    "    return df_avg, auc_avg, f1_avg_per_class, num_features_avg \n",
    "\n",
    "# Crea un oggetto StratifiedKFold per la cross-validation stratificata\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## calcolo oov con tokenizzazione spacy #######################################\n",
    "##################################### bio embedding #####################################################\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "columns_to_vectorize = ['testo_tradotto', 'motivo_ricovero_tradotto', 'anamnesi_tradotto', \n",
    "                             'esameobiettivo_tradotto', 'terapiafarmaingresso_tradotto', \n",
    "                             'decorso_tradotto', 'laboratorio_tradotto', 'interventi_tradotto', \n",
    "                             'followup_tradotto', 'terapie2_tradotto', 'terapie3_tradotto', \n",
    "                             'esami_tradotto', 'reparto_tradotto']\n",
    "\n",
    "X = translated_dataset[columns_to_vectorize]  # Passa l'intero dataset, con le 13 colonne\n",
    "y = translated_dataset['positivi']\n",
    "# Converti le etichette in numeri con LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)  # Trasforma le etichette in numeri\n",
    "\n",
    "\n",
    "# Carica il modello SpaCy (puoi usare \"en_core_web_sm\" o un altro modello disponibile)\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"parser\", \"ner\"])  # Disabilita parser e ner per velocità\n",
    "\n",
    "def tokenize_with_spacy(text):\n",
    "    \"\"\"\n",
    "    Tokenizza un testo usando SpaCy.\n",
    "    :param text: stringa preprocessata.\n",
    "    :return: lista di token.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Tokenizza il testo usando SpaCy\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_space]  # Esclude spazi vuoti\n",
    "    return tokens\n",
    "\n",
    "def calculate_oov_percentage_word2vec(texts, word2vec_model, global_oov_counter, global_valid_words):\n",
    "    total_words = 0\n",
    "    oov_words = 0\n",
    "\n",
    "    for text in texts:\n",
    "        # Usa SpaCy per tokenizzare\n",
    "        words = tokenize_with_spacy(text)\n",
    "        \n",
    "        total_words += len(words)\n",
    "        \n",
    "        for word in words:\n",
    "            # Controlla se la parola è presente nel vocabolario di Word2Vec\n",
    "            if word in word2vec_model:\n",
    "                global_valid_words.add(word)  # Aggiungi al set globale delle parole valide\n",
    "            else:\n",
    "                oov_words += 1\n",
    "                global_oov_counter[word] += 1  # Aggiungi alla conta globale degli OOV\n",
    "    \n",
    "    # Calcola la percentuale di OOV\n",
    "    oov_percentage = (oov_words / total_words) * 100 if total_words > 0 else 0\n",
    "    return oov_words, total_words, oov_percentage\n",
    "\n",
    "# Carica il modello pre-addestrato di Word2Vec\n",
    "word2vec_model_path = \"/home/v.acampora/bio_embedding_intrinsic\"\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True)\n",
    "\n",
    "# Contatori globali\n",
    "global_oov_counter = Counter()  # Per tenere traccia delle parole OOV globali\n",
    "global_valid_words = set()  # Per tenere traccia delle parole valide globali\n",
    "\n",
    "# Se hai un DataFrame, applica la funzione per ciascuna colonna\n",
    "all_oov_words = 0\n",
    "all_total_words = 0\n",
    "\n",
    "for col in X.columns:\n",
    "    print(f\"Calcolando OOV per la colonna: {col}\")\n",
    "    texts = X[col].dropna().astype(str).tolist()  # Assicurati che i dati siano in formato stringa\n",
    "    oov_words, total_words, oov_percentage = calculate_oov_percentage_word2vec(\n",
    "        texts, word2vec_model, global_oov_counter, global_valid_words)\n",
    "    \n",
    "    # Aggiorna i contatori globali\n",
    "    all_oov_words += oov_words\n",
    "    all_total_words += total_words\n",
    "\n",
    "    print(f\"Parole OOV nella colonna '{col}': {oov_words}, Totale parole: {total_words}, Percentuale OOV: {oov_percentage:.2f}%\")\n",
    "\n",
    "# Calcolo delle statistiche globali\n",
    "total_unique_words = len(global_valid_words) + len(global_oov_counter)\n",
    "oov_percentage_global = (len(global_oov_counter) / total_unique_words) * 100 if total_unique_words > 0 else 0\n",
    "\n",
    "# Risultati globali\n",
    "print(\"\\n### Statistiche globali ###\")\n",
    "print(f\"Totale parole valide uniche: {len(global_valid_words)}\")\n",
    "print(f\"Totale parole OOV uniche: {len(global_oov_counter)}\")\n",
    "print(f\"Totale parole uniche (valide + OOV): {total_unique_words}\")\n",
    "print(f\"Percentuale globale di parole OOV: {oov_percentage_global:.2f}%\")\n",
    "\n",
    "# Stampa le prime 50 parole OOV globali\n",
    "print(\"\\nPrime 50 parole OOV globali ordinate per frequenza:\")\n",
    "for i, (word, count) in enumerate(global_oov_counter.most_common(500)):\n",
    "    print(f\"{i+1}. {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dizionario per contare le parole OOV per ogni riga di ogni colonna\n",
    "column_oov_counts = {col: [] for col in X.columns}\n",
    "\n",
    "# Calcolo degli OOV per riga in ogni colonna\n",
    "for col in X.columns:\n",
    "    print(f\"Calcolando OOV per la colonna: {col}\")\n",
    "    texts = X[col].dropna().astype(str).tolist()  # Assicurati che i dati siano stringhe\n",
    "    for text in texts:\n",
    "        words = tokenize_with_spacy(text)  # Usa la tokenizzazione con SpaCy\n",
    "        oov_count = sum(1 for word in words if word not in word2vec_model)\n",
    "        column_oov_counts[col].append(oov_count)  # Aggiungi il conteggio di OOV per questa riga\n",
    "    \n",
    "    print(f\"Completato per colonna: {col}\")\n",
    "\n",
    "# Calcolare la media degli OOV per ogni colonna rispetto a tutte le righe\n",
    "column_oov_averages = {col: np.mean(oov_counts) for col, oov_counts in column_oov_counts.items()}\n",
    "\n",
    "# Ordinare le colonne per media di OOV decrescente\n",
    "sorted_columns = sorted(column_oov_averages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Stampa delle colonne ordinate per media degli OOV\n",
    "print(\"\\n### Media degli OOV per colonna ###\")\n",
    "for col, avg_oov in sorted_columns:\n",
    "    print(f\"Colonna '{col}': Media OOV = {avg_oov:.2f}\")\n",
    "\n",
    "# Creare un grafico a barre con la media degli OOV per colonna\n",
    "plt.figure(figsize=(12, 6))\n",
    "columns = [item[0] for item in sorted_columns]\n",
    "averages = [item[1] for item in sorted_columns]\n",
    "plt.bar(columns, averages, color='skyblue')\n",
    "plt.xlabel('Colonne')\n",
    "plt.ylabel('Media OOV per riga')\n",
    "plt.title('Media delle parole OOV per colonna')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Contenitore per le statistiche delle colonne\n",
    "column_oov_stats = []\n",
    "\n",
    "for col in X.columns:\n",
    "    print(f\"Calcolando OOV per la colonna: {col}\")\n",
    "    texts = X[col].dropna().astype(str).tolist()  # Assicurati che i dati siano in formato stringa\n",
    "    oov_words, total_words, oov_percentage = calculate_oov_percentage_word2vec(\n",
    "        texts, word2vec_model, global_oov_counter, global_valid_words)\n",
    "    \n",
    "    # Salva i dati per calcoli successivi\n",
    "    column_oov_stats.append({\n",
    "        \"column\": col,\n",
    "        \"oov_words\": oov_words,\n",
    "        \"total_words\": total_words,\n",
    "        \"oov_percentage\": oov_percentage\n",
    "    })\n",
    "\n",
    "    # Aggiorna i contatori globali\n",
    "    all_oov_words += oov_words\n",
    "    all_total_words += total_words\n",
    "\n",
    "    print(f\"Parole OOV nella colonna '{col}': {oov_words}, Totale parole: {total_words}, Percentuale OOV: {oov_percentage:.2f}%\")\n",
    "\n",
    "# Calcolo delle statistiche globali\n",
    "total_unique_words = len(global_valid_words) + len(global_oov_counter)\n",
    "oov_percentage_global = (len(global_oov_counter) / total_unique_words) * 100 if total_unique_words > 0 else 0\n",
    "\n",
    "# Risultati globali\n",
    "print(\"\\n### Statistiche globali ###\")\n",
    "print(f\"Totale parole valide uniche: {len(global_valid_words)}\")\n",
    "print(f\"Totale parole OOV uniche: {len(global_oov_counter)}\")\n",
    "print(f\"Totale parole uniche (valide + OOV): {total_unique_words}\")\n",
    "print(f\"Percentuale globale di parole OOV: {oov_percentage_global:.2f}%\")\n",
    "\n",
    "# Creazione del DataFrame per le statistiche per colonna\n",
    "column_oov_stats_df = pd.DataFrame(column_oov_stats)\n",
    "column_oov_stats_df[\"oov_per_word\"] = column_oov_stats_df[\"oov_words\"] / column_oov_stats_df[\"total_words\"]\n",
    "\n",
    "# Ordina le colonne per il numero medio di OOV\n",
    "sorted_columns = column_oov_stats_df.sort_values(\"oov_per_word\", ascending=False)\n",
    "\n",
    "# Stampa delle colonne ordinate in base alla media degli OOV\n",
    "print(\"\\n### Colonne ordinate per numero medio di OOV ###\")\n",
    "print(sorted_columns[[\"column\", \"oov_per_word\"]])\n",
    "\n",
    "# Creazione del grafico a barre\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_columns[\"column\"], sorted_columns[\"oov_per_word\"], color='skyblue')\n",
    "plt.title(\"Numero medio di OOV per colonna\")\n",
    "plt.xlabel(\"Colonne\")\n",
    "plt.ylabel(\"Media di OOV\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# word2vec con embedding in inglese generici #########################i\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Carica il modello SpaCy con embedding generali in inglese (ad esempio \"en_core_web_md\")\n",
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"parser\", \"ner\"])  # Disabilita parser e ner per velocità\n",
    "\n",
    "# Definizione delle colonne del tuo dataset\n",
    "columns_to_vectorize = ['testo_tradotto', 'motivo_ricovero_tradotto', 'anamnesi_tradotto', \n",
    "                        'esameobiettivo_tradotto', 'terapiafarmaingresso_tradotto', \n",
    "                        'decorso_tradotto', 'laboratorio_tradotto', 'interventi_tradotto', \n",
    "                        'followup_tradotto', 'terapie2_tradotto', 'terapie3_tradotto', \n",
    "                        'esami_tradotto', 'reparto_tradotto']\n",
    "\n",
    "X = translated_dataset[columns_to_vectorize]  # Passa l'intero dataset, con le 13 colonne\n",
    "y = translated_dataset['positivi']\n",
    "# Converti le etichette in numeri con LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)  # Trasforma le etichette in numeri\n",
    "\n",
    "\n",
    "def tokenize_with_spacy(text):\n",
    "    \"\"\"\n",
    "    Tokenizza un testo usando SpaCy.\n",
    "    :param text: stringa preprocessata.\n",
    "    :return: lista di token.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # Tokenizza il testo usando SpaCy\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_space]  # Esclude spazi vuoti\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def calculate_oov_percentage_spacy(texts, nlp, global_oov_counter, global_valid_words):\n",
    "    total_words = 0\n",
    "    oov_words = 0\n",
    "\n",
    "    for text in texts:\n",
    "        # Usa SpaCy per tokenizzare\n",
    "        words = tokenize_with_spacy(text)\n",
    "        \n",
    "        total_words += len(words)\n",
    "        \n",
    "        for word in words:\n",
    "            # Ottieni il token dal vocabolario spaCy\n",
    "            token = nlp.vocab[word]  # Usa l'indicizzazione per ottenere il token\n",
    "\n",
    "            if token.has_vector:  # Se la parola ha un vettore di embedding\n",
    "                global_valid_words.add(word)  # Aggiungi al set globale delle parole valide\n",
    "            else:\n",
    "                oov_words += 1\n",
    "                global_oov_counter[word] += 1  # Aggiungi alla conta globale degli OOV\n",
    "    \n",
    "    # Calcola la percentuale di OOV\n",
    "    oov_percentage = (oov_words / total_words) * 100 if total_words > 0 else 0\n",
    "    return oov_words, total_words, oov_percentage\n",
    "\n",
    "# Contatori globali\n",
    "global_oov_counter = Counter()  # Per tenere traccia delle parole OOV globali\n",
    "global_valid_words = set()  # Per tenere traccia delle parole valide globali\n",
    "\n",
    "# Se hai un DataFrame, applica la funzione per ciascuna colonna\n",
    "all_oov_words = 0\n",
    "all_total_words = 0\n",
    "\n",
    "for col in X.columns:\n",
    "    print(f\"Calcolando OOV per la colonna: {col}\")\n",
    "    texts = X[col].dropna().astype(str).tolist()  # Assicurati che i dati siano in formato stringa\n",
    "    oov_words, total_words, oov_percentage = calculate_oov_percentage_spacy(\n",
    "        texts, nlp, global_oov_counter, global_valid_words)\n",
    "    \n",
    "    # Aggiorna i contatori globali\n",
    "    all_oov_words += oov_words\n",
    "    all_total_words += total_words\n",
    "\n",
    "    print(f\"Parole OOV nella colonna '{col}': {oov_words}, Totale parole: {total_words}, Percentuale OOV: {oov_percentage:.2f}%\")\n",
    "\n",
    "# Calcolo delle statistiche globali\n",
    "total_unique_words = len(global_valid_words) + len(global_oov_counter)\n",
    "oov_percentage_global = (len(global_oov_counter) / total_unique_words) * 100 if total_unique_words > 0 else 0\n",
    "\n",
    "# Risultati globali\n",
    "print(\"\\n### Statistiche globali ###\")\n",
    "print(f\"Totale parole valide uniche: {len(global_valid_words)}\")\n",
    "print(f\"Totale parole OOV uniche: {len(global_oov_counter)}\")\n",
    "print(f\"Totale parole uniche (valide + OOV): {total_unique_words}\")\n",
    "print(f\"Percentuale globale di parole OOV: {oov_percentage_global:.2f}%\")\n",
    "\n",
    "# Stampa le prime 50 parole OOV globali\n",
    "print(\"\\nPrime 50 parole OOV globali ordinate per frequenza:\")\n",
    "for i, (word, count) in enumerate(global_oov_counter.most_common(50)):\n",
    "    print(f\"{i+1}. {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### grafico \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Contenitore per le statistiche delle colonne\n",
    "column_oov_stats = []\n",
    "\n",
    "for col in X.columns:\n",
    "    print(f\"Calcolando OOV per la colonna: {col}\")\n",
    "    texts = X[col].dropna().astype(str).tolist()  # Assicurati che i dati siano in formato stringa\n",
    "    oov_words, total_words, oov_percentage = calculate_oov_percentage_spacy(\n",
    "        texts, nlp, global_oov_counter, global_valid_words)\n",
    "    \n",
    "    # Salva i dati per calcoli successivi\n",
    "    column_oov_stats.append({\n",
    "        \"column\": col,\n",
    "        \"oov_words\": oov_words,\n",
    "        \"total_words\": total_words,\n",
    "        \"oov_percentage\": oov_percentage\n",
    "    })\n",
    "\n",
    "    # Aggiorna i contatori globali\n",
    "    all_oov_words += oov_words\n",
    "    all_total_words += total_words\n",
    "\n",
    "    print(f\"Parole OOV nella colonna '{col}': {oov_words}, Totale parole: {total_words}, Percentuale OOV: {oov_percentage:.2f}%\")\n",
    "\n",
    "# Calcolo delle statistiche globali\n",
    "total_unique_words = len(global_valid_words) + len(global_oov_counter)\n",
    "oov_percentage_global = (len(global_oov_counter) / total_unique_words) * 100 if total_unique_words > 0 else 0\n",
    "\n",
    "# Risultati globali\n",
    "print(\"\\n### Statistiche globali ###\")\n",
    "print(f\"Totale parole valide uniche: {len(global_valid_words)}\")\n",
    "print(f\"Totale parole OOV uniche: {len(global_oov_counter)}\")\n",
    "print(f\"Totale parole uniche (valide + OOV): {total_unique_words}\")\n",
    "print(f\"Percentuale globale di parole OOV: {oov_percentage_global:.2f}%\")\n",
    "\n",
    "# Creazione del DataFrame per le statistiche per colonna\n",
    "column_oov_stats_df = pd.DataFrame(column_oov_stats)\n",
    "column_oov_stats_df[\"oov_per_word\"] = column_oov_stats_df[\"oov_words\"] / column_oov_stats_df[\"total_words\"]\n",
    "\n",
    "# Ordina le colonne per il numero medio di OOV\n",
    "sorted_columns = column_oov_stats_df.sort_values(\"oov_per_word\", ascending=False)\n",
    "\n",
    "# Stampa delle colonne ordinate in base alla media degli OOV\n",
    "print(\"\\n### Colonne ordinate per numero medio di OOV ###\")\n",
    "print(sorted_columns[[\"column\", \"oov_per_word\"]])\n",
    "\n",
    "# Creazione del grafico a barre\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_columns[\"column\"], sorted_columns[\"oov_per_word\"], color='skyblue')\n",
    "plt.title(\"Numero medio di OOV per colonna\")\n",
    "plt.xlabel(\"Colonne\")\n",
    "plt.ylabel(\"Media di OOV\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contenitore per le statistiche delle colonne\n",
    "column_oov_stats = []\n",
    "\n",
    "for col in X.columns:\n",
    "    print(f\"Calcolando OOV per la colonna: {col}\")\n",
    "    texts = X[col].dropna().astype(str).tolist()  # Assicurati che i dati siano in formato stringa\n",
    "    \n",
    "    # Lista per memorizzare il numero di OOV per ciascuna riga\n",
    "    oov_per_row = []\n",
    "\n",
    "    for text in texts:\n",
    "        oov_words, total_words, _ = calculate_oov_percentage_spacy(\n",
    "            [text], nlp, global_oov_counter, global_valid_words)\n",
    "        oov_per_row.append(oov_words)  # Aggiungi il numero di OOV di questa riga\n",
    "\n",
    "    # Calcola la media di OOV per riga\n",
    "    mean_oov_per_row = np.mean(oov_per_row)\n",
    "\n",
    "    # Salva i dati per calcoli successivi\n",
    "    column_oov_stats.append({\n",
    "        \"column\": col,\n",
    "        \"mean_oov_per_row\": mean_oov_per_row\n",
    "    })\n",
    "\n",
    "    print(f\"Colonna '{col}': Media OOV per riga: {mean_oov_per_row:.2f}\")\n",
    "\n",
    "# Creazione del DataFrame per le statistiche per colonna\n",
    "column_oov_stats_df = pd.DataFrame(column_oov_stats)\n",
    "\n",
    "# Ordina le colonne per la media di OOV per riga\n",
    "sorted_columns = column_oov_stats_df.sort_values(\"mean_oov_per_row\", ascending=False)\n",
    "\n",
    "# Creazione del grafico a barre\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_columns[\"column\"], sorted_columns[\"mean_oov_per_row\"], color='skyblue')\n",
    "plt.title(\"Media di OOV per riga per colonna\")\n",
    "plt.xlabel(\"Colonne\")\n",
    "plt.ylabel(\"Media di OOV per riga\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### NO SMOTE #################################\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Carica il modello Word2Vec pre-addestrato\n",
    "model_path = \"/home/v.acampora/bio_embedding_intrinsic\"\n",
    "bioword_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "columns_to_vectorize = ['testo_tradotto', 'motivo_ricovero_tradotto', 'anamnesi_tradotto', \n",
    "                             'esameobiettivo_tradotto', 'terapiafarmaingresso_tradotto', \n",
    "                             'decorso_tradotto', 'laboratorio_tradotto', 'interventi_tradotto', \n",
    "                             'followup_tradotto', 'terapie2_tradotto', 'terapie3_tradotto', \n",
    "                             'esami_tradotto', 'reparto_tradotto']\n",
    "\n",
    "X = translated_dataset[columns_to_vectorize]  # Passa l'intero dataset, con le 13 colonne\n",
    "y = translated_dataset['positivi']\n",
    "# Converti le etichette in numeri con LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)  # Trasforma le etichette in numeri\n",
    "\n",
    "\n",
    "class MultiColumnWord2Vec(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model, columns):\n",
    "        self.model = model\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Funzione per vettorizzare il testo\n",
    "        def vectorize_text(text):\n",
    "            #words = word_tokenize(text.lower())  # Tokenizzazione e normalizzazione\n",
    "            words = text.split()  # Semplice divisione in parole basata su spazio\n",
    "            word_vectors = []\n",
    "            for word in words:\n",
    "                if word in self.model:  # Se la parola è nel vocabolario di Word2Vec\n",
    "                    word_vectors.append(self.model[word])\n",
    "            if word_vectors:\n",
    "                return np.mean(word_vectors, axis=0)  # Media dei vettori delle parole\n",
    "            else:\n",
    "                #return None # Restituisce None se non ci sono parole nel vocabolario\n",
    "                return np.zeros(self.model.vector_size)  # Restituisce un vettore zero se non ci sono parole nel vocabolario\n",
    "\n",
    "        # Vettorizza ciascuna colonna separatamente e concatena i risultati\n",
    "        vectors = []\n",
    "        for col in self.columns:\n",
    "            column_text = X[col].fillna(\"\").tolist()\n",
    "            column_vectors = np.array([vectorize_text(text) for text in column_text])\n",
    "            vectors.append(column_vectors)\n",
    "        \n",
    "        # Concatena i vettori di tutte le colonne\n",
    "        return np.hstack(vectors)\n",
    "\n",
    "\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "    \n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "        ('multi_word2vec', MultiColumnWord2Vec(model=bioword_model, columns=columns_to_vectorize)),\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline to the data\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Trasforma i dati e ottieni la dimensione\n",
    "    X_word2vec = pipeline.named_steps['multi_word2vec'].transform(X)\n",
    "    num_samples, num_features = X_word2vec.shape\n",
    "    print(f\"Dimensione della matrice Word2Vec: {num_samples} campioni, {num_features} features\")\n",
    "\n",
    "    # Calcola le metriche usando la funzione\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "    \n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "    \n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### SMOTE #######################################\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "    \n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "        ('multi_word2vec', MultiColumnWord2Vec(model=bioword_model, columns=columns_to_vectorize)),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline to the data\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Trasforma i dati e ottieni la dimensione\n",
    "    X_word2vec = pipeline.named_steps['multi_word2vec'].transform(X)\n",
    "    num_samples, num_features = X_word2vec.shape\n",
    "    print(f\"Dimensione della matrice Word2Vec: {num_samples} campioni, {num_features} features\")\n",
    "\n",
    "    # Calcola le metriche usando la funzione\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "    \n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "    \n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### SMOTE + UNDERSAMPLING #######################################\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "    \n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "        ('multi_word2vec', MultiColumnWord2Vec(model=bioword_model, columns=columns_to_vectorize)),\n",
    "        ('smote', SMOTE(random_state=42, sampling_strategy=0.6)),  # Applica SMOTE fino al 60% della classe maggioritaria\n",
    "        ('undersample', RandomUnderSampler(sampling_strategy=1.0, random_state=42)),\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline to the data\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Trasforma i dati e ottieni la dimensione\n",
    "    X_word2vec = pipeline.named_steps['multi_word2vec'].transform(X)\n",
    "    num_samples, num_features = X_word2vec.shape\n",
    "    print(f\"Dimensione della matrice Word2Vec: {num_samples} campioni, {num_features} features\")\n",
    "\n",
    "    # Calcola le metriche usando la funzione\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "    \n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "    \n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
