{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### carica dataset LDO 20-21 ############################ \n",
    "\n",
    "dataLDO2020 = pd.read_excel('/home/a.renda/to_move/LDO/filtrato_per_keyword/20-21_LDO_26K/LDO_20200101_20210101 pulito.ods', engine='odf')\n",
    "dataLDO2021=pd.read_excel('/home/a.renda/to_move/LDO/filtrato_per_keyword/20-21_LDO_26K/LDO_20210101_20220101 pulito.ods', engine='odf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra le righe che non contengono numeri (escludendo anche i NaN)\n",
    "dataLDO2021 = dataLDO2021[dataLDO2021['nosologico'].astype(str).str.contains(r'\\d')]\n",
    "#rimosse 212 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nColumns LDO 20-21:\", dataLDO2020.columns)\n",
    "print(\"\\nColumns LDO 21-22:\", dataLDO2021.columns)\n",
    "print(\"\\nShape LDO 20-21:\", dataLDO2020.shape)\n",
    "print(\"\\nShape LDO 21-22:\", dataLDO2021.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## carica database filtrato ###############################\n",
    "\n",
    "databaseFiltrato=pd.read_csv('/home/a.renda/to_move/LDO/labeled/20-21_341/DatabaseFiltrato.csv', sep=';')\n",
    "print(databaseFiltrato.shape)\n",
    "print(databaseFiltrato.columns) # la prima colonna è solo un contatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra le righe che non contengono numeri (escludendo anche i NaN)\n",
    "databaseFiltrato = databaseFiltrato[databaseFiltrato['nosologico'].astype(str).str.contains(r'\\d')]\n",
    "\n",
    "#tolte 5 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### filtra ldo 2020 ##########################\n",
    "\n",
    "# Converti la colonna 'nosologico' del secondo dataset in int\n",
    "databaseFiltrato['nosologico'] = pd.to_numeric(databaseFiltrato['nosologico'], errors='coerce')\n",
    "\n",
    "\n",
    "# Trova i nosologici comuni\n",
    "comuni2020 = dataLDO2020['nosologico'].isin(databaseFiltrato['nosologico'])\n",
    "\n",
    "# Filtra il primo dataset\n",
    "dataset_filtrato2020 = dataLDO2020[comuni2020]\n",
    "print(dataset_filtrato2020.columns)\n",
    "print(dataset_filtrato2020.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### filtra ldo 2021 ###########################\n",
    "\n",
    "# Trova i nosologici comuni\n",
    "comuni2021 = dataLDO2021['nosologico'].isin(databaseFiltrato['nosologico'])\n",
    "\n",
    "# Filtra il primo dataset\n",
    "dataset_filtrato2021 = dataLDO2021[comuni2021]\n",
    "print(dataset_filtrato2021.columns)\n",
    "print(dataset_filtrato2021.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## merge ldo2020 e ldo 2021 filtrati ###################################\n",
    "\n",
    "merged_dataset = pd.concat([dataset_filtrato2020, dataset_filtrato2021], ignore_index=True) # non ci sono duplicati tra i due dataset \n",
    "\n",
    "# Risultato finale\n",
    "print(\"\\nColumns merged dataset:\",merged_dataset.columns)\n",
    "print(\"\\nShape merged dataset:\",merged_dataset.shape)\n",
    "print(merged_dataset['testo'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## carica dataset con nosologici positivi #####################\n",
    "\n",
    "Positivi= pd.read_excel('/home/a.renda/to_move/LDO/labeled/20-21_341/NosologiciPositivi_341.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### aggiungere la colonna positivi #########################################\n",
    "\n",
    "# Convertire la colonna 'Nosologico' in numerico nel dataset NosologiciPositivi\n",
    "nosologici_positivi = pd.to_numeric(Positivi['NosologiciPositivi'], errors='coerce').dropna()\n",
    "\n",
    "# Creare la colonna 'positivi' nel DataFrame merged_dataset\n",
    "merged_dataset['positivi'] = merged_dataset['nosologico'].isin(nosologici_positivi).astype(int)\n",
    "\n",
    "\n",
    "# Contare quanti 1 e quanti 0 ci sono nella colonna 'positivi'\n",
    "count_positivi = merged_dataset['positivi'].value_counts()\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"\\nConteggio dei valori nella colonna 'positivi':\")\n",
    "print(f\"Numero di 1 (positivi): {count_positivi.get(1, 0)}\")\n",
    "print(f\"Numero di 0 (non positivi): {count_positivi.get(0, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unire parole e numeri in 'reparto' rimuovendo lo spazio e sostituendo con un trattino\n",
    "merged_dataset['reparto'] = merged_dataset['reparto'].str.replace(r'(\\w) (\\d)', r'\\1-\\2', regex=True)\n",
    "\n",
    "# Visualizza i risultati\n",
    "print(merged_dataset['reparto'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### pulisci il testo: risoluzione di errori di codifica, sostituzione caratteri speciali #######################\n",
    "import ftfy\n",
    "\n",
    "# Applica ftfy.fix_text() a tutte le colonne di testo nel dataset, gestendo i valori non testuali\n",
    "for col in merged_dataset.select_dtypes(include='object').columns:\n",
    "    merged_dataset[col] = merged_dataset[col].apply(lambda x: ftfy.fix_text(x) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## PRE-PROCESSING ##########################\n",
    "############# valutare se il preprocessing deve essere modificato #######################\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Carica il modello SpaCy per l'italiano\n",
    "nlp = spacy.load(\"it_core_news_lg\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Lista delle colonne su cui applicare il preprocessing\n",
    "colonne_da_preprocessare = ['testo', 'motivo_ricovero', 'anamnesi', \n",
    "                             'esameobiettivo', 'terapiafarmaingresso', \n",
    "                             'decorso', 'laboratorio', 'interventi', \n",
    "                             'followup', 'terapie2', 'terapie3', \n",
    "                             'esami', 'reparto']  # Sostituisci con i nomi reali delle colonne\n",
    "\n",
    "def clean_token(token):\n",
    "\n",
    "    # Rimuovi caratteri non alfanumerici (inclusi simboli, punteggiatura, numeri, ecc.) e sostituiscili con uno spazio\n",
    "    #cleaned_token = re.sub(r'[^a-zA-ZàèéìòùÀÈÉÌÒÙ]', ' ', token)\n",
    "    cleaned_token = re.sub(r'\\s+', ' ', token).strip()  # Normalizza gli spazi multipli\n",
    "\n",
    "    return cleaned_token\n",
    "\n",
    "    \n",
    "# Funzione per tokenizzare e preprocessare il testo\n",
    "def preprocess_text(row):\n",
    "    if not isinstance(row, str):  # Verifica che il dato sia una stringa valida\n",
    "        return \"\"  # Restituisce una stringa vuota se non valido\n",
    "    \n",
    "    # Rimuove date nel formato 'dd/mm/yyyy' e 'dd/mm/yy'\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', '', row)\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}', '', row)  # Rimuove numeri separati da /\n",
    "      \n",
    "    # Aggiunge spazi tra numeri e lettere (es. \"800duloxetina\" -> \"800 duloxetina\")\n",
    "    row = re.sub(r'(\\d+)([a-zA-Z]+)', r'\\1 \\2', row)\n",
    "    row = re.sub(r'([a-zA-Z]+)(\\d+)', r'\\1 \\2', row)\n",
    "\n",
    "    # Aggiunge spazi tra parole composte tipo \"vediAllegato\" -> \"vedi Allegato\"\n",
    "    row = re.sub(r'([a-zàèéìòù])([A-ZÀÈÉÌÒÙ])', r'\\1 \\2', row)\n",
    "\n",
    "    # normalizza  spazi (se ci sono più spazi consecutivi, vengono ridotti a uno solo) e rimuove  spazi all'inizio e alla fine della stringa\n",
    "    #row = re.sub(r'[^\\w\\s]', ' ', row)  # Rimuove caratteri non alfanumerici e parentesi\n",
    "\n",
    "    doc = nlp(row)\n",
    "   \n",
    "    # Filtra e normalizza i token\n",
    "    tokens_puliti = [\n",
    "        clean_token(token.text)\n",
    "        for token in doc\n",
    "        #if not token.is_punct and not token.is_stop and len(token.text) > 1 #and token.text.isalpha() \n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Applica un filtro finale per rimuovere manualmente le lettere singole\n",
    "    tokens_puliti = [token for token in tokens_puliti if len(token) > 1]  # Assicura che tutte le parole siano > 1 carattere\n",
    "\n",
    "    # Ricombina i token in una stringa\n",
    "    return \" \".join(tokens_puliti)\n",
    "\n",
    "# Applica il preprocessing per ciascuna colonna specificata\n",
    "for colonna in colonne_da_preprocessare:\n",
    "    nuova_colonna = f\"{colonna}_preprocessed\"  # Nome della nuova colonna\n",
    "    merged_dataset[nuova_colonna] = merged_dataset[colonna].apply(preprocess_text)\n",
    "\n",
    "# Visualizza i risultati per le nuove colonne preprocessate\n",
    "print(merged_dataset[[colonna for colonna in colonne_da_preprocessare] + \n",
    "                     [f\"{col}_preprocessed\" for col in colonne_da_preprocessare]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ DIMENSIONE VOCABOLARIO ############################\n",
    "# Calcola il vocabolario\n",
    "vocab_set = set()\n",
    "# Itera sulle prime 100 righe delle colonne preprocessate\n",
    "for colonna in colonne_da_preprocessare:\n",
    "    for testo in merged_dataset[f\"{colonna}_preprocessed\"]:  # Limita a 100 righe\n",
    "        if isinstance(testo, str):  # Assicurati che il testo non sia NaN\n",
    "            vocab_set.update(testo.split())\n",
    "\n",
    "# Calcola la dimensione del vocabolario\n",
    "vocabolario_dimensione = len(vocab_set)\n",
    "print(\"Dimensione del vocabolario:\", vocabolario_dimensione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## CLASSIFIERS #################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Definisci una lista di classificatori che vuoi provare\n",
    "classifiers = {\n",
    "    #'RandomForest': RandomForestClassifier(n_jobs=-1, max_depth=10, max_features=0.1, random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=3000),\n",
    "    #'SVM': SVC(probability=True,random_state=42),\n",
    "    #'KNeighbors': KNeighborsClassifier(),\n",
    "    #'DecisionTree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    #'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    #'extremeGradientBoosting': XGBClassifier( learning_rate=0.1, random_state=42, n_jobs=-1, max_depth=6),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### CROSS VALIDATION #########################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "\n",
    "\n",
    "# Funzione modificata per includere la media delle features\n",
    "def eval_cross_validation(pipeline, X, y, skf):\n",
    "    list_reports = []\n",
    "    list_f1 = []\n",
    "    list_auc = []  # Lista per raccogliere i valori di AUC mediati\n",
    "    list_num_features = []  # Lista per raccogliere il numero di features per ogni fold\n",
    "    \n",
    "    # Crea una lista per raccogliere precision, recall e f1-score mediati\n",
    "    precision_sum = {}\n",
    "    recall_sum = {}\n",
    "    f1_sum = {}\n",
    "    \n",
    "    # Liste per la deviazione standard\n",
    "    precision_values = {}\n",
    "    recall_values = {}\n",
    "    f1_values = {}\n",
    "\n",
    "    # Inizializza il supporto per ogni classe\n",
    "    unique_labels = np.unique(y)  # Usa np.unique per gli array NumPy\n",
    "    support_sum = {str(label): 0 for label in unique_labels}  # Assicurati che le etichette siano stringhe\n",
    "\n",
    "    for train, val in skf.split(X, y):\n",
    "        X_tr = X.iloc[train]  # differenza con 'baseline1'\n",
    "        y_tr = y[train]  # Modifica per lavorare con l'array NumPy\n",
    "        X_val = X.iloc[val]\n",
    "        y_val = y[val]  # Modifica per lavorare con l'array NumPy\n",
    "\n",
    "        # Addestra il pipeline sul training set\n",
    "        pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "        # Previsioni sul validation set\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        y_pred_prob = pipeline.predict_proba(X_val)[:, 1]  # Probabilità della classe positiva\n",
    "\n",
    "        # Crea il classification report come dizionario\n",
    "        cr = classification_report(y_val, y_pred, output_dict=True)\n",
    "\n",
    "        # Aggiungi il report alla lista\n",
    "        list_reports.append(cr)\n",
    "\n",
    "        # Estrai il F1-score\n",
    "        list_f1.append(cr['weighted avg']['f1-score'])\n",
    "\n",
    "        # Calcola l'AUC per questo fold\n",
    "        auc_score = roc_auc_score(y_val, y_pred_prob)\n",
    "        list_auc.append(auc_score)\n",
    "\n",
    "        # Raccogli il numero di features per questo fold\n",
    "        X_word2vec = pipeline.named_steps['bert'].transform(X_val)  # Modificato per estrarre la trasformazione\n",
    "        list_num_features.append(X_word2vec.shape[1])  # Numero di features per questo fold\n",
    "\n",
    "        # Somma le metriche per ogni classe\n",
    "        for label, metrics in cr.items():\n",
    "            if label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                precision_sum[label] = precision_sum.get(label, 0) + metrics['precision']\n",
    "                recall_sum[label] = recall_sum.get(label, 0) + metrics['recall']\n",
    "                f1_sum[label] = f1_sum.get(label, 0) + metrics['f1-score']\n",
    "\n",
    "                # Aggiungi valori per la deviazione standard\n",
    "                precision_values[label] = precision_values.get(label, []) + [metrics['precision']]\n",
    "                recall_values[label] = recall_values.get(label, []) + [metrics['recall']]\n",
    "                f1_values[label] = f1_values.get(label, []) + [metrics['f1-score']]\n",
    "\n",
    "                # Somma il supporto per questa fold\n",
    "                support_sum[str(label)] += metrics['support']  # Usa str(label) per garantire la corrispondenza\n",
    "\n",
    "    # Calcola la media dell'AUC\n",
    "    auc_avg = np.mean(list_auc)\n",
    "\n",
    "    # Calcola la media del numero di features\n",
    "    num_features_avg = np.mean(list_num_features)\n",
    "\n",
    "    # Calcola la media delle metriche per ogni classe\n",
    "    num_folds = skf.get_n_splits()\n",
    "    precision_avg = {label: precision_sum[label] / num_folds for label in precision_sum}\n",
    "    recall_avg = {label: recall_sum[label] / num_folds for label in recall_sum}\n",
    "    f1_avg_per_class = {label: f1_sum[label] / num_folds for label in f1_sum}\n",
    "\n",
    "    # Calcola la deviazione standard per ogni metrica\n",
    "    precision_std = {label: np.std(precision_values[label]) for label in precision_values}\n",
    "    recall_std = {label: np.std(recall_values[label]) for label in recall_values}\n",
    "    f1_std = {label: np.std(f1_values[label]) for label in f1_values}\n",
    "\n",
    "    # Calcola il supporto medio per ciascuna classe\n",
    "    support_avg = {label: support_sum[label] / num_folds for label in support_sum}\n",
    "\n",
    "    # Crea un DataFrame per visualizzare le metriche\n",
    "    df_avg = pd.DataFrame({\n",
    "        'Precision': precision_avg,\n",
    "        'Recall': recall_avg,\n",
    "        'F1-Score': f1_avg_per_class,\n",
    "        'Precision Std': precision_std,\n",
    "        'Recall Std': recall_std,\n",
    "        'F1-Score Std': f1_std,\n",
    "        'Support': support_avg,  # Supporto medio\n",
    "        'Avg Features': num_features_avg,  # Media delle features\n",
    "    })  # Trasponi per avere le classi come righe\n",
    "\n",
    "    return df_avg, auc_avg, f1_avg_per_class, num_features_avg \n",
    "\n",
    "# Crea un oggetto StratifiedKFold per la cross-validation stratificata\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### BERT VECTORIZER ###############################\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "columns_to_vectorize = ['testo_preprocessed', 'motivo_ricovero_preprocessed', 'anamnesi_preprocessed', \n",
    "                             'esameobiettivo_preprocessed', 'terapiafarmaingresso_preprocessed', \n",
    "                             'decorso_preprocessed', 'laboratorio_preprocessed', 'interventi_preprocessed', \n",
    "                             'followup_preprocessed', 'terapie2_preprocessed', 'terapie3_preprocessed', \n",
    "                             'esami_preprocessed', 'reparto_preprocessed']\n",
    "\n",
    "X = merged_dataset[columns_to_vectorize]  # Passa l'intero dataset, con le 13 colonne\n",
    "y = merged_dataset['positivi']\n",
    "# Converti le etichette in numeri con LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)  # Trasforma le etichette in numeri\n",
    "\n",
    "# Carica il tokenizzatore e il modello di BERT\n",
    "model_name = \"dbmdz/bert-base-italian-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name, max_length=512, device=None):\n",
    "        \"\"\"\n",
    "        Inizializza il trasformatore BERT.\n",
    "        \n",
    "        :param model_name: Il nome del modello BERT pre-addestrato.\n",
    "        :param max_length: La lunghezza massima per il padding delle sequenze di input.\n",
    "        :param device: Il dispositivo da usare ('cuda' o 'cpu'). Se None, viene scelto automaticamente.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        self.device = device or ('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Metodo di fitting (non necessario per il trasformatore).\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, batch_size=32):\n",
    "        \"\"\"\n",
    "        Applica la trasformazione ai dati di input in batch.\n",
    "        \"\"\"\n",
    "        # Prepara i dati\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            texts = X.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1).tolist()\n",
    "        elif isinstance(X, pd.Series):\n",
    "            texts = X.dropna().astype(str).tolist()\n",
    "        else:\n",
    "            raise ValueError(\"ClinicalBertTransformer accetta solo input di tipo DataFrame o Series.\")\n",
    "\n",
    "        embeddings = []\n",
    "\n",
    "        # Elaborazione a batch\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            input_ids = []\n",
    "            attention_masks = []\n",
    "\n",
    "            for text in batch_texts:\n",
    "                encoding = self.tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_length,\n",
    "                    pad_to_max_length=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt',\n",
    "                )\n",
    "                input_ids.append(encoding['input_ids'])\n",
    "                attention_masks.append(encoding['attention_mask'])\n",
    "\n",
    "            input_ids = torch.cat(input_ids, dim=0).to(self.device)\n",
    "            attention_masks = torch.cat(attention_masks, dim=0).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "        # Concatena tutti i batch\n",
    "        return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### BERT VECTORIZER ###############################\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "columns_to_vectorize = ['testo_preprocessed', 'motivo_ricovero_preprocessed', 'anamnesi_preprocessed', \n",
    "                             'esameobiettivo_preprocessed', 'terapiafarmaingresso_preprocessed', \n",
    "                             'decorso_preprocessed', 'laboratorio_preprocessed', 'interventi_preprocessed', \n",
    "                             'followup_preprocessed', 'terapie2_preprocessed', 'terapie3_preprocessed', \n",
    "                             'esami_preprocessed', 'reparto_preprocessed']\n",
    "\n",
    "X = merged_dataset[columns_to_vectorize]  # Passa l'intero dataset, con le 13 colonne\n",
    "y = merged_dataset['positivi']\n",
    "# Converti le etichette in numeri con LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)  # Trasforma le etichette in numeri\n",
    "\n",
    "# Carica il tokenizzatore e il modello di BERT\n",
    "model_name = \"dbmdz/bert-base-italian-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name, max_length=512, device=None):\n",
    "        \"\"\"\n",
    "        Inizializza il trasformatore BERT.\n",
    "        \n",
    "        :param model_name: Il nome del modello BERT pre-addestrato.\n",
    "        :param max_length: La lunghezza massima per il padding delle sequenze di input.\n",
    "        :param device: Il dispositivo da usare ('cuda' o 'cpu'). Se None, viene scelto automaticamente.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        self.device = device or ('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Metodo di fitting (non necessario per il trasformatore).\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, batch_size=32):\n",
    "        \"\"\"\n",
    "        Applica la trasformazione ai dati di input in batch.\n",
    "        \"\"\"\n",
    "        # Prepara i dati\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            texts = X.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1).tolist()\n",
    "        elif isinstance(X, pd.Series):\n",
    "            texts = X.dropna().astype(str).tolist()\n",
    "        else:\n",
    "            raise ValueError(\"ClinicalBertTransformer accetta solo input di tipo DataFrame o Series.\")\n",
    "\n",
    "        embeddings = []\n",
    "\n",
    "        # Elaborazione a batch\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            input_ids = []\n",
    "            attention_masks = []\n",
    "\n",
    "            for text in batch_texts:\n",
    "                encoding = self.tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_length,\n",
    "                    pad_to_max_length=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt',\n",
    "                )\n",
    "                input_ids.append(encoding['input_ids'])\n",
    "                attention_masks.append(encoding['attention_mask'])\n",
    "\n",
    "            input_ids = torch.cat(input_ids, dim=0).to(self.device)\n",
    "            attention_masks = torch.cat(attention_masks, dim=0).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "        # Concatena tutti i batch\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "    \n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "    ('bert', BertTransformer(model_name)),  # Il tuo trasformatore basato su BERT\n",
    "    ('classificazione', clf)  # Il tuo classificatore (ad esempio Random Forest)\n",
    "])\n",
    "\n",
    "    # Fit the pipeline to the data\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Trasforma i dati e ottieni la dimensione\n",
    "    X_word2vec = pipeline.named_steps['bert'].transform(X)\n",
    "    num_samples, num_features = X_word2vec.shape\n",
    "    print(f\"Dimensione della matrice Bert: {num_samples} campioni, {num_features} features\")\n",
    "\n",
    "    # Calcola le metriche usando la funzione\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "    \n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "    \n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ CALCOLO OOV ###########################\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_oov_percentage(texts, tokenizer, global_oov_counter, global_valid_words):\n",
    "    \"\"\"\n",
    "    Calcola la percentuale di parole OOV usando il tokenizer di BERT.\n",
    "    \n",
    "    :param texts: Lista di testi (singola colonna o una lista di colonne).\n",
    "    :param tokenizer: Tokenizer pre-addestrato BERT.\n",
    "    :param global_oov_counter: Un contatore globale per le parole OOV.\n",
    "    :param global_valid_words: Un set globale di parole valide.\n",
    "    :return: Percentuale di OOV e statistiche globali.\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    oov_words = 0\n",
    "\n",
    "    for text in texts:\n",
    "        # Suddividi il testo in parole\n",
    "        words = text.split()\n",
    "        total_words += len(words)\n",
    "        \n",
    "        for word in words:\n",
    "            # Tokenizza ogni parola\n",
    "            tokens = tokenizer.tokenize(word)\n",
    "            if len(tokens) > 1 or (len(tokens) == 1 and tokens[0] == '[UNK]'):\n",
    "                # Conta come OOV se suddivisa in più token o è [UNK]\n",
    "                oov_words += 1\n",
    "                global_oov_counter[word] += 1  # Aggiungi alla conta globale degli OOV\n",
    "            else:\n",
    "                global_valid_words.add(word)  # Aggiungi alla lista delle parole valide\n",
    "    \n",
    "    # Calcola la percentuale di OOV\n",
    "    oov_percentage = (oov_words / total_words) * 100 if total_words > 0 else 0\n",
    "    return oov_words, total_words, oov_percentage\n",
    "\n",
    "# Carica il tokenizzatore e il modello di BERT\n",
    "model_name = \"dbmdz/bert-base-italian-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Contatori globali\n",
    "global_oov_counter = Counter()  # Per tenere traccia delle parole OOV globali\n",
    "global_valid_words = set()  # Per tenere traccia delle parole valide globali\n",
    "\n",
    "# Se hai un DataFrame, applica la funzione per ciascuna colonna\n",
    "all_oov_words = 0\n",
    "all_total_words = 0\n",
    "\n",
    "for col in X.columns:\n",
    "    print(f\"Calcolando OOV per la colonna: {col}\")\n",
    "    texts = X[col].dropna().astype(str).tolist()  # Assicurati che i dati siano in formato stringa\n",
    "    oov_words, total_words, oov_percentage = calculate_oov_percentage(texts, tokenizer, global_oov_counter, global_valid_words)\n",
    "    \n",
    "    # Aggiorna i contatori globali\n",
    "    all_oov_words += oov_words\n",
    "    all_total_words += total_words\n",
    "\n",
    "    print(f\"Parole OOV nella colonna '{col}': {oov_words}, Totale parole: {total_words}, Percentuale OOV: {oov_percentage:.2f}%\")\n",
    "\n",
    "# Calcolo delle statistiche globali\n",
    "total_unique_words = len(global_valid_words) + len(global_oov_counter)\n",
    "oov_percentage_global = (len(global_oov_counter) / total_unique_words) * 100 if total_unique_words > 0 else 0\n",
    "\n",
    "# Risultati globali\n",
    "print(\"\\n### Statistiche globali ###\")\n",
    "print(f\"Totale parole valide uniche: {len(global_valid_words)}\")\n",
    "print(f\"Totale parole OOV uniche: {len(global_oov_counter)}\")\n",
    "print(f\"Totale parole uniche (valide + OOV): {total_unique_words}\")\n",
    "print(f\"Percentuale globale di parole OOV: {oov_percentage_global:.2f}%\")\n",
    "\n",
    "# Stampa le prime 50 parole OOV globali\n",
    "print(\"\\nPrime 50 parole OOV globali ordinate per frequenza:\")\n",
    "for i, (word, count) in enumerate(global_oov_counter.most_common(100)):\n",
    "    print(f\"{i+1}. {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### SMOTE ###################################\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "\n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "        ('bert', BertTransformer(model_name)),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    X = merged_dataset[columns_to_vectorize]  # Passa l'intero dataset, con le 13 colonne\n",
    "    y = merged_dataset['positivi']\n",
    "\n",
    "    # Fit della pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Calcola le metriche usando la funzione (modifica per adattare alla tua funzione eval_cross_validation)\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### SMOTE + UNDERSAMPLING ###################################\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "\n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "        ('bert', BertTransformer(model_name))\n",
    "        ('smote', SMOTE(random_state=42, sampling_strategy=0.6)),\n",
    "        ('undersample', RandomUnderSampler(sampling_strategy=1.0, random_state=42)),\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    X = merged_dataset[columns_to_vectorize]  # Passa l'intero dataset, con le 13 colonne\n",
    "    y = merged_dataset['positivi']\n",
    "\n",
    "    # Fit della pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Calcola le metriche usando la funzione (modifica per adattare alla tua funzione eval_cross_validation)\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
