{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v.acampora/venv2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-07 22:18:05.083241: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-07 22:18:05.717678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### carica dataset LDO 20-21 ############################ \n",
    "\n",
    "dataLDO2020 = pd.read_excel('/home/a.renda/to_move/LDO/filtrato_per_keyword/20-21_LDO_26K/LDO_20200101_20210101 pulito.ods', engine='odf')\n",
    "dataLDO2021=pd.read_excel('/home/a.renda/to_move/LDO/filtrato_per_keyword/20-21_LDO_26K/LDO_20210101_20220101 pulito.ods', engine='odf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra le righe che non contengono numeri (escludendo anche i NaN)\n",
    "dataLDO2021 = dataLDO2021[dataLDO2021['nosologico'].astype(str).str.contains(r'\\d')]\n",
    "#rimosse 212 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns LDO 20-21: Index(['nosologico', 'time_inserimento', 'dataInizio', 'dataFine', 'testo',\n",
      "       'motivo_ricovero', 'anamnesi', 'esameobiettivo', 'terapiafarmaingresso',\n",
      "       'decorso', 'laboratorio', 'interventi', 'followup', 'terapie2',\n",
      "       'terapie3', 'esami', 'reparto'],\n",
      "      dtype='object')\n",
      "\n",
      "Columns LDO 21-22: Index(['nosologico', 'time_inserimento', 'dataInizio', 'dataFine', 'testo',\n",
      "       'motivo_ricovero', 'anamnesi', 'esameobiettivo', 'terapiafarmaingresso',\n",
      "       'decorso', 'laboratorio', 'interventi', 'followup', 'terapie2',\n",
      "       'terapie3', 'esami', 'reparto'],\n",
      "      dtype='object')\n",
      "\n",
      "Shape LDO 20-21: (24123, 17)\n",
      "\n",
      "Shape LDO 21-22: (39274, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nColumns LDO 20-21:\", dataLDO2020.columns)\n",
    "print(\"\\nColumns LDO 21-22:\", dataLDO2021.columns)\n",
    "print(\"\\nShape LDO 20-21:\", dataLDO2020.shape)\n",
    "print(\"\\nShape LDO 21-22:\", dataLDO2021.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26237, 3)\n",
      "Index(['Unnamed: 0', 'nosologico', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "######################## carica database filtrato ###############################\n",
    "\n",
    "databaseFiltrato=pd.read_csv('/home/a.renda/to_move/LDO/labeled/20-21_341/DatabaseFiltrato.csv', sep=';')\n",
    "print(databaseFiltrato.shape)\n",
    "print(databaseFiltrato.columns) # la prima colonna è solo un contatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra le righe che non contengono numeri (escludendo anche i NaN)\n",
    "databaseFiltrato = databaseFiltrato[databaseFiltrato['nosologico'].astype(str).str.contains(r'\\d')]\n",
    "\n",
    "#tolte 5 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['nosologico', 'time_inserimento', 'dataInizio', 'dataFine', 'testo',\n",
      "       'motivo_ricovero', 'anamnesi', 'esameobiettivo', 'terapiafarmaingresso',\n",
      "       'decorso', 'laboratorio', 'interventi', 'followup', 'terapie2',\n",
      "       'terapie3', 'esami', 'reparto'],\n",
      "      dtype='object')\n",
      "(8974, 17)\n"
     ]
    }
   ],
   "source": [
    "#################### filtra ldo 2020 ##########################\n",
    "\n",
    "# Converti la colonna 'nosologico' del secondo dataset in int\n",
    "databaseFiltrato['nosologico'] = pd.to_numeric(databaseFiltrato['nosologico'], errors='coerce')\n",
    "\n",
    "\n",
    "# Trova i nosologici comuni\n",
    "comuni2020 = dataLDO2020['nosologico'].isin(databaseFiltrato['nosologico'])\n",
    "\n",
    "# Filtra il primo dataset\n",
    "dataset_filtrato2020 = dataLDO2020[comuni2020]\n",
    "print(dataset_filtrato2020.columns)\n",
    "print(dataset_filtrato2020.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['nosologico', 'time_inserimento', 'dataInizio', 'dataFine', 'testo',\n",
      "       'motivo_ricovero', 'anamnesi', 'esameobiettivo', 'terapiafarmaingresso',\n",
      "       'decorso', 'laboratorio', 'interventi', 'followup', 'terapie2',\n",
      "       'terapie3', 'esami', 'reparto'],\n",
      "      dtype='object')\n",
      "(17218, 17)\n"
     ]
    }
   ],
   "source": [
    "####################### filtra ldo 2021 ###########################\n",
    "\n",
    "# Trova i nosologici comuni\n",
    "comuni2021 = dataLDO2021['nosologico'].isin(databaseFiltrato['nosologico'])\n",
    "\n",
    "# Filtra il primo dataset\n",
    "dataset_filtrato2021 = dataLDO2021[comuni2021]\n",
    "print(dataset_filtrato2021.columns)\n",
    "print(dataset_filtrato2021.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns merged dataset: Index(['nosologico', 'time_inserimento', 'dataInizio', 'dataFine', 'testo',\n",
      "       'motivo_ricovero', 'anamnesi', 'esameobiettivo', 'terapiafarmaingresso',\n",
      "       'decorso', 'laboratorio', 'interventi', 'followup', 'terapie2',\n",
      "       'terapie3', 'esami', 'reparto'],\n",
      "      dtype='object')\n",
      "\n",
      "Shape merged dataset: (26192, 17)\n",
      "ObesitÃ  di classe III complicata da ipertensione arteriosa, insulino-resistenza con ridotta tolleranza glucidica, epatomegalia steatosica, insufficienza venosa arti inferiori, ipovitaminosi D e ernia iatale,colelitiasi trattata con colecistectomia,regredita a obesitÃ  di classe I dopo intervento chirurgico di bypass gastrico (2011) al peso di 142 Kg con persistenza di ipovitaminosi D, remissione delle comorbiditÃ  e comparsa di ipoglicemia reattiva sintomatica. Micronodulia tiroidea. Anemia microcitica ipocrocromica, sideropenica. Emorroidi congeste del canale anale.Terapia consigliata alla dimissione:Pr: Bariatrifast cpS: 1 cp ore 8.00Pr: Glucobay 50 mg cpS: 1 cp a colazione 1 cp a pranzo 1 cp a cena  da aumentare a 2 cp a colazione 2 cp a pranzo e 2 cp a cena in base alla sintomatologia e fino a nuove indicazione. Pr: Sideral forte cpS: 1 cp al giorno a digiuno   \n"
     ]
    }
   ],
   "source": [
    "######################## merge ldo2020 e ldo 2021 filtrati ###################################\n",
    "\n",
    "merged_dataset = pd.concat([dataset_filtrato2020, dataset_filtrato2021], ignore_index=True) # non ci sono duplicati tra i due dataset \n",
    "\n",
    "# Risultato finale\n",
    "print(\"\\nColumns merged dataset:\",merged_dataset.columns)\n",
    "print(\"\\nShape merged dataset:\",merged_dataset.shape)\n",
    "print(merged_dataset['testo'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## carica dataset con nosologici positivi #####################\n",
    "\n",
    "Positivi= pd.read_excel('/home/a.renda/to_move/LDO/labeled/20-21_341/NosologiciPositivi_341.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conteggio dei valori nella colonna 'positivi':\n",
      "Numero di 1 (positivi): 341\n",
      "Numero di 0 (non positivi): 25851\n"
     ]
    }
   ],
   "source": [
    "################################### aggiungere la colonna positivi #########################################\n",
    "\n",
    "# Convertire la colonna 'Nosologico' in numerico nel dataset NosologiciPositivi\n",
    "nosologici_positivi = pd.to_numeric(Positivi['NosologiciPositivi'], errors='coerce').dropna()\n",
    "\n",
    "# Creare la colonna 'positivi' nel DataFrame merged_dataset\n",
    "merged_dataset['positivi'] = merged_dataset['nosologico'].isin(nosologici_positivi).astype(int)\n",
    "\n",
    "\n",
    "# Contare quanti 1 e quanti 0 ci sono nella colonna 'positivi'\n",
    "count_positivi = merged_dataset['positivi'].value_counts()\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"\\nConteggio dei valori nella colonna 'positivi':\")\n",
    "print(f\"Numero di 1 (positivi): {count_positivi.get(1, 0)}\")\n",
    "print(f\"Numero di 0 (non positivi): {count_positivi.get(0, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    UO Endocrinologia-1\n",
      "1    UO Endocrinologia-1\n",
      "2    UO Endocrinologia-1\n",
      "3    UO Endocrinologia-1\n",
      "4    UO Endocrinologia-1\n",
      "Name: reparto, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Unire parole e numeri in 'reparto' rimuovendo lo spazio e sostituendo con un trattino\n",
    "merged_dataset['reparto'] = merged_dataset['reparto'].str.replace(r'(\\w) (\\d)', r'\\1-\\2', regex=True)\n",
    "\n",
    "# Visualizza i risultati\n",
    "print(merged_dataset['reparto'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### pulisci il testo: risoluzione di errori di codifica, sostituzione caratteri speciali #######################\n",
    "import ftfy\n",
    "\n",
    "# Applica ftfy.fix_text() a tutte le colonne di testo nel dataset, gestendo i valori non testuali\n",
    "for col in merged_dataset.select_dtypes(include='object').columns:\n",
    "    merged_dataset[col] = merged_dataset[col].apply(lambda x: ftfy.fix_text(x) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   testo  \\\n",
      "0      Obesità di classe III complicata da ipertensio...   \n",
      "1      Carcinoma della tiroide a cellule ossifile (4 ...   \n",
      "2      Carcinoma papillare differenziato della tiroid...   \n",
      "3      gozzo multinodulare con quattro focolai di car...   \n",
      "4      Carcinoma papillare (1,5 cm) variante classica...   \n",
      "...                                                  ...   \n",
      "26187  Cistopessi fasciale in paziente con cistocele ...   \n",
      "26188  Polipectomia resettoscopica, miomectomia reset...   \n",
      "26189  Laparoscopia, isterectomia sovracervicale, ann...   \n",
      "26190  Polipectomia resettoscopica in paziente con po...   \n",
      "26191                                  Mioma sottomucoso   \n",
      "\n",
      "                                         motivo_ricovero  \\\n",
      "0      Accertamenti in paziente con sangue occulto po...   \n",
      "1      trattamento radiometabolico con 131-I a dose alta   \n",
      "2      Terapia radiometabolica con 131-I a scopo adiu...   \n",
      "3      terapia radiometabolica con 131-I a scopo adiu...   \n",
      "4       trattamento radiometabolico con 131I a dose alta   \n",
      "...                                                  ...   \n",
      "26187                             Cistocele di III grado   \n",
      "26188    Polipo endometriale in paziente con metrorragia   \n",
      "26189                       Cisto-isterocele di IV grado   \n",
      "26190                        Polipectomia resettoscopica   \n",
      "26191                         Miomectomia resettoscopica   \n",
      "\n",
      "                                                anamnesi  \\\n",
      "0      Paziente con crisi ipoglicemiche post chirurgi...   \n",
      "1      Il paziente è seguito per carcinoma della tiro...   \n",
      "2      Carcinoma papillare differenziato della tiroid...   \n",
      "3      gozzo multinodulare con quattro focolai di car...   \n",
      "4      carcinoma papillare (1,5 cm) variante classica...   \n",
      "...                                                  ...   \n",
      "26187                                                NaN   \n",
      "26188                                                NaN   \n",
      "26189                                                NaN   \n",
      "26190                                                NaN   \n",
      "26191                                                NaN   \n",
      "\n",
      "                                          esameobiettivo  \\\n",
      "0                                                    NaN   \n",
      "1      Condizioni generali: buone.Cute: lesioni psori...   \n",
      "2                                                    NaN   \n",
      "3      Collo: cicatrice da tiroidectomia lievemente e...   \n",
      "4      - Altezza: 1.65 m; - Peso: 52 kg; - IMC: 19,1;...   \n",
      "...                                                  ...   \n",
      "26187                                                NaN   \n",
      "26188                                                NaN   \n",
      "26189                                                NaN   \n",
      "26190                                                NaN   \n",
      "26191                                                NaN   \n",
      "\n",
      "                                    terapiafarmaingresso  \\\n",
      "0                       bariatric 1 compressa al giorno.   \n",
      "1      Eutirox 100 mcg dal lunedì al sabatoEutirox 50...   \n",
      "2                                                    NaN   \n",
      "3                       Pr:Eutirox 150 mcg/dieS:1 cp/die   \n",
      "4                               Eutirox 100 mcg 1 cp/die   \n",
      "...                                                  ...   \n",
      "26187                                                NaN   \n",
      "26188                                                NaN   \n",
      "26189                                                NaN   \n",
      "26190                                                NaN   \n",
      "26191                                                NaN   \n",
      "\n",
      "                                                 decorso laboratorio  \\\n",
      "0      Durante la degenza sono state eseguite esofago...         NaN   \n",
      "1      trattamento radiometabolico con 131-I a dose a...         NaN   \n",
      "2      Terapia radiometabolica con 131-I a scopo adiu...         NaN   \n",
      "3      terapia radiometabolica con 131-I a scopo adiu...         NaN   \n",
      "4      trattamento radiometabolico con 131I a dose al...         NaN   \n",
      "...                                                  ...         ...   \n",
      "26187  In data 15/11/2021 la paziente è stata sottopo...         NaN   \n",
      "26188  In data odierna la paziente è stata sottoposta...         NaN   \n",
      "26189  In data 22/11/2021 la paziente è stata sottopo...         NaN   \n",
      "26190  In data odierna la paziente è stata sottoposta...         NaN   \n",
      "26191  In data 29/11 la paziente è stata sottoposta a...         NaN   \n",
      "\n",
      "                                              interventi  \\\n",
      "0                                                    NaN   \n",
      "1                                                    NaN   \n",
      "2                                                    NaN   \n",
      "3                                                    NaN   \n",
      "4                                                    NaN   \n",
      "...                                                  ...   \n",
      "26187  15-NOV-21: Disinfezione di genitali esterni e ...   \n",
      "26188  19-NOV-21: In anestesia locale previa disinfez...   \n",
      "26189                                                NaN   \n",
      "26190  Disinfezione dei genitali esterni e vagina. Al...   \n",
      "26191  29-NOV-21: Disinfezione dei genitali esterni e...   \n",
      "\n",
      "                                                followup  \\\n",
      "0      Consigliamo di effettuare una visita di contro...   \n",
      "1      Ricordiamo di tornare presso la Medicina Nucle...   \n",
      "2      Abbiamo programmato una visita endocrinologica...   \n",
      "3      Ricordiamo di recarsi presso la Medicina Nucle...   \n",
      "4      Ricordiamo di recarsi presso la Medicina Nucle...   \n",
      "...                                                  ...   \n",
      "26187  Si consiglia:-Dita leggera a basso contenuto d...   \n",
      "26188  L'esito dell'esame istologico definitivo le sa...   \n",
      "26189  Si consiglia:- Medicazione a giorni alterni de...   \n",
      "26190  Indicazioni alla dimissione:- Comune antidolor...   \n",
      "26191  Indicazioni alla dimissione:- Visita di contro...   \n",
      "\n",
      "                                                terapie2  ...  \\\n",
      "0                                                    NaN  ...   \n",
      "1                                                    NaN  ...   \n",
      "2                                                    NaN  ...   \n",
      "3                                                    NaN  ...   \n",
      "4                                                    NaN  ...   \n",
      "...                                                  ...  ...   \n",
      "26187  Cefamezin-2gr-endovena-intraoperatorio-Enoxapa...  ...   \n",
      "26188                                                NaN  ...   \n",
      "26189                                                NaN  ...   \n",
      "26190                                                NaN  ...   \n",
      "26191                                                NaN  ...   \n",
      "\n",
      "                             esameobiettivo_preprocessed  \\\n",
      "0                                                          \n",
      "1      Condizioni generali buone Cute lesioni psorias...   \n",
      "2                                                          \n",
      "3      Collo cicatrice da tiroidectomia lievemente er...   \n",
      "4      Altezza 1.65 Peso 52 kg IMC 19,1 Pressione Fre...   \n",
      "...                                                  ...   \n",
      "26187                                                      \n",
      "26188                                                      \n",
      "26189                                                      \n",
      "26190                                                      \n",
      "26191                                                      \n",
      "\n",
      "                       terapiafarmaingresso_preprocessed  \\\n",
      "0                          bariatric compressa al giorno   \n",
      "1      Eutirox 100 mcg dal lunedì al sabato Eutirox 5...   \n",
      "2                                                          \n",
      "3                      Pr Eutirox 150 mcg die S:1 cp die   \n",
      "4                                 Eutirox 100 mcg cp die   \n",
      "...                                                  ...   \n",
      "26187                                                      \n",
      "26188                                                      \n",
      "26189                                                      \n",
      "26190                                                      \n",
      "26191                                                      \n",
      "\n",
      "                                    decorso_preprocessed  \\\n",
      "0      Durante la degenza sono state eseguite esofago...   \n",
      "1      trattamento radiometabolico con 131-I dose alt...   \n",
      "2      Terapia radiometabolica con 131-I scopo adiuvante   \n",
      "3      terapia radiometabolica con 131-I scopo adiuva...   \n",
      "4      trattamento radiometabolico con 131 dose alta ...   \n",
      "...                                                  ...   \n",
      "26187  In data la paziente stata sottoposta ad interv...   \n",
      "26188  In data odierna la paziente stata sottoposta p...   \n",
      "26189  In data la paziente stata sottoposta intervent...   \n",
      "26190  In data odierna la paziente stata sottoposta p...   \n",
      "26191  In data la paziente stata sottoposta miomectom...   \n",
      "\n",
      "      laboratorio_preprocessed  \\\n",
      "0                                \n",
      "1                                \n",
      "2                                \n",
      "3                                \n",
      "4                                \n",
      "...                        ...   \n",
      "26187                            \n",
      "26188                            \n",
      "26189                            \n",
      "26190                            \n",
      "26191                            \n",
      "\n",
      "                                 interventi_preprocessed  \\\n",
      "0                                                          \n",
      "1                                                          \n",
      "2                                                          \n",
      "3                                                          \n",
      "4                                                          \n",
      "...                                                  ...   \n",
      "26187  15-NOV-21 Disinfezione di genitali esterni vag...   \n",
      "26188  19-NOV-21 In anestesia locale previa disinfezi...   \n",
      "26189                                                      \n",
      "26190  Disinfezione dei genitali esterni vagina Alles...   \n",
      "26191  29-NOV-21 Disinfezione dei genitali esterni va...   \n",
      "\n",
      "                                   followup_preprocessed  \\\n",
      "0      Consigliamo di effettuare una visita di contro...   \n",
      "1      Ricordiamo di tornare presso la Medicina Nucle...   \n",
      "2      Abbiamo programmato una visita endocrinologica...   \n",
      "3      Ricordiamo di recarsi presso la Medicina Nucle...   \n",
      "4      Ricordiamo di recarsi presso la Medicina Nucle...   \n",
      "...                                                  ...   \n",
      "26187  Si consiglia:-Dita leggera basso contenuto di ...   \n",
      "26188  L' esito dell' esame istologico definitivo le ...   \n",
      "26189  Si consiglia:- Medicazione giorni alterni dell...   \n",
      "26190  Indicazioni alla dimissione:- Comune antidolor...   \n",
      "26191  Indicazioni alla dimissione:- Visita di contro...   \n",
      "\n",
      "                                   terapie2_preprocessed  \\\n",
      "0                                                          \n",
      "1                                                          \n",
      "2                                                          \n",
      "3                                                          \n",
      "4                                                          \n",
      "...                                                  ...   \n",
      "26187  Cefamezin-2 gr endovena intraoperatorio-Enoxap...   \n",
      "26188                                                      \n",
      "26189                                                      \n",
      "26190                                                      \n",
      "26191                                                      \n",
      "\n",
      "                                   terapie3_preprocessed  \\\n",
      "0                                                          \n",
      "1      EUTIROX*50 cpr 100 mcg-100 mcg die-1 cp die da...   \n",
      "2                                                          \n",
      "3      EUTIROX*50 cpr 175 mcg-175 mcg die-1 cp die al...   \n",
      "4          EUTIROX*50 cpr 125 mcg-125 mcg die-1 cp die--   \n",
      "...                                                  ...   \n",
      "26187  Enoxaparina-2000 UI sottocute-1 fiala al giorn...   \n",
      "26188  Comune antidolorifico al bisogno Tachipirina 1...   \n",
      "26189  Comune antidolorifico al bisogno Tachipirina 1...   \n",
      "26190                                                      \n",
      "26191  Comune antidolorifico al bisogno ad esempio pa...   \n",
      "\n",
      "                                      esami_preprocessed  \\\n",
      "0                                                          \n",
      "1                                                          \n",
      "2                                                          \n",
      "3                                                          \n",
      "4                                                          \n",
      "...                                                  ...   \n",
      "26187  100108 Emocromo-Emocromo Piastrine 28710 micro...   \n",
      "26188                                                      \n",
      "26189  100108 Emocromo-Emocromo Piastrine 26210 micro...   \n",
      "26190                                                      \n",
      "26191                                                      \n",
      "\n",
      "                          reparto_preprocessed  \n",
      "0                          UO Endocrinologia-1  \n",
      "1                          UO Endocrinologia-1  \n",
      "2                          UO Endocrinologia-1  \n",
      "3                          UO Endocrinologia-1  \n",
      "4                          UO Endocrinologia-1  \n",
      "...                                        ...  \n",
      "26187  UO Ostetricia Ginecologia-1 Pensionanti  \n",
      "26188  UO Ostetricia Ginecologia-1 Pensionanti  \n",
      "26189  UO Ostetricia Ginecologia-1 Pensionanti  \n",
      "26190  UO Ostetricia Ginecologia-1 Pensionanti  \n",
      "26191  UO Ostetricia Ginecologia-1 Pensionanti  \n",
      "\n",
      "[26192 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "########################## PRE-PROCESSING ##########################\n",
    "############# valutare se il preprocessing deve essere modificato #######################\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Carica il modello SpaCy per l'italiano\n",
    "nlp = spacy.load(\"it_core_news_lg\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Lista delle colonne su cui applicare il preprocessing\n",
    "colonne_da_preprocessare = ['testo', 'motivo_ricovero', 'anamnesi', \n",
    "                             'esameobiettivo', 'terapiafarmaingresso', \n",
    "                             'decorso', 'laboratorio', 'interventi', \n",
    "                             'followup', 'terapie2', 'terapie3', \n",
    "                             'esami', 'reparto']  # Sostituisci con i nomi reali delle colonne\n",
    "\n",
    "def clean_token(token):\n",
    "\n",
    "    # Rimuovi caratteri non alfanumerici (inclusi simboli, punteggiatura, numeri, ecc.) e sostituiscili con uno spazio\n",
    "    #cleaned_token = re.sub(r'[^a-zA-ZàèéìòùÀÈÉÌÒÙ]', ' ', token)\n",
    "    cleaned_token = re.sub(r'\\s+', ' ', token).strip()  # Normalizza gli spazi multipli\n",
    "\n",
    "    return cleaned_token\n",
    "\n",
    "    \n",
    "# Funzione per tokenizzare e preprocessare il testo\n",
    "def preprocess_text(row):\n",
    "    if not isinstance(row, str):  # Verifica che il dato sia una stringa valida\n",
    "        return \"\"  # Restituisce una stringa vuota se non valido\n",
    "    \n",
    "    # Rimuove date nel formato 'dd/mm/yyyy' e 'dd/mm/yy'\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', '', row)\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}', '', row)  # Rimuove numeri separati da /\n",
    "      \n",
    "    # Aggiunge spazi tra numeri e lettere (es. \"800duloxetina\" -> \"800 duloxetina\")\n",
    "    row = re.sub(r'(\\d+)([a-zA-Z]+)', r'\\1 \\2', row)\n",
    "    row = re.sub(r'([a-zA-Z]+)(\\d+)', r'\\1 \\2', row)\n",
    "\n",
    "    # Aggiunge spazi tra parole composte tipo \"vediAllegato\" -> \"vedi Allegato\"\n",
    "    row = re.sub(r'([a-zàèéìòù])([A-ZÀÈÉÌÒÙ])', r'\\1 \\2', row)\n",
    "\n",
    "    # normalizza  spazi (se ci sono più spazi consecutivi, vengono ridotti a uno solo) e rimuove  spazi all'inizio e alla fine della stringa\n",
    "    #row = re.sub(r'[^\\w\\s]', ' ', row)  # Rimuove caratteri non alfanumerici e parentesi\n",
    "\n",
    "    doc = nlp(row)\n",
    "   \n",
    "    # Filtra e normalizza i token\n",
    "    tokens_puliti = [\n",
    "        clean_token(token.text)\n",
    "        for token in doc\n",
    "        #if not token.is_punct and not token.is_stop and len(token.text) > 1 #and token.text.isalpha() \n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Applica un filtro finale per rimuovere manualmente le lettere singole\n",
    "    tokens_puliti = [token for token in tokens_puliti if len(token) > 1]  # Assicura che tutte le parole siano > 1 carattere\n",
    "\n",
    "    # Ricombina i token in una stringa\n",
    "    return \" \".join(tokens_puliti)\n",
    "\n",
    "# Applica il preprocessing per ciascuna colonna specificata\n",
    "for colonna in colonne_da_preprocessare:\n",
    "    nuova_colonna = f\"{colonna}_preprocessed\"  # Nome della nuova colonna\n",
    "    merged_dataset[nuova_colonna] = merged_dataset[colonna].apply(preprocess_text)\n",
    "\n",
    "# Visualizza i risultati per le nuove colonne preprocessate\n",
    "print(merged_dataset[[colonna for colonna in colonne_da_preprocessare] + \n",
    "                     [f\"{col}_preprocessed\" for col in colonne_da_preprocessare]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensione del vocabolario: 494115\n"
     ]
    }
   ],
   "source": [
    "############################ DIMENSIONE VOCABOLARIO ############################\n",
    "# Calcola il vocabolario\n",
    "vocab_set = set()\n",
    "# Itera sulle prime 100 righe delle colonne preprocessate\n",
    "for colonna in colonne_da_preprocessare:\n",
    "    for testo in merged_dataset[f\"{colonna}_preprocessed\"]:  # Limita a 100 righe\n",
    "        if isinstance(testo, str):  # Assicurati che il testo non sia NaN\n",
    "            vocab_set.update(testo.split())\n",
    "\n",
    "# Calcola la dimensione del vocabolario\n",
    "vocabolario_dimensione = len(vocab_set)\n",
    "print(\"Dimensione del vocabolario:\", vocabolario_dimensione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## CLASSIFIERS #################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Definisci una lista di classificatori che vuoi provare\n",
    "classifiers = {\n",
    "    #'RandomForest': RandomForestClassifier(n_jobs=-1, max_depth=10, max_features=0.1, random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=3000),\n",
    "    #'SVM': SVC(probability=True,random_state=42),\n",
    "    #'KNeighbors': KNeighborsClassifier(),\n",
    "    #'DecisionTree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    #'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    #'extremeGradientBoosting': XGBClassifier( learning_rate=0.1, random_state=42, n_jobs=-1, max_depth=6),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### CROSS VALIDATION #########################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "\n",
    "\n",
    "# Funzione modificata per includere la media delle features\n",
    "def eval_cross_validation(pipeline, X, y, skf):\n",
    "    list_reports = []\n",
    "    list_f1 = []\n",
    "    list_auc = []  # Lista per raccogliere i valori di AUC mediati\n",
    "    list_num_features = []  # Lista per raccogliere il numero di features per ogni fold\n",
    "    \n",
    "    # Crea una lista per raccogliere precision, recall e f1-score mediati\n",
    "    precision_sum = {}\n",
    "    recall_sum = {}\n",
    "    f1_sum = {}\n",
    "    \n",
    "    # Liste per la deviazione standard\n",
    "    precision_values = {}\n",
    "    recall_values = {}\n",
    "    f1_values = {}\n",
    "\n",
    "    # Inizializza il supporto per ogni classe\n",
    "    unique_labels = np.unique(y)  # Usa np.unique per gli array NumPy\n",
    "    support_sum = {str(label): 0 for label in unique_labels}  # Assicurati che le etichette siano stringhe\n",
    "\n",
    "    for train, val in skf.split(X, y):\n",
    "        X_tr = X.iloc[train]  # differenza con 'baseline1'\n",
    "        y_tr = y[train]  # Modifica per lavorare con l'array NumPy\n",
    "        X_val = X.iloc[val]\n",
    "        y_val = y[val]  # Modifica per lavorare con l'array NumPy\n",
    "\n",
    "        # Addestra il pipeline sul training set\n",
    "        pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "        # Previsioni sul validation set\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        y_pred_prob = pipeline.predict_proba(X_val)[:, 1]  # Probabilità della classe positiva\n",
    "\n",
    "        # Crea il classification report come dizionario\n",
    "        cr = classification_report(y_val, y_pred, output_dict=True)\n",
    "\n",
    "        # Aggiungi il report alla lista\n",
    "        list_reports.append(cr)\n",
    "\n",
    "        # Estrai il F1-score\n",
    "        list_f1.append(cr['weighted avg']['f1-score'])\n",
    "\n",
    "        # Calcola l'AUC per questo fold\n",
    "        auc_score = roc_auc_score(y_val, y_pred_prob)\n",
    "        list_auc.append(auc_score)\n",
    "\n",
    "        # Raccogli il numero di features per questo fold\n",
    "        X_word2vec = pipeline.named_steps['bert'].transform(X_val)  # Modificato per estrarre la trasformazione\n",
    "        list_num_features.append(X_word2vec.shape[1])  # Numero di features per questo fold\n",
    "\n",
    "        # Somma le metriche per ogni classe\n",
    "        for label, metrics in cr.items():\n",
    "            if label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                precision_sum[label] = precision_sum.get(label, 0) + metrics['precision']\n",
    "                recall_sum[label] = recall_sum.get(label, 0) + metrics['recall']\n",
    "                f1_sum[label] = f1_sum.get(label, 0) + metrics['f1-score']\n",
    "\n",
    "                # Aggiungi valori per la deviazione standard\n",
    "                precision_values[label] = precision_values.get(label, []) + [metrics['precision']]\n",
    "                recall_values[label] = recall_values.get(label, []) + [metrics['recall']]\n",
    "                f1_values[label] = f1_values.get(label, []) + [metrics['f1-score']]\n",
    "\n",
    "                # Somma il supporto per questa fold\n",
    "                support_sum[str(label)] += metrics['support']  # Usa str(label) per garantire la corrispondenza\n",
    "\n",
    "    # Calcola la media dell'AUC\n",
    "    auc_avg = np.mean(list_auc)\n",
    "\n",
    "    # Calcola la media del numero di features\n",
    "    num_features_avg = np.mean(list_num_features)\n",
    "\n",
    "    # Calcola la media delle metriche per ogni classe\n",
    "    num_folds = skf.get_n_splits()\n",
    "    precision_avg = {label: precision_sum[label] / num_folds for label in precision_sum}\n",
    "    recall_avg = {label: recall_sum[label] / num_folds for label in recall_sum}\n",
    "    f1_avg_per_class = {label: f1_sum[label] / num_folds for label in f1_sum}\n",
    "\n",
    "    # Calcola la deviazione standard per ogni metrica\n",
    "    precision_std = {label: np.std(precision_values[label]) for label in precision_values}\n",
    "    recall_std = {label: np.std(recall_values[label]) for label in recall_values}\n",
    "    f1_std = {label: np.std(f1_values[label]) for label in f1_values}\n",
    "\n",
    "    # Calcola il supporto medio per ciascuna classe\n",
    "    support_avg = {label: support_sum[label] / num_folds for label in support_sum}\n",
    "\n",
    "    # Crea un DataFrame per visualizzare le metriche\n",
    "    df_avg = pd.DataFrame({\n",
    "        'Precision': precision_avg,\n",
    "        'Recall': recall_avg,\n",
    "        'F1-Score': f1_avg_per_class,\n",
    "        'Precision Std': precision_std,\n",
    "        'Recall Std': recall_std,\n",
    "        'F1-Score Std': f1_std,\n",
    "        'Support': support_avg,  # Supporto medio\n",
    "        'Avg Features': num_features_avg,  # Media delle features\n",
    "    })  # Trasponi per avere le classi come righe\n",
    "\n",
    "    return df_avg, auc_avg, f1_avg_per_class, num_features_avg \n",
    "\n",
    "# Crea un oggetto StratifiedKFold per la cross-validation stratificata\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### BERT VECTORIZER ###############################\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "columns_to_vectorize = ['testo_preprocessed', 'motivo_ricovero_preprocessed', 'anamnesi_preprocessed', \n",
    "                             'esameobiettivo_preprocessed', 'terapiafarmaingresso_preprocessed', \n",
    "                             'decorso_preprocessed', 'laboratorio_preprocessed', 'interventi_preprocessed', \n",
    "                             'followup_preprocessed', 'terapie2_preprocessed', 'terapie3_preprocessed', \n",
    "                             'esami_preprocessed', 'reparto_preprocessed']\n",
    "\n",
    "X = merged_dataset[columns_to_vectorize]  # Passa l'intero dataset, con le 13 colonne\n",
    "y = merged_dataset['positivi']\n",
    "# Converti le etichette in numeri con LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)  # Trasforma le etichette in numeri\n",
    "\n",
    "# Carica il tokenizzatore e il modello di BERT\n",
    "model_name = \"dbmdz/bert-base-italian-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name, max_length=512, device=None):\n",
    "        \"\"\"\n",
    "        Inizializza il trasformatore BERT.\n",
    "        \n",
    "        :param model_name: Il nome del modello BERT pre-addestrato.\n",
    "        :param max_length: La lunghezza massima per il padding delle sequenze di input.\n",
    "        :param device: Il dispositivo da usare ('cuda' o 'cpu'). Se None, viene scelto automaticamente.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        self.device = device or ('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Metodo di fitting (non necessario per il trasformatore).\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, batch_size=32):\n",
    "        \"\"\"\n",
    "        Applica la trasformazione ai dati di input in batch.\n",
    "        \"\"\"\n",
    "        # Prepara i dati\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            texts = X.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1).tolist()\n",
    "        elif isinstance(X, pd.Series):\n",
    "            texts = X.dropna().astype(str).tolist()\n",
    "        else:\n",
    "            raise ValueError(\"ClinicalBertTransformer accetta solo input di tipo DataFrame o Series.\")\n",
    "\n",
    "        embeddings = []\n",
    "\n",
    "        # Elaborazione a batch\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            input_ids = []\n",
    "            attention_masks = []\n",
    "\n",
    "            for text in batch_texts:\n",
    "                encoding = self.tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_length,\n",
    "                    pad_to_max_length=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt',\n",
    "                )\n",
    "                input_ids.append(encoding['input_ids'])\n",
    "                attention_masks.append(encoding['attention_mask'])\n",
    "\n",
    "            input_ids = torch.cat(input_ids, dim=0).to(self.device)\n",
    "            attention_masks = torch.cat(attention_masks, dim=0).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "        # Concatena tutti i batch\n",
    "        return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Modello: LogisticRegression ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensione della matrice Bert: 26192 campioni, 768 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report mediato:\n",
      "   Precision    Recall  F1-Score  Precision Std  Recall Std  F1-Score Std  \\\n",
      "0   0.987203  0.999652  0.993388       0.000345    0.000321      0.000153   \n",
      "1   0.193333  0.017563  0.031751       0.244404    0.026921      0.047453   \n",
      "\n",
      "   Support  Avg Features  \n",
      "0   2585.1         768.0  \n",
      "1     34.1         768.0  \n",
      "AUC medio: 0.7646\n",
      "Numero medio di features: 768.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############################### BERT VECTORIZER ###############################\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "columns_to_vectorize = ['testo_preprocessed', 'motivo_ricovero_preprocessed', 'anamnesi_preprocessed', \n",
    "                             'esameobiettivo_preprocessed', 'terapiafarmaingresso_preprocessed', \n",
    "                             'decorso_preprocessed', 'laboratorio_preprocessed', 'interventi_preprocessed', \n",
    "                             'followup_preprocessed', 'terapie2_preprocessed', 'terapie3_preprocessed', \n",
    "                             'esami_preprocessed', 'reparto_preprocessed']\n",
    "\n",
    "X = merged_dataset[columns_to_vectorize]  # Passa l'intero dataset, con le 13 colonne\n",
    "y = merged_dataset['positivi']\n",
    "# Converti le etichette in numeri con LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)  # Trasforma le etichette in numeri\n",
    "\n",
    "# Carica il tokenizzatore e il modello di BERT\n",
    "model_name = \"dbmdz/bert-base-italian-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "class BertTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name, max_length=512, device=None):\n",
    "        \"\"\"\n",
    "        Inizializza il trasformatore BERT.\n",
    "        \n",
    "        :param model_name: Il nome del modello BERT pre-addestrato.\n",
    "        :param max_length: La lunghezza massima per il padding delle sequenze di input.\n",
    "        :param device: Il dispositivo da usare ('cuda' o 'cpu'). Se None, viene scelto automaticamente.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name \n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        self.device = device or ('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Metodo di fitting (non necessario per il trasformatore).\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, batch_size=32):\n",
    "        \"\"\"\n",
    "        Applica la trasformazione ai dati di input in batch.\n",
    "        \"\"\"\n",
    "        # Prepara i dati\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            texts = X.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1).tolist()\n",
    "        elif isinstance(X, pd.Series):\n",
    "            texts = X.dropna().astype(str).tolist()\n",
    "        else:\n",
    "            raise ValueError(\"ClinicalBertTransformer accetta solo input di tipo DataFrame o Series.\")\n",
    "\n",
    "        embeddings = []\n",
    "\n",
    "        # Elaborazione a batch\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            input_ids = []\n",
    "            attention_masks = []\n",
    "\n",
    "            for text in batch_texts:\n",
    "                encoding = self.tokenizer.encode_plus(\n",
    "                    text,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_length,\n",
    "                    pad_to_max_length=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt',\n",
    "                )\n",
    "                input_ids.append(encoding['input_ids'])\n",
    "                attention_masks.append(encoding['attention_mask'])\n",
    "\n",
    "            input_ids = torch.cat(input_ids, dim=0).to(self.device)\n",
    "            attention_masks = torch.cat(attention_masks, dim=0).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids, attention_mask=attention_masks)\n",
    "\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "        # Concatena tutti i batch\n",
    "        return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "    \n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "    ('bert', BertTransformer(model_name)),  # Il tuo trasformatore basato su BERT\n",
    "    ('classificazione', clf)  # Il tuo classificatore (ad esempio Random Forest)\n",
    "])\n",
    "\n",
    "    # Fit the pipeline to the data\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Trasforma i dati e ottieni la dimensione\n",
    "    X_word2vec = pipeline.named_steps['bert'].transform(X)\n",
    "    num_samples, num_features = X_word2vec.shape\n",
    "    print(f\"Dimensione della matrice Bert: {num_samples} campioni, {num_features} features\")\n",
    "\n",
    "    # Calcola le metriche usando la funzione\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "    \n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "    \n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolando OOV per la colonna: testo_preprocessed\n",
      "Parole OOV nella colonna 'testo_preprocessed': 343325, Totale parole: 676517, Percentuale OOV: 50.75%\n",
      "Calcolando OOV per la colonna: motivo_ricovero_preprocessed\n",
      "Parole OOV nella colonna 'motivo_ricovero_preprocessed': 141497, Totale parole: 325730, Percentuale OOV: 43.44%\n",
      "Calcolando OOV per la colonna: anamnesi_preprocessed\n",
      "Parole OOV nella colonna 'anamnesi_preprocessed': 2773833, Totale parole: 7025366, Percentuale OOV: 39.48%\n",
      "Calcolando OOV per la colonna: esameobiettivo_preprocessed\n",
      "Parole OOV nella colonna 'esameobiettivo_preprocessed': 557349, Totale parole: 1216145, Percentuale OOV: 45.83%\n",
      "Calcolando OOV per la colonna: terapiafarmaingresso_preprocessed\n",
      "Parole OOV nella colonna 'terapiafarmaingresso_preprocessed': 181218, Totale parole: 415903, Percentuale OOV: 43.57%\n",
      "Calcolando OOV per la colonna: decorso_preprocessed\n",
      "Parole OOV nella colonna 'decorso_preprocessed': 1787938, Totale parole: 5875979, Percentuale OOV: 30.43%\n",
      "Calcolando OOV per la colonna: laboratorio_preprocessed\n",
      "Parole OOV nella colonna 'laboratorio_preprocessed': 2022365, Totale parole: 4729903, Percentuale OOV: 42.76%\n",
      "Calcolando OOV per la colonna: interventi_preprocessed\n",
      "Parole OOV nella colonna 'interventi_preprocessed': 663486, Totale parole: 1662726, Percentuale OOV: 39.90%\n",
      "Calcolando OOV per la colonna: followup_preprocessed\n",
      "Parole OOV nella colonna 'followup_preprocessed': 902605, Totale parole: 3176319, Percentuale OOV: 28.42%\n",
      "Calcolando OOV per la colonna: terapie2_preprocessed\n",
      "Parole OOV nella colonna 'terapie2_preprocessed': 42473, Totale parole: 74312, Percentuale OOV: 57.15%\n",
      "Calcolando OOV per la colonna: terapie3_preprocessed\n",
      "Parole OOV nella colonna 'terapie3_preprocessed': 656935, Totale parole: 1318060, Percentuale OOV: 49.84%\n",
      "Calcolando OOV per la colonna: esami_preprocessed\n",
      "Parole OOV nella colonna 'esami_preprocessed': 455369, Totale parole: 760448, Percentuale OOV: 59.88%\n",
      "Calcolando OOV per la colonna: reparto_preprocessed\n",
      "Parole OOV nella colonna 'reparto_preprocessed': 73719, Totale parole: 86563, Percentuale OOV: 85.16%\n",
      "\n",
      "### Statistiche globali ###\n",
      "Totale parole valide uniche: 12505\n",
      "Totale parole OOV uniche: 481610\n",
      "Totale parole uniche (valide + OOV): 494115\n",
      "Percentuale globale di parole OOV: 97.47%\n",
      "\n",
      "Prime 50 parole OOV globali ordinate per frequenza:\n",
      "1. cp: 120192\n",
      "2. dell': 102322\n",
      "3. ):: 84316\n",
      "4. cpr: 78531\n",
      "5. l': 75141\n",
      "6. 2021: 57123\n",
      "7. TC: 51145\n",
      "8. mg-1: 48286\n",
      "9. all': 44126\n",
      "10. dx: 37677\n",
      "11. Hg: 37331\n",
      "12. dimissione: 33699\n",
      "13. UO: 32819\n",
      "14. addome: 32589\n",
      "15. ricovero: 30562\n",
      "16. Paziente: 24552\n",
      "17. PCR: 23957\n",
      "18. degenza: 23697\n",
      "19. vigile: 23336\n",
      "20. Emocromo-Emocromo: 22198\n",
      "21. arteriosa: 21073\n",
      "22. bilateralmente: 20389\n",
      "23. chirurgica: 19588\n",
      "24. Hb: 19311\n",
      "25. All': 18737\n",
      "26. trattabile: 18642\n",
      "27. mmol: 18613\n",
      "28. antibiotica: 18561\n",
      "29. gtt: 18471\n",
      "30. ECG: 18344\n",
      "31. ematochimici: 16837\n",
      "32. pleurico: 16819\n",
      "33. d': 16750\n",
      "34. ev: 16735\n",
      "35. dolente: 16610\n",
      "36. Addome: 16408\n",
      "37. HCO: 15939\n",
      "38. arto: 15872\n",
      "39. ng: 15857\n",
      "40. eseguiva: 15522\n",
      "41. dolorabile: 15421\n",
      "42. sn: 15404\n",
      "43. lesione: 15121\n",
      "44. arteria: 14948\n",
      "45. RX: 14856\n",
      "46. collaborante: 14824\n",
      "47. sx: 14626\n",
      "48. Eq: 14557\n",
      "49. EGA: 14510\n",
      "50. ventricolare: 14294\n",
      "51. L': 14223\n",
      "52. aortica: 14217\n",
      "53. normofrequente: 14192\n",
      "54. bpm: 14097\n",
      "55. emocromo: 13962\n",
      "56. rivalutazione: 13746\n",
      "57. stenosi: 13608\n",
      "58. apiretica: 13534\n",
      "59. mdc: 13307\n",
      "60. apiretico: 13204\n",
      "61. operatorio: 13183\n",
      "62. decorso: 13115\n",
      "63. Esame: 12720\n",
      "64. gg: 12648\n",
      "65. --: 12557\n",
      "66. gastrores: 12550\n",
      "67. Veniva: 12095\n",
      "68. mcg: 12077\n",
      "69. accertamenti: 11971\n",
      "70. V-2: 11953\n",
      "71. pregressa: 11941\n",
      "72. domiciliare: 11918\n",
      "73. palpazione: 11868\n",
      "74. SARS-Co: 11669\n",
      "75. 3-: 11575\n",
      "76. edemi: 11474\n",
      "77. somministrazione: 11409\n",
      "78. drenaggio: 11406\n",
      "79. Ig: 11359\n",
      "80. flogosi: 11256\n",
      "81. polmonari: 11243\n",
      "82. Rx: 11185\n",
      "83. ipertensione: 11113\n",
      "84. cc: 10959\n",
      "85. ambulatoriale: 10806\n",
      "86. E': 10758\n",
      "87. dall': 10680\n",
      "88. U.O.: 10573\n",
      "89. creatinina: 10558\n",
      "90. Consulenza: 10496\n",
      "91. impegnativa: 10487\n",
      "92. follow: 10465\n",
      "93. vascolare: 10291\n",
      "94. referto: 10276\n",
      "95. istologico: 10275\n",
      "96. cps: 10249\n",
      "97. distale: 10215\n",
      "98. ischemica: 10139\n",
      "99. declivi: 9982\n",
      "100. ispessimento: 9956\n"
     ]
    }
   ],
   "source": [
    "############################ CALCOLO OOV ###########################\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_oov_percentage(texts, tokenizer, global_oov_counter, global_valid_words):\n",
    "    \"\"\"\n",
    "    Calcola la percentuale di parole OOV usando il tokenizer di BERT.\n",
    "    \n",
    "    :param texts: Lista di testi (singola colonna o una lista di colonne).\n",
    "    :param tokenizer: Tokenizer pre-addestrato BERT.\n",
    "    :param global_oov_counter: Un contatore globale per le parole OOV.\n",
    "    :param global_valid_words: Un set globale di parole valide.\n",
    "    :return: Percentuale di OOV e statistiche globali.\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    oov_words = 0\n",
    "\n",
    "    for text in texts:\n",
    "        # Suddividi il testo in parole\n",
    "        words = text.split()\n",
    "        total_words += len(words)\n",
    "        \n",
    "        for word in words:\n",
    "            # Tokenizza ogni parola\n",
    "            tokens = tokenizer.tokenize(word)\n",
    "            if len(tokens) > 1 or (len(tokens) == 1 and tokens[0] == '[UNK]'):\n",
    "                # Conta come OOV se suddivisa in più token o è [UNK]\n",
    "                oov_words += 1\n",
    "                global_oov_counter[word] += 1  # Aggiungi alla conta globale degli OOV\n",
    "            else:\n",
    "                global_valid_words.add(word)  # Aggiungi alla lista delle parole valide\n",
    "    \n",
    "    # Calcola la percentuale di OOV\n",
    "    oov_percentage = (oov_words / total_words) * 100 if total_words > 0 else 0\n",
    "    return oov_words, total_words, oov_percentage\n",
    "\n",
    "# Carica il tokenizzatore e il modello di BERT\n",
    "model_name = \"dbmdz/bert-base-italian-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Contatori globali\n",
    "global_oov_counter = Counter()  # Per tenere traccia delle parole OOV globali\n",
    "global_valid_words = set()  # Per tenere traccia delle parole valide globali\n",
    "\n",
    "# Se hai un DataFrame, applica la funzione per ciascuna colonna\n",
    "all_oov_words = 0\n",
    "all_total_words = 0\n",
    "\n",
    "for col in X.columns:\n",
    "    print(f\"Calcolando OOV per la colonna: {col}\")\n",
    "    texts = X[col].dropna().astype(str).tolist()  # Assicurati che i dati siano in formato stringa\n",
    "    oov_words, total_words, oov_percentage = calculate_oov_percentage(texts, tokenizer, global_oov_counter, global_valid_words)\n",
    "    \n",
    "    # Aggiorna i contatori globali\n",
    "    all_oov_words += oov_words\n",
    "    all_total_words += total_words\n",
    "\n",
    "    print(f\"Parole OOV nella colonna '{col}': {oov_words}, Totale parole: {total_words}, Percentuale OOV: {oov_percentage:.2f}%\")\n",
    "\n",
    "# Calcolo delle statistiche globali\n",
    "total_unique_words = len(global_valid_words) + len(global_oov_counter)\n",
    "oov_percentage_global = (len(global_oov_counter) / total_unique_words) * 100 if total_unique_words > 0 else 0\n",
    "\n",
    "# Risultati globali\n",
    "print(\"\\n### Statistiche globali ###\")\n",
    "print(f\"Totale parole valide uniche: {len(global_valid_words)}\")\n",
    "print(f\"Totale parole OOV uniche: {len(global_oov_counter)}\")\n",
    "print(f\"Totale parole uniche (valide + OOV): {total_unique_words}\")\n",
    "print(f\"Percentuale globale di parole OOV: {oov_percentage_global:.2f}%\")\n",
    "\n",
    "# Stampa le prime 50 parole OOV globali\n",
    "print(\"\\nPrime 50 parole OOV globali ordinate per frequenza:\")\n",
    "for i, (word, count) in enumerate(global_oov_counter.most_common(100)):\n",
    "    print(f\"{i+1}. {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Modello: LogisticRegression ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/v.acampora/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#################################### SMOTE ###################################\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "\n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "        ('bert', BertTransformer(model_name)),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    X = merged_dataset[columns_to_vectorize]  # Passa l'intero dataset, con le 13 colonne\n",
    "    y = merged_dataset['positivi']\n",
    "\n",
    "    # Fit della pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Calcola le metriche usando la funzione (modifica per adattare alla tua funzione eval_cross_validation)\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### SMOTE + UNDERSAMPLING ###################################\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "\n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "        ('bert', BertTransformer(model_name))\n",
    "        ('smote', SMOTE(random_state=42, sampling_strategy=0.6)),\n",
    "        ('undersample', RandomUnderSampler(sampling_strategy=1.0, random_state=42)),\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    X = merged_dataset[columns_to_vectorize]  # Passa l'intero dataset, con le 13 colonne\n",
    "    y = merged_dataset['positivi']\n",
    "\n",
    "    # Fit della pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Calcola le metriche usando la funzione (modifica per adattare alla tua funzione eval_cross_validation)\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
