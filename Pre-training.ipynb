{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### carica dataset LDO 20-21 ############################ \n",
    "\n",
    "dataLDO2020 = pd.read_excel('/home/a.renda/to_move/LDO/filtrato_per_keyword/20-21_LDO_26K/LDO_20200101_20210101 pulito.ods', engine='odf')\n",
    "dataLDO2021=pd.read_excel('/home/a.renda/to_move/LDO/filtrato_per_keyword/20-21_LDO_26K/LDO_20210101_20220101 pulito.ods', engine='odf')\n",
    "\n",
    "# Filtra le righe che non contengono numeri (escludendo anche i NaN)\n",
    "dataLDO2021 = dataLDO2021[dataLDO2021['nosologico'].astype(str).str.contains(r'\\d')]\n",
    "#rimosse 212 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nColumns LDO 20-21:\", dataLDO2020.columns)\n",
    "print(\"\\nColumns LDO 21-22:\", dataLDO2021.columns)\n",
    "print(\"\\nShape LDO 20-21:\", dataLDO2020.shape)\n",
    "print(\"\\nShape LDO 21-22:\", dataLDO2021.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## carica database filtrato ###############################\n",
    "\n",
    "databaseFiltrato=pd.read_csv('/home/a.renda/to_move/LDO/labeled/20-21_341/DatabaseFiltrato.csv', sep=';')\n",
    "print(databaseFiltrato.shape)\n",
    "print(databaseFiltrato.columns) # la prima colonna è solo un contatore\n",
    "\n",
    "# Filtra le righe che non contengono numeri (escludendo anche i NaN)\n",
    "databaseFiltrato = databaseFiltrato[databaseFiltrato['nosologico'].astype(str).str.contains(r'\\d')]\n",
    "\n",
    "#tolte 5 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### filtra ldo 2020 ##########################\n",
    "\n",
    "# Converti la colonna 'nosologico' del secondo dataset in int\n",
    "databaseFiltrato['nosologico'] = pd.to_numeric(databaseFiltrato['nosologico'], errors='coerce')\n",
    "\n",
    "\n",
    "# Trova i nosologici comuni\n",
    "comuni2020 = dataLDO2020['nosologico'].isin(databaseFiltrato['nosologico'])\n",
    "\n",
    "# Filtra il primo dataset\n",
    "dataset_filtrato2020 = dataLDO2020[comuni2020]\n",
    "print(dataset_filtrato2020.columns)\n",
    "print(dataset_filtrato2020.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### filtra ldo 2021 ###########################\n",
    "\n",
    "# Trova i nosologici comuni\n",
    "comuni2021 = dataLDO2021['nosologico'].isin(databaseFiltrato['nosologico'])\n",
    "\n",
    "# Filtra il primo dataset\n",
    "dataset_filtrato2021 = dataLDO2021[comuni2021]\n",
    "print(dataset_filtrato2021.columns)\n",
    "print(dataset_filtrato2021.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## merge ldo2020 e ldo 2021 filtrati ###################################\n",
    "\n",
    "merged_dataset = pd.concat([dataset_filtrato2020, dataset_filtrato2021], ignore_index=True) # non ci sono duplicati tra i due dataset \n",
    "\n",
    "# Risultato finale\n",
    "print(\"\\nColumns merged dataset:\",merged_dataset.columns)\n",
    "print(\"\\nShape merged dataset:\",merged_dataset.shape)\n",
    "print(merged_dataset['testo'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### pulisci il testo: risoluzione di errori di codifica, sostituzione caratteri speciali #######################\n",
    "\n",
    "import ftfy\n",
    "\n",
    "# Applica ftfy.fix_text() a tutte le colonne di testo nel dataset, gestendo i valori non testuali\n",
    "for col in merged_dataset.select_dtypes(include='object').columns:\n",
    "    merged_dataset[col] = merged_dataset[col].apply(lambda x: ftfy.fix_text(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## carica dataset con nosologici positivi #####################\n",
    "\n",
    "Positivi= pd.read_excel('/home/a.renda/to_move/LDO/labeled/20-21_341/NosologiciPositivi_341.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### aggiungere la colonna positivi #########################################\n",
    "\n",
    "# Convertire la colonna 'Nosologico' in numerico nel dataset NosologiciPositivi\n",
    "nosologici_positivi = pd.to_numeric(Positivi['NosologiciPositivi'], errors='coerce').dropna()\n",
    "\n",
    "# Creare la colonna 'positivi' nel DataFrame merged_dataset\n",
    "merged_dataset['positivi'] = merged_dataset['nosologico'].isin(nosologici_positivi).astype(int)\n",
    "\n",
    "\n",
    "# Contare quanti 1 e quanti 0 ci sono nella colonna 'positivi'\n",
    "count_positivi = merged_dataset['positivi'].value_counts()\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"\\nConteggio dei valori nella colonna 'positivi':\")\n",
    "print(f\"Numero di 1 (positivi): {count_positivi.get(1, 0)}\")\n",
    "print(f\"Numero di 0 (non positivi): {count_positivi.get(0, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPUtil\n",
    "\n",
    "def check_gpu_usage():\n",
    "    gpu_stats = GPUtil.getGPUs()\n",
    "    for gpu in gpu_stats:\n",
    "        print(f\"GPU {gpu.id} Memory Used: {gpu.memoryUsed} MB / {gpu.memoryTotal} MB\")\n",
    "        \n",
    "check_gpu_usage()  # Stampa l'uso della memoria GPU prima e dopo ogni batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################### testo unico ##############################################\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Configura il dispositivo\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specifica le colonne da unire\n",
    "colonne_da_unire = ['testo', 'motivo_ricovero', 'anamnesi', \n",
    "                             'esameobiettivo', 'terapiafarmaingresso', \n",
    "                             'decorso', 'laboratorio', 'interventi', \n",
    "                             'followup', 'terapie2', 'terapie3', \n",
    "                             'esami', 'reparto']\n",
    "\n",
    "# Rimuovi le interruzioni di riga da ogni colonna\n",
    "merged_dataset[colonne_da_unire] = merged_dataset[colonne_da_unire].apply(lambda x: x.str.replace('\\n', ' ', regex=False))\n",
    "\n",
    "# Unisci solo le colonne specificate\n",
    "merged_dataset['testo_unico'] = merged_dataset[colonne_da_unire].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "\n",
    "# Salva il risultato in un file di testo\n",
    "with open(\"dataset_medico.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in merged_dataset['testo_unico']:\n",
    "        f.write(text.strip() + \"\\n\")\n",
    "\n",
    "\n",
    "# Carica il modello e spostalo sulla GPU\n",
    "model = BertForMaskedLM.from_pretrained(\"dbmdz/bert-base-italian-uncased\").to(device)\n",
    "# Carica il tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-uncased\")\n",
    "\n",
    "\n",
    "# Funzione per dividere un testo in frammenti di massimo 512 token\n",
    "def split_into_chunks(text, tokenizer, max_length=512):\n",
    "    tokens = tokenizer(text, truncation=False, add_special_tokens=False)['input_ids']\n",
    "    chunks = [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]\n",
    "    return chunks\n",
    "\n",
    "# Segmenta il dataset\n",
    "new_dataset = []\n",
    "for row in merged_dataset['testo_unico']:  # Itera sui testi unici nel tuo dataset\n",
    "    chunks = split_into_chunks(row, tokenizer)  # Dividi ogni testo in frammenti\n",
    "    for chunk in chunks:  # Ricostruisci il testo da ogni frammento di token\n",
    "        new_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        new_dataset.append({\"text\": new_text})  # Salva come dizionario per compatibilità con Hugging Face\n",
    "\n",
    "# Converti il nuovo dataset in un formato compatibile con Hugging Face\n",
    "hf_dataset = Dataset.from_list(new_dataset)\n",
    "\n",
    "# Suddivisione in training e validation set (80-20)\n",
    "dataset_split = hf_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset_split['train']\n",
    "val_dataset = dataset_split['test']\n",
    "\n",
    "# Tokenizzazione\n",
    "def tokenize_function(examples):\n",
    "   return tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numero di esempi nel training set: {len(train_dataset)}\")\n",
    "print(f\"Numero di esempi nel validation set: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## PRE-TRAINING ######################################\n",
    "\n",
    "import torch\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Crea un dataloader\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(tokenized_train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(tokenized_val_dataset, batch_size=8, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# Ottimizzatore\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Directory per salvare i checkpoint\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Parametri per Early Stopping\n",
    "patience = 3\n",
    "min_delta = 1e-4\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "stop_training = False\n",
    "\n",
    "# Numero di epoche totali\n",
    "num_epochs = 10\n",
    "\n",
    "# Controllo se esiste un checkpoint\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pt\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Caricamento del checkpoint da {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # Riprendi dalla prossima epoca\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "    epochs_no_improve = checkpoint['epochs_no_improve']\n",
    "else:\n",
    "    start_epoch = 0\n",
    "\n",
    "# Loop di training\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    if stop_training:\n",
    "        print(f\"Early stopping attivato: interrompo il training dopo {epoch} epoche.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Inizio dell'epoca {epoch + 1}\")\n",
    "    epoch_loss = 0  # Resetta la perdita per questa epoca\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        # Assicurati che tutti i tensori siano sulla GPU\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "        # Aggiungi i labels (per Masked Language Modeling)\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        inputs[\"labels\"] = labels\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        if loss is not None:\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Salva checkpoint ogni 1000 step\n",
    "        if step % 1000 == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pt\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'epochs_no_improve': epochs_no_improve,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint salvato: {checkpoint_path}\")\n",
    "            print(f\"Epoch {epoch + 1}, Step {step}, Training Loss: {loss.item()}\")\n",
    "\n",
    "    epoch_loss /= len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} finished with loss: {epoch_loss}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            labels = inputs[\"input_ids\"].clone()\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "            inputs[\"labels\"] = labels\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            if loss is not None:\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            # Stampa ogni 1000 step\n",
    "            if step % 1000 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Step {step}, Validation Loss: {loss.item()}\")\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f\"Validation loss per l'epoca {epoch + 1}: {val_loss}\")\n",
    "\n",
    "    # Early Stopping e salvataggio del miglior modello\n",
    "    if val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        print(\"Miglioramento trovato, salvataggio del modello...\")\n",
    "        model.save_pretrained(\"best_model\")\n",
    "        tokenizer.save_pretrained(\"best_model\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Nessun miglioramento per {epochs_no_improve} epoca(e).\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping: nessun miglioramento per {patience} epoche consecutive.\")\n",
    "        stop_training = True\n",
    "\n",
    "    # Salva checkpoint alla fine dell'epoca\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"latest_checkpoint.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'epochs_no_improve': epochs_no_improve,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint salvato: {checkpoint_path}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "# Salvataggio finale del modello\n",
    "final_model_dir = \"Med_bert_final\"\n",
    "model.save_pretrained(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "print(f\"Modello finale salvato a: {final_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### ottieni embedding senza troncamento ##############################\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Carica il modello e il tokenizer\n",
    "model_name = \"Med_bert_final\"\n",
    "model = AutoModel.from_pretrained(model_name).to(\"cuda:2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Assicurati che il modello sia in modalità di valutazione\n",
    "model.eval()\n",
    "\n",
    "# Parametri\n",
    "batch_size = 16  # Regola in base alla memoria GPU disponibile\n",
    "max_length = 512  # Lunghezza massima dei testi tokenizzati\n",
    "column_embeddings = []  # Lista per contenere tutti gli embedding\n",
    "\n",
    "# Colonne di testo da processare\n",
    "columns = ['testo', 'motivo_ricovero', 'anamnesi', \n",
    "           'esameobiettivo', 'terapiafarmaingresso', \n",
    "           'decorso', 'laboratorio', 'interventi', \n",
    "           'followup', 'terapie2', 'terapie3', \n",
    "           'esami', 'reparto']\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_embeddings_with_window(text, tokenizer, model, window_size=512, overlap=128):\n",
    "    # Tokenizza il testo senza troncamento\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)[\"input_ids\"]\n",
    "    embeddings = []\n",
    "\n",
    "    # Usa una finestra mobile con sovrapposizione\n",
    "    for start in range(0, len(tokens[0]), window_size - overlap):\n",
    "        end = min(start + window_size, len(tokens[0]))\n",
    "        window_tokens = tokens[:, start:end]\n",
    "\n",
    "        # Estrai gli embeddings per la finestra corrente\n",
    "        with torch.no_grad():\n",
    "            outputs = model(window_tokens.to(\"cuda:2\"))\n",
    "            window_embeddings = outputs.last_hidden_state.mean(dim=1)  # Ottieni l'embedding medio per la finestra\n",
    "\n",
    "            embeddings.append(window_embeddings)\n",
    "\n",
    "    # Combina gli embedding delle finestre (media o concatenazione)\n",
    "    # In questo caso, facciamo la media degli embedding delle finestre per ottenere un unico embedding per riga\n",
    "    combined_embedding = torch.mean(torch.stack(embeddings), dim=0)  # Media lungo la dimensione delle finestre\n",
    "\n",
    "    return combined_embedding\n",
    "\n",
    "# Esegui l'elaborazione per ogni colonna\n",
    "column_embeddings = []\n",
    "for col in columns:\n",
    "    print(f\"Processing column: {col}\")\n",
    "    merged_dataset[col] = merged_dataset[col].astype(str)\n",
    "    texts = merged_dataset[col].tolist()\n",
    "    embeddings_list = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Processing column '{col}'\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        for text in batch_texts:\n",
    "            # Estrai gli embedding usando la finestra mobile per ogni testo e combinali\n",
    "            embeddings = get_embeddings_with_window(text, tokenizer, model)\n",
    "            embeddings_list.append(embeddings)\n",
    "\n",
    "    # Concatenazione finale dei batch per quella colonna\n",
    "    column_embeddings_tensor = torch.stack(embeddings_list)  # Usando stack invece di cat per mantenere la struttura corretta\n",
    "    # Rimuovi la dimensione extra che non serve\n",
    "    column_embeddings_tensor = column_embeddings_tensor.squeeze(1)\n",
    "    column_embeddings.append(column_embeddings_tensor)\n",
    "    print(f\"Column '{col}' processed: {column_embeddings_tensor.shape}\")\n",
    "    \n",
    "\n",
    "# Concatenazione finale di tutti gli embedding\n",
    "final_embeddings = torch.cat(column_embeddings, dim=1)\n",
    "print(f\"Final embeddings shape: {final_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### ottieni embedding con troncamento ##############################\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Carica il modello e il tokenizer\n",
    "model_name = \"Med_bert_final\"\n",
    "model = AutoModel.from_pretrained(model_name).to(\"cuda:2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Assicurati che il modello sia in modalità di valutazione\n",
    "model.eval()\n",
    "\n",
    "# Parametri\n",
    "batch_size = 16  # Regola in base alla memoria GPU disponibile\n",
    "max_length = 512  # Lunghezza massima dei testi tokenizzati\n",
    "column_embeddings = []  # Lista per contenere tutti gli embedding\n",
    "\n",
    "# Colonne di testo da processare\n",
    "columns = ['testo', 'motivo_ricovero', 'anamnesi', \n",
    "           'esameobiettivo', 'terapiafarmaingresso', \n",
    "           'decorso', 'laboratorio', 'interventi', \n",
    "           'followup', 'terapie2', 'terapie3', \n",
    "           'esami', 'reparto']\n",
    "\n",
    "for col in columns:\n",
    "    print(f\"Processing column: {col}\")\n",
    "\n",
    "    # Filtra solo le righe valide (testo non nullo)\n",
    "    #merged_dataset = merged_dataset[merged_dataset[col].notnull()]\n",
    "\n",
    "    # Converti i valori in stringa (se necessario)\n",
    "    merged_dataset[col] = merged_dataset[col].astype(str)\n",
    "\n",
    "    # Estrai i testi dalla colonna\n",
    "    texts = merged_dataset[col].tolist()\n",
    "\n",
    "    embeddings_list = []\n",
    "\n",
    "    # Processa i dati a batch\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Processing column '{col}'\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "        # Tokenizza e convertili in tensori\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda:2\")\n",
    "\n",
    "        # Estrai gli embedding\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)  # Usa la media degli embedding\n",
    "\n",
    "        embeddings_list.append(batch_embeddings)\n",
    "\n",
    "\n",
    "    # Concatenazione dei batch in un'unica tensor\n",
    "    column_embeddings_tensor = torch.cat(embeddings_list, dim=0)\n",
    "    column_embeddings.append(column_embeddings_tensor)\n",
    "\n",
    "    print(f\"Column '{col}' processed: {column_embeddings_tensor.shape}\")\n",
    "\n",
    "# Concatenazione finale degli embedding\n",
    "final_embeddings = torch.cat(column_embeddings, dim=1)\n",
    "print(f\"Final embeddings shape: {final_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## CLASSIFIERS #################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Definisci una lista di classificatori che vuoi provare\n",
    "classifiers = {\n",
    "    #'RandomForest': RandomForestClassifier(n_jobs=-1, max_depth=10, max_features=0.1, random_state=42),\n",
    "    #'LogisticRegression': LogisticRegression(random_state=42, max_iter=200),\n",
    "    #'SVM': SVC(probability=True,random_state=42),\n",
    "    #'KNeighbors': KNeighborsClassifier(),\n",
    "    #'DecisionTree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    #'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'extremeGradientBoosting': XGBClassifier( learning_rate=0.1, random_state=42, n_jobs=-1, max_depth=6),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## CROSS VALIDATION ########################\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def eval_cross_validation(pipeline, X, y, skf):\n",
    "    list_reports = []\n",
    "    list_auc = []  # Lista per raccogliere i valori di AUC\n",
    "    list_num_features = []  # Lista per raccogliere il numero di features per ogni fold\n",
    "\n",
    "    # Metriche aggregate per ogni classe\n",
    "    precision_sum = {}\n",
    "    recall_sum = {}\n",
    "    f1_sum = {}\n",
    "    support_sum = {}  # Per il calcolo del supporto medio\n",
    "\n",
    "    # Liste per calcolare deviazioni standard\n",
    "    precision_values = {}\n",
    "    recall_values = {}\n",
    "    f1_values = {}\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Addestra il pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Previsioni e probabilità\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        y_pred_prob = pipeline.predict_proba(X_val)[:, 1]  # Probabilità della classe positiva\n",
    "\n",
    "        # Classification report per il fold\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        list_reports.append(report)\n",
    "\n",
    "        # AUC per il fold\n",
    "        auc = roc_auc_score(y_val, y_pred_prob)\n",
    "        list_auc.append(auc)\n",
    "\n",
    "        # Numero di features (fisso per embedding precalcolati)\n",
    "        num_features = X.shape[1]\n",
    "        list_num_features.append(num_features)\n",
    "\n",
    "        # Somma le metriche per ogni classe\n",
    "        for label, metrics in report.items():\n",
    "            if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "                precision_sum[label] = precision_sum.get(label, 0) + metrics[\"precision\"]\n",
    "                recall_sum[label] = recall_sum.get(label, 0) + metrics[\"recall\"]\n",
    "                f1_sum[label] = f1_sum.get(label, 0) + metrics[\"f1-score\"]\n",
    "                support_sum[label] = support_sum.get(label, 0) + metrics[\"support\"]\n",
    "\n",
    "                # Aggiungi i valori per la deviazione standard\n",
    "                precision_values[label] = precision_values.get(label, []) + [metrics[\"precision\"]]\n",
    "                recall_values[label] = recall_values.get(label, []) + [metrics[\"recall\"]]\n",
    "                f1_values[label] = f1_values.get(label, []) + [metrics[\"f1-score\"]]\n",
    "\n",
    "    # Calcola le medie delle metriche\n",
    "    num_folds = skf.get_n_splits()\n",
    "    precision_avg = {label: precision_sum[label] / num_folds for label in precision_sum}\n",
    "    recall_avg = {label: recall_sum[label] / num_folds for label in recall_sum}\n",
    "    f1_avg_per_class = {label: f1_sum[label] / num_folds for label in f1_sum}\n",
    "    support_avg = {label: support_sum[label] / num_folds for label in support_sum}\n",
    "\n",
    "    # Calcola le deviazioni standard\n",
    "    precision_std = {label: np.std(precision_values[label]) for label in precision_values}\n",
    "    recall_std = {label: np.std(recall_values[label]) for label in recall_values}\n",
    "    f1_std = {label: np.std(f1_values[label]) for label in f1_values}\n",
    "\n",
    "    # Media di AUC e numero di features\n",
    "    auc_avg = np.mean(list_auc)\n",
    "    num_features_avg = np.mean(list_num_features)\n",
    "\n",
    "    # Crea un DataFrame riassuntivo\n",
    "    df_avg = pd.DataFrame({\n",
    "        \"Precision\": precision_avg,\n",
    "        \"Recall\": recall_avg,\n",
    "        \"F1-Score\": f1_avg_per_class,\n",
    "        \"Support Avg\": support_avg,  # Supporto medio\n",
    "        \"Precision Std\": precision_std,\n",
    "        \"Recall Std\": recall_std,\n",
    "        \"F1-Score Std\": f1_std,\n",
    "    })\n",
    "\n",
    "    return df_avg, auc_avg, f1_avg_per_class, num_features_avg\n",
    "\n",
    "\n",
    "# Configura StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### NO SMOTE ###################################\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "\n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([('classificazione', clf)])\n",
    "\n",
    "    # Usa gli embedding precalcolati\n",
    "    X = final_embeddings.cpu().detach().numpy()  # Sposta i tensori sulla CPU e convertili in un array NumPy\n",
    "    y = merged_dataset['positivi']\n",
    "\n",
    "    # Fit della pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Calcola le metriche usando la funzione (modifica per adattare alla tua funzione eval_cross_validation)\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### SMOTE ###################################\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "\n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    # Usa gli embedding precalcolati\n",
    "    X = final_embeddings.cpu().detach().numpy()  # Sposta i tensori sulla CPU e convertili in un array NumPy\n",
    "    y = merged_dataset['positivi']\n",
    "\n",
    "    # Fit della pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Calcola le metriche usando la funzione (modifica per adattare alla tua funzione eval_cross_validation)\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### SMOTE + UNDERSAMPLING ###################################\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "\n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', SMOTE(random_state=42, sampling_strategy=0.6)),\n",
    "        ('undersample', RandomUnderSampler(sampling_strategy=1.0, random_state=42)),\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    # Usa gli embedding precalcolati\n",
    "    X = final_embeddings.cpu().detach().numpy()  # Sposta i tensori sulla CPU e convertili in un array NumPy\n",
    "    y = merged_dataset['positivi']\n",
    "\n",
    "    # Fit della pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Calcola le metriche usando la funzione (modifica per adattare alla tua funzione eval_cross_validation)\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Configurazione\n",
    "model_name = \"Med_bert_final\"\n",
    "device = \"cuda:2\"\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "learning_rate = 2e-5\n",
    "patience = 3\n",
    "checkpoint_dir = \"checkpoints\"  # Directory per salvare i checkpoint\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Dataset personalizzato\n",
    "class MultiInputDataset(Dataset):\n",
    "    def __init__(self, dataframe, columns, tokenizer, model, max_length, overlap):\n",
    "        self.data = dataframe\n",
    "        self.columns = columns\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.max_length = max_length\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for col in self.columns:\n",
    "            text = str(self.data.iloc[idx][col])\n",
    "            # Ottieni gli embeddings per il testo completo\n",
    "            embedding = get_embeddings_with_window(text, self.tokenizer, self.model, self.max_length, self.overlap)\n",
    "            \n",
    "            #item[col] = embedding\n",
    "            #item[col] = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "            item[col] = {\n",
    "                \"input_ids\": embedding[\"input_ids\"],  # Embedding già presente\n",
    "                \"attention_mask\": embedding[\"attention_mask\"]  # Creiamo una mask di attenzione\n",
    "            \n",
    "            }\n",
    "        item[\"labels\"] = torch.tensor(self.data.iloc[idx][\"positivi\"], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Modello multi-input\n",
    "class MultiInputBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, num_columns):\n",
    "        super(MultiInputBERT, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.num_columns = num_columns\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * num_columns, num_labels)\n",
    "\n",
    "    def forward(self, inputs_per_column):\n",
    "        column_embeddings = []\n",
    "        for col in inputs_per_column:\n",
    "            input_ids = inputs_per_column[col][\"input_ids\"]\n",
    "            attention_mask = inputs_per_column[col][\"attention_mask\"]\n",
    "            \n",
    "            # Assicurati che siano in batch form e abbiano la forma corretta\n",
    "            input_ids = input_ids.to(device)  # Aggiungi il dispositivo (cuda)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            # Passa i tensori come input a BERT\n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pooled_output = outputs.pooler_output\n",
    "            column_embeddings.append(pooled_output)\n",
    "\n",
    "        # Concatenare le rappresentazioni delle colonne\n",
    "        combined_embeddings = torch.cat(column_embeddings, dim=1)\n",
    "        logits = self.classifier(combined_embeddings)\n",
    "        return logits\n",
    "# Carica il tokenizer e il modello\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = MultiInputBERT(model_name=model_name, num_labels=2, num_columns=len(columns)).to(device)\n",
    "\n",
    "# Dataset\n",
    "dataset = MultiInputDataset(dataframe=merged_dataset, \n",
    "                            columns=columns, \n",
    "                            tokenizer=tokenizer, \n",
    "                            model=model,  # Passa l'oggetto modello, non il nome\n",
    "                            max_length=512, \n",
    "                            overlap=128)\n",
    "\n",
    "# Suddivisione in train e validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Dataloader\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# Ottimizzatore e funzione di perdita\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Funzione per salvare il checkpoint\n",
    "def save_checkpoint(epoch, model, optimizer, best_val_loss, checkpoint_dir):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "# Funzione per caricare il checkpoint\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "    print(f\"Checkpoint loaded: {checkpoint_path} (Epoch {epoch}, Best Val Loss: {best_val_loss:.4f})\")\n",
    "    return epoch, best_val_loss\n",
    "\n",
    "# Controlla se esistono checkpoint e riprendi\n",
    "start_epoch = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "last_checkpoint = None\n",
    "for file in os.listdir(checkpoint_dir):\n",
    "    if file.startswith(\"checkpoint_epoch_\"):\n",
    "        last_checkpoint = os.path.join(checkpoint_dir, file)\n",
    "\n",
    "if last_checkpoint:\n",
    "    start_epoch, best_val_loss = load_checkpoint(last_checkpoint, model, optimizer)\n",
    "\n",
    "# Early stopping\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "\n",
    "# Training loop con checkpointing\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        inputs_per_column = {col: {k: v.to(device) for k, v in batch[col].items()} for col in columns}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(inputs_per_column)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch + 1} Validation\"):\n",
    "            inputs_per_column = {col: {k: v.to(device) for k, v in batch[col].items()} for col in columns}\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(inputs_per_column)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"Validation loss improved to {val_loss:.4f}. Saving model and checkpoint...\")\n",
    "        save_checkpoint(epoch + 1, model, optimizer, best_val_loss, checkpoint_dir)\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement in validation loss for {epochs_no_improve} epochs.\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Salva il modello migliore\n",
    "if best_model_state:\n",
    "    torch.save(best_model_state, \"fine_tuned_model.pth\")\n",
    "    print(\"Model saved as fine_tuned_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "# Carica il modello e il tokenizer\n",
    "model_name = \"Med_bert_final\"\n",
    "model = AutoModel.from_pretrained(model_name).to(\"cuda:2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "columns = [\n",
    "    'testo', 'motivo_ricovero', 'anamnesi', 'esameobiettivo',\n",
    "    'terapiafarmaingresso', 'decorso', 'laboratorio', 'interventi',\n",
    "    'followup', 'terapie2', 'terapie3', 'esami', 'reparto'\n",
    "]\n",
    "\n",
    "# Prepara un dizionario con i dati di ogni colonna\n",
    "texts_per_column = {col: merged_dataset[col].astype(str).tolist() for col in columns}\n",
    "labels = merged_dataset[\"positivi\"].tolist()\n",
    "\n",
    "\n",
    "# Tokenizza ogni colonna\n",
    "encodings_per_column = {\n",
    "    col: tokenizer(texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "    for col, texts in texts_per_column.items()\n",
    "}\n",
    "\n",
    "\n",
    "class MultiInputDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings_per_column, labels):\n",
    "        self.encodings_per_column = encodings_per_column\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {col: {key: val[idx] for key, val in encodings.items()}\n",
    "                for col, encodings in self.encodings_per_column.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "dataset = MultiInputDataset(encodings_per_column, labels)\n",
    "\n",
    "\n",
    "class MultiInputBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, num_columns):\n",
    "        super(MultiInputBERT, self).__init__()\n",
    "        self.bert_models = nn.ModuleList([AutoModel.from_pretrained(model_name) for _ in range(num_columns)])\n",
    "        self.classifier = nn.Linear(num_columns * 768, num_labels)  # 768 = hidden size di BERT\n",
    "\n",
    "    def forward(self, inputs_per_column):\n",
    "        # Elaborazione separata di ciascuna colonna\n",
    "        embeddings = []\n",
    "        for i, inputs in enumerate(inputs_per_column.values()):\n",
    "            outputs = self.bert_models[i](**inputs)\n",
    "            embeddings.append(outputs.pooler_output)  # Usa l'output della pooler di BERT\n",
    "\n",
    "        # Combina gli embeddings delle colonne\n",
    "        combined_embedding = torch.cat(embeddings, dim=1)  # Concatenazione lungo la dimensione delle features\n",
    "\n",
    "        # Classificazione\n",
    "        logits = self.classifier(combined_embedding)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "model = MultiInputBERT(\n",
    "    model_name=\"Med_bert_final\",\n",
    "    num_labels=2,  # Per classificazione binaria\n",
    "    num_columns=len(columns)\n",
    ").to(\"cuda:2\")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Ciclo di addestramento\n",
    "epochs = 5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Estrai input separati per ogni colonna\n",
    "        inputs_per_column = {col: {key: val.to(\"cuda:2\") for key, val in batch[col].items()}\n",
    "                             for col in columns}\n",
    "        labels = batch[\"labels\"].to(\"cuda:2\")\n",
    "\n",
    "        # Passa i dati al modello\n",
    "        outputs = model(inputs_per_column)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Loss = {loss.item()}\")\n",
    "\n",
    "\n",
    "# Cartella per salvare il modello\n",
    "save_directory = Path(\"./fine_tuned_model\")\n",
    "save_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Salva il modello e il tokenizer\n",
    "model.model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Fine-tuned model saved in {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Inference sul validation set\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:  # Usa un DataLoader per il validation set\n",
    "        inputs_per_column = {col: {key: val.to(\"cuda\") for key, val in batch[col].items()} for col in columns}\n",
    "        labels = batch[\"labels\"].to(\"cuda\")\n",
    "\n",
    "        outputs = model(inputs_per_column)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, predictions))\n",
    "print(classification_report(true_labels, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
