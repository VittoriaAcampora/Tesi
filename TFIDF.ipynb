{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source venv2/bin/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show spacy\n",
    "!pip show torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install thinc==8.0.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydantic==1.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### carica dataset LDO 20-21 ############################ \n",
    "\n",
    "dataLDO2020 = pd.read_excel('/home/a.renda/to_move/LDO/filtrato_per_keyword/20-21_LDO_26K/LDO_20200101_20210101 pulito.ods', engine='odf')\n",
    "dataLDO2021=pd.read_excel('/home/a.renda/to_move/LDO/filtrato_per_keyword/20-21_LDO_26K/LDO_20210101_20220101 pulito.ods', engine='odf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra le righe che non contengono numeri (escludendo anche i NaN)\n",
    "dataLDO2021 = dataLDO2021[dataLDO2021['nosologico'].astype(str).str.contains(r'\\d')]\n",
    "#rimosse 212 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nColumns LDO 20-21:\", dataLDO2020.columns)\n",
    "print(\"\\nColumns LDO 21-22:\", dataLDO2021.columns)\n",
    "print(\"\\nShape LDO 20-21:\", dataLDO2020.shape)\n",
    "print(\"\\nShape LDO 21-22:\", dataLDO2021.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## carica database filtrato ###############################\n",
    "\n",
    "databaseFiltrato=pd.read_csv('/home/a.renda/to_move/LDO/labeled/20-21_341/DatabaseFiltrato.csv', sep=';')\n",
    "print(databaseFiltrato.shape)\n",
    "print(databaseFiltrato.columns) # la prima colonna è solo un contatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra le righe che non contengono numeri (escludendo anche i NaN)\n",
    "databaseFiltrato = databaseFiltrato[databaseFiltrato['nosologico'].astype(str).str.contains(r'\\d')]\n",
    "\n",
    "#tolte 5 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### filtra ldo 2020 ##########################\n",
    "\n",
    "# Converti la colonna 'nosologico' del secondo dataset in int\n",
    "databaseFiltrato['nosologico'] = pd.to_numeric(databaseFiltrato['nosologico'], errors='coerce')\n",
    "\n",
    "\n",
    "# Trova i nosologici comuni\n",
    "comuni2020 = dataLDO2020['nosologico'].isin(databaseFiltrato['nosologico'])\n",
    "\n",
    "# Filtra il primo dataset\n",
    "dataset_filtrato2020 = dataLDO2020[comuni2020]\n",
    "print(dataset_filtrato2020.columns)\n",
    "print(dataset_filtrato2020.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### filtra ldo 2021 ###########################\n",
    "\n",
    "# Trova i nosologici comuni\n",
    "comuni2021 = dataLDO2021['nosologico'].isin(databaseFiltrato['nosologico'])\n",
    "\n",
    "# Filtra il primo dataset\n",
    "dataset_filtrato2021 = dataLDO2021[comuni2021]\n",
    "print(dataset_filtrato2021.columns)\n",
    "print(dataset_filtrato2021.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## merge ldo2020 e ldo 2021 filtrati ###################################\n",
    "\n",
    "merged_dataset = pd.concat([dataset_filtrato2020, dataset_filtrato2021], ignore_index=True) # non ci sono duplicati tra i due dataset \n",
    "\n",
    "# Risultato finale\n",
    "print(\"\\nColumns merged dataset:\",merged_dataset.columns)\n",
    "print(\"\\nShape merged dataset:\",merged_dataset.shape)\n",
    "print(merged_dataset['testo'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## carica dataset con nosologici positivi #####################\n",
    "\n",
    "Positivi= pd.read_excel('/home/a.renda/to_move/LDO/labeled/20-21_341/NosologiciPositivi_341.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### aggiungere la colonna positivi #########################################\n",
    "\n",
    "# Convertire la colonna 'Nosologico' in numerico nel dataset NosologiciPositivi\n",
    "nosologici_positivi = pd.to_numeric(Positivi['NosologiciPositivi'], errors='coerce').dropna()\n",
    "\n",
    "# Creare la colonna 'positivi' nel DataFrame merged_dataset\n",
    "merged_dataset['positivi'] = merged_dataset['nosologico'].isin(nosologici_positivi).astype(int)\n",
    "\n",
    "\n",
    "# Contare quanti 1 e quanti 0 ci sono nella colonna 'positivi'\n",
    "count_positivi = merged_dataset['positivi'].value_counts()\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"\\nConteggio dei valori nella colonna 'positivi':\")\n",
    "print(f\"Numero di 1 (positivi): {count_positivi.get(1, 0)}\")\n",
    "print(f\"Numero di 0 (non positivi): {count_positivi.get(0, 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unire parole e numeri in 'reparto' rimuovendo lo spazio e sostituendo con un trattino\n",
    "merged_dataset['reparto'] = merged_dataset['reparto'].str.replace(r'(\\w) (\\d)', r'\\1-\\2', regex=True)\n",
    "\n",
    "# Visualizza i risultati\n",
    "print(merged_dataset['reparto'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### pulisci il testo: risoluzione di errori di codifica, sostituzione caratteri speciali #######################\n",
    "import ftfy\n",
    "\n",
    "# Applica ftfy.fix_text() a tutte le colonne di testo nel dataset, gestendo i valori non testuali\n",
    "for col in merged_dataset.select_dtypes(include='object').columns:\n",
    "    merged_dataset[col] = merged_dataset[col].apply(lambda x: ftfy.fix_text(x) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## PRE-PROCESSING vecchio ##########################\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Carica il modello SpaCy per l'italiano\n",
    "nlp = spacy.load(\"it_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Lista delle colonne su cui applicare il preprocessing\n",
    "colonne_da_preprocessare = ['testo', 'motivo_ricovero', 'anamnesi', \n",
    "                             'esameobiettivo', 'terapiafarmaingresso', \n",
    "                             'decorso', 'laboratorio', 'interventi', \n",
    "                             'followup', 'terapie2', 'terapie3', \n",
    "                             'esami', 'reparto']  # Sostituisci con i nomi reali delle colonne\n",
    "\n",
    "# Funzione per pulire i token\n",
    "def clean_token(token):\n",
    "\n",
    "    # Rimuovi numeri con virgola\n",
    "    if re.match(r'^\\d{1,3}(?:,\\d{1,3})$', token):\n",
    "        return ''  # Scarta il token\n",
    "\n",
    "    # Mantieni numeri, lettere, rimuovendo altri caratteri speciali\n",
    "    cleaned_token = re.sub(r'[^a-zA-Z0-9]', '', token) \n",
    "\n",
    "    # Scarta il token se è composto solo da un carattere o solo da un numero\n",
    "    if re.match(r'^\\d+$', cleaned_token):  # Se è solo un numero, scartalo\n",
    "        return ''\n",
    "    return '' if len(cleaned_token) == 1 else cleaned_token\n",
    "\n",
    "\n",
    "# Funzione per tokenizzare e preprocessare\n",
    "def preprocess_text(row):\n",
    "    if not isinstance(row, str):  # Verifica che row sia una stringa\n",
    "        return \"\"  # Restituisce una stringa vuota se non è valida (ovvero quando vede un nan restiruisce una stringa vuota)\n",
    "    \n",
    "    # Rimuove date nel formato 'dd/mm/yyyy' e 'dd/mm/yy'\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', '', row)\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}', '', row)  # Rimuove numeri separati da /\n",
    "\n",
    "    # Gestisci i punti tra le parole (ad esempio \"v.analisi\" diventa \"v analisi\")\n",
    "    row = re.sub(r'(\\w)\\.(\\w)', r'\\1 \\2', row)  # Aggiunge uno spazio tra parole separate da punto\n",
    "\n",
    "        \n",
    "    # Tokenizza il testo con SpaCy\n",
    "    doc = nlp(row)\n",
    "    \n",
    "    # Filtra stopwords e punteggiatura\n",
    "    tokens_puliti = [\n",
    "        clean_token(token.lemma_.lower())  # Lemmatizza, porta a minuscolo e pulisce il token\n",
    "        for token in doc\n",
    "        if not token.is_punct and not token.is_stop \n",
    "    ]\n",
    "    \n",
    "    # Ricombina i token in una stringa\n",
    "    return \" \".join(tokens_puliti)\n",
    "\n",
    "# Applica il preprocessing per ciascuna colonna specificata\n",
    "for colonna in colonne_da_preprocessare:\n",
    "    nuova_colonna = f\"{colonna}_preprocessed\"  # Nome della nuova colonna\n",
    "    merged_dataset[nuova_colonna] = merged_dataset[colonna].apply(preprocess_text)\n",
    "\n",
    "# Visualizza i risultati per le nuove colonne preprocessate\n",
    "print(merged_dataset[[colonna for colonna in colonne_da_preprocessare] + \n",
    "                     [f\"{col}_preprocessed\" for col in colonne_da_preprocessare]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## PRE-PROCESSING nuovo ##########################\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Carica il modello SpaCy per l'italiano\n",
    "nlp = spacy.load(\"it_core_news_lg\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Lista delle colonne su cui applicare il preprocessing\n",
    "colonne_da_preprocessare = ['testo', 'motivo_ricovero', 'anamnesi', \n",
    "                             'esameobiettivo', 'terapiafarmaingresso', \n",
    "                             'decorso', 'laboratorio', 'interventi', \n",
    "                             'followup', 'terapie2', 'terapie3', \n",
    "                             'esami', 'reparto']  # Sostituisci con i nomi reali delle colonne\n",
    "\n",
    "def clean_token(token):\n",
    "\n",
    "    # Rimuovi caratteri non alfanumerici (inclusi simboli, punteggiatura, numeri, ecc.) e sostituiscili con uno spazio\n",
    "    cleaned_token = re.sub(r'[^a-zA-ZàèéìòùÀÈÉÌÒÙ]', ' ', token)\n",
    "    cleaned_token = re.sub(r'\\s+', ' ', cleaned_token).strip()  # Normalizza gli spazi multipli\n",
    "\n",
    "    return cleaned_token\n",
    "\n",
    "    \n",
    "# Funzione per tokenizzare e preprocessare il testo\n",
    "def preprocess_text(row):\n",
    "    if not isinstance(row, str):  # Verifica che il dato sia una stringa valida\n",
    "        return \"\"  # Restituisce una stringa vuota se non valido\n",
    "    \n",
    "    # Rimuove date nel formato 'dd/mm/yyyy' e 'dd/mm/yy'\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', '', row)\n",
    "    row = re.sub(r'\\d{1,2}/\\d{1,2}', '', row)  # Rimuove numeri separati da /\n",
    "      \n",
    "    # Aggiunge spazi tra numeri e lettere (es. \"800duloxetina\" -> \"800 duloxetina\")\n",
    "    row = re.sub(r'(\\d+)([a-zA-Z]+)', r'\\1 \\2', row)\n",
    "    row = re.sub(r'([a-zA-Z]+)(\\d+)', r'\\1 \\2', row)\n",
    "\n",
    "    # Aggiunge spazi tra parole composte tipo \"vediAllegato\" -> \"vedi Allegato\"\n",
    "    row = re.sub(r'([a-zàèéìòù])([A-ZÀÈÉÌÒÙ])', r'\\1 \\2', row)\n",
    "\n",
    "    # normalizza  spazi (se ci sono più spazi consecutivi, vengono ridotti a uno solo) e rimuove  spazi all'inizio e alla fine della stringa\n",
    "    row = re.sub(r'[^\\w\\s]', ' ', row)  # Rimuove caratteri non alfanumerici e parentesi\n",
    "\n",
    "    doc = nlp(row)\n",
    "   \n",
    "    # Filtra e normalizza i token\n",
    "    tokens_puliti = [\n",
    "        clean_token(token.lemma_.lower())\n",
    "        for token in doc\n",
    "        if not token.is_punct and not token.is_stop and len(token.text) > 1 #and token.text.isalpha() \n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Applica un filtro finale per rimuovere manualmente le lettere singole\n",
    "    tokens_puliti = [token for token in tokens_puliti if len(token) > 1]  # Assicura che tutte le parole siano > 1 carattere\n",
    "    \n",
    "\n",
    "    # Ricombina i token in una stringa\n",
    "    return \" \".join(tokens_puliti)\n",
    "\n",
    "# Applica il preprocessing per ciascuna colonna specificata\n",
    "for colonna in colonne_da_preprocessare:\n",
    "    nuova_colonna = f\"{colonna}_preprocessed\"  # Nome della nuova colonna\n",
    "    merged_dataset[nuova_colonna] = merged_dataset[colonna].apply(preprocess_text)\n",
    "\n",
    "# Visualizza i risultati per le nuove colonne preprocessate\n",
    "print(merged_dataset[[colonna for colonna in colonne_da_preprocessare] + \n",
    "                     [f\"{col}_preprocessed\" for col in colonne_da_preprocessare]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ DIMENSIONE VOCABOLARIO ############################\n",
    "# Calcola il vocabolario\n",
    "vocab_set = set()\n",
    "# Itera sulle prime 100 righe delle colonne preprocessate\n",
    "for colonna in colonne_da_preprocessare:\n",
    "    for testo in merged_dataset[f\"{colonna}_preprocessed\"]:  # Limita a 100 righe\n",
    "        if isinstance(testo, str):  # Assicurati che il testo non sia NaN\n",
    "            vocab_set.update(testo.split())\n",
    "\n",
    "# Calcola la dimensione del vocabolario\n",
    "vocabolario_dimensione = len(vocab_set)\n",
    "print(\"Dimensione del vocabolario:\", vocabolario_dimensione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combina tutte le colonne di testo in un'unica colonna\n",
    "# Filtra solo le colonne di testo preprocessato che desideri analizzare\n",
    "columns_of_interest = ['testo_preprocessed', 'motivo_ricovero_preprocessed', 'anamnesi_preprocessed', \n",
    "                             'esameobiettivo_preprocessed', 'terapiafarmaingresso_preprocessed', \n",
    "                             'decorso_preprocessed', 'laboratorio_preprocessed', 'interventi_preprocessed', \n",
    "                             'followup_preprocessed', 'terapie2_preprocessed', 'terapie3_preprocessed', \n",
    "                             'esami_preprocessed', 'reparto_preprocessed']  # Sostituisci con i nomi delle colonne\n",
    "merged_dataset['combined_text'] = merged_dataset[columns_of_interest].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "# Calcola il vocabolario dalla colonna 'combined_text'\n",
    "vocab_set = set()\n",
    "for testo in merged_dataset['combined_text']:\n",
    "    if isinstance(testo, str):  # Assicurati che il testo non sia NaN\n",
    "        vocab_set.update(testo.split())\n",
    "\n",
    "# Calcola la dimensione del vocabolario\n",
    "vocabolario_dimensione = len(vocab_set)\n",
    "\n",
    "# Vettorizzazione usando TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Filtra i NaN e vettorizza la colonna 'combined_text'\n",
    "valid_texts = merged_dataset['combined_text'].dropna()  # Rimuove i NaN\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(valid_texts)\n",
    "\n",
    "# Numero di features (parole) nella matrice TF-IDF\n",
    "num_features = tfidf_matrix.shape[1]\n",
    "\n",
    "# Confronto tra dimensione del vocabolario e numero di features\n",
    "print(\"Dimensione del vocabolario:\", vocabolario_dimensione)\n",
    "print(\"Numero di features nella matrice TF-IDF:\", num_features)\n",
    "\n",
    "# Verifica dell'uguaglianza\n",
    "if vocabolario_dimensione == num_features:\n",
    "    print(\"La dimensione del vocabolario coincide con il numero di features nella matrice TF-IDF.\")\n",
    "else:\n",
    "    print(\"La dimensione del vocabolario NON coincide con il numero di features nella matrice TF-IDF.\")\n",
    "    # Trova le discrepanze\n",
    "    tfidf_features = set(tfidf_vectorizer.get_feature_names_out())\n",
    "    vocab_not_in_tfidf = vocab_set - tfidf_features\n",
    "    tfidf_not_in_vocab = tfidf_features - vocab_set\n",
    "\n",
    "    print(\"\\nParole presenti nel vocabolario manuale ma non nel TF-IDF:\")\n",
    "    print(vocab_not_in_tfidf)\n",
    "\n",
    "    print(\"\\nParole presenti nel TF-IDF ma non nel vocabolario manuale:\")\n",
    "    print(tfidf_not_in_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### MEDIA NUMERO PAROLE COLONNE PRE-PROCESSATE #######################\n",
    "\n",
    "# Funzione per calcolare il numero di parole\n",
    "def count_words(text):\n",
    "    if isinstance(text, str):  # Verifica se il testo è una stringa\n",
    "        return len(text.split())  # Conta le parole\n",
    "    elif isinstance(text, (int, float)):  # Se è un numero\n",
    "        return 1  # Considera il numero come una parola\n",
    "    return 0  # Restituisce 0 se non è una stringa o un numero\n",
    "\n",
    "for colonna in colonne_da_preprocessare:\n",
    "    nuova_colonna = f\"{colonna}_preprocessed\"\n",
    "    word_counts = merged_dataset[nuova_colonna].apply(count_words)  # Conta le parole in ogni stringa\n",
    "    average_word_count = word_counts.mean()  # Calcola la lunghezza media\n",
    "    print(f\"Lunghezza media della colonna '{nuova_colonna}' in parole: {average_word_count:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_dataset['testo'][2])\n",
    "print(merged_dataset['testo_preprocessed'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## CLASSIFIERS #################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Definisci una lista di classificatori che vuoi provare\n",
    "classifiers = {\n",
    "    #'RandomForest': RandomForestClassifier(n_jobs=-1, max_depth=10, max_features=0.1, random_state=42),\n",
    "    #'LogisticRegression': LogisticRegression(random_state=42, max_iter=200),\n",
    "    #'SVM': SVC(probability=True,random_state=42),\n",
    "    #'KNeighbors': KNeighborsClassifier(),\n",
    "    #'DecisionTree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    #'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'extremeGradientBoosting': XGBClassifier( learning_rate=0.1, random_state=42, n_jobs=-1, max_depth=6),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### CROSS VALIDATION #########################\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "import numpy as np\n",
    "\n",
    "# Funzione modificata per includere la media delle features\n",
    "def eval_cross_validation(pipeline, X, y, skf):\n",
    "    list_reports = []\n",
    "    list_f1 = []\n",
    "    list_auc = []  # Lista per raccogliere i valori di AUC mediati\n",
    "    list_num_features = []  # Lista per raccogliere il numero di features per ogni fold\n",
    "    \n",
    "    # Crea una lista per raccogliere precision, recall e f1-score mediati\n",
    "    precision_sum = {}\n",
    "    recall_sum = {}\n",
    "    f1_sum = {}\n",
    "    \n",
    "    # Liste per la deviazione standard\n",
    "    precision_values = {}\n",
    "    recall_values = {}\n",
    "    f1_values = {}\n",
    "\n",
    "    # Inizializza il supporto per ogni classe\n",
    "    unique_labels = y.unique()\n",
    "    support_sum = {str(label): 0 for label in unique_labels}  # Assicurati che le etichette siano stringhe\n",
    "\n",
    "    for train, val in skf.split(X, y):\n",
    "        X_tr = X.iloc[train]  # differenza con 'baseline1'\n",
    "        y_tr = y.iloc[train]\n",
    "        X_val = X.iloc[val]\n",
    "        y_val = y.iloc[val]\n",
    "\n",
    "        # Addestra il pipeline sul training set\n",
    "        pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "        # Previsioni sul validation set\n",
    "        y_pred = pipeline.predict(X_val)\n",
    "        y_pred_prob = pipeline.predict_proba(X_val)[:, 1]  # Probabilità della classe positiva\n",
    "\n",
    "        # Crea il classification report come dizionario\n",
    "        cr = classification_report(y_val, y_pred, output_dict=True)\n",
    "\n",
    "        # Aggiungi il report alla lista\n",
    "        list_reports.append(cr)\n",
    "\n",
    "        # Estrai il F1-score\n",
    "        list_f1.append(cr['weighted avg']['f1-score'])\n",
    "\n",
    "        # Calcola l'AUC per questo fold\n",
    "        auc_score = roc_auc_score(y_val, y_pred_prob)\n",
    "        list_auc.append(auc_score)\n",
    "\n",
    "        # Raccogli il numero di features per questo fold\n",
    "        X_tfidf = pipeline.named_steps['tfidf'].transform(X_val)  # Modificato per estrarre la trasformazione\n",
    "        list_num_features.append(X_tfidf.shape[1])  # Numero di features per questo fold\n",
    "\n",
    "        # Somma le metriche per ogni classe\n",
    "        for label, metrics in cr.items():\n",
    "            if label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                precision_sum[label] = precision_sum.get(label, 0) + metrics['precision']\n",
    "                recall_sum[label] = recall_sum.get(label, 0) + metrics['recall']\n",
    "                f1_sum[label] = f1_sum.get(label, 0) + metrics['f1-score']\n",
    "\n",
    "                # Aggiungi valori per la deviazione standard\n",
    "                precision_values[label] = precision_values.get(label, []) + [metrics['precision']]\n",
    "                recall_values[label] = recall_values.get(label, []) + [metrics['recall']]\n",
    "                f1_values[label] = f1_values.get(label, []) + [metrics['f1-score']]\n",
    "\n",
    "                # Somma il supporto per questa fold\n",
    "                support_sum[str(label)] += metrics['support']  # Usa str(label) per garantire la corrispondenza\n",
    "\n",
    "    # Calcola la media dell'AUC\n",
    "    auc_avg = np.mean(list_auc)\n",
    "\n",
    "    # Calcola la media del numero di features\n",
    "    num_features_avg = np.mean(list_num_features)\n",
    "\n",
    "    # Calcola la media delle metriche per ogni classe\n",
    "    num_folds = skf.get_n_splits()\n",
    "    precision_avg = {label: precision_sum[label] / num_folds for label in precision_sum}\n",
    "    recall_avg = {label: recall_sum[label] / num_folds for label in recall_sum}\n",
    "    f1_avg_per_class = {label: f1_sum[label] / num_folds for label in f1_sum}\n",
    "\n",
    "    # Calcola la deviazione standard per ogni metrica\n",
    "    precision_std = {label: np.std(precision_values[label]) for label in precision_values}\n",
    "    recall_std = {label: np.std(recall_values[label]) for label in recall_values}\n",
    "    f1_std = {label: np.std(f1_values[label]) for label in f1_values}\n",
    "\n",
    "    # Calcola il supporto medio per ciascuna classe\n",
    "    support_avg = {label: support_sum[label] / num_folds for label in support_sum}\n",
    "\n",
    "    # Crea un DataFrame per visualizzare le metriche\n",
    "    df_avg = pd.DataFrame({\n",
    "        'Precision': precision_avg,\n",
    "        'Recall': recall_avg,\n",
    "        'F1-Score': f1_avg_per_class,\n",
    "        'Precision Std': precision_std,\n",
    "        'Recall Std': recall_std,\n",
    "        'F1-Score Std': f1_std,\n",
    "        'Support': support_avg,  # Supporto medio\n",
    "        'Avg Features': num_features_avg,  # Media delle features\n",
    "    })  # Trasponi per avere le classi come righe\n",
    "\n",
    "    return df_avg, auc_avg, f1_avg_per_class, num_features_avg \n",
    " \n",
    "\n",
    "# Crea un oggetto StratifiedKFold per la cross-validation stratificata\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### CUSTOM TRASFORMER SENZA MAX FEATURES ########################\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "\n",
    "class CustomTfidfTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        CustomTfidfTransformer applica un TfidfVectorizer separatamente per ogni colonna.\n",
    "        Non impone limiti sul numero di features, quindi utilizza tutte le parole per ogni colonna.\n",
    "        \"\"\"\n",
    "        self.tfidf_column_vectorizers = {}\n",
    "        self.column_feature_counts = {}  # Dictionario per accumulare le features di ogni colonna\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Crea un TfidfVectorizer per ogni colonna e registra il numero di features\n",
    "        for col in X.columns:\n",
    "            col_data = X[col].fillna('').astype(str)\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            self.tfidf_column_vectorizers[col] = vectorizer\n",
    "            self.tfidf_column_vectorizers[col].fit(col_data)\n",
    "\n",
    "            # Salva il numero di features per questa colonna\n",
    "            num_features_col = len(vectorizer.get_feature_names_out())\n",
    "            if col in self.column_feature_counts:\n",
    "                self.column_feature_counts[col].append(num_features_col)\n",
    "            else:\n",
    "                self.column_feature_counts[col] = [num_features_col]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        transformed_columns = []\n",
    "        for col in X.columns:\n",
    "            col_data = X[col].fillna('').astype(str)\n",
    "            transformed_columns.append(self.tfidf_column_vectorizers[col].transform(col_data))\n",
    "        X_combined = hstack(transformed_columns)\n",
    "        return X_combined\n",
    "\n",
    "    def mean_features_per_column(self):\n",
    "        \"\"\"\n",
    "        Calcola e restituisce il numero medio di features per colonna.\n",
    "        \"\"\"\n",
    "        mean_features = {col: sum(counts) / len(counts) for col, counts in self.column_feature_counts.items()}\n",
    "        return mean_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### CLASSIFICATION SENZA MAX FEATURES E SENZA SMOTE ###########################\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Lista delle colonne preprocessate\n",
    "colonne_preprocessate = [f\"{colonna}_preprocessed\" for colonna in colonne_da_preprocessare]\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "X = merged_dataset[colonne_preprocessate]\n",
    "y = merged_dataset['positivi']\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "    \n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', CustomTfidfTransformer() ),  # Usa il CustomTfidfTransformer\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    # Trasforma i dati e ottieni la matrice TF-IDF combinata\n",
    "    X_tfidf = pipeline.named_steps['tfidf'].fit_transform(X)\n",
    "    \n",
    "    #num_samples, num_features = X_tfidf.shape\n",
    "    #print(f\"Dimensione della matrice TF-IDF combinata: {num_samples} campioni, {num_features} features\")\n",
    "\n",
    "    # Calcola le metriche usando la funzione\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Stampa il numero medio di features per colonna\n",
    "    mean_features = pipeline.named_steps['tfidf'].mean_features_per_column()\n",
    "    print(\"\\nNumero medio di features per ciascuna colonna:\")\n",
    "    for col, mean_count in mean_features.items():\n",
    "        print(f\"Colonna '{col}': {mean_count:.2f} features\")\n",
    "    \n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### CLASSIFICATION SENZA MAX FEATURES ###########################\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Lista delle colonne preprocessate\n",
    "colonne_preprocessate = [f\"{colonna}_preprocessed\" for colonna in colonne_da_preprocessare]\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "X = merged_dataset[colonne_preprocessate]\n",
    "y = merged_dataset['positivi']\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "    \n",
    "    # Crea la pipeline per il classificatore corrente\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', CustomTfidfTransformer() ),  # Usa il CustomTfidfTransformer\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    # Trasforma i dati e ottieni la matrice TF-IDF combinata\n",
    "    X_tfidf = pipeline.named_steps['tfidf'].fit_transform(X)\n",
    "    #num_samples, num_features = X_tfidf.shape\n",
    "    #print(f\"Dimensione della matrice TF-IDF combinata: {num_samples} campioni, {num_features} features\")\n",
    "\n",
    "    # Calcola le metriche usando la funzione\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Stampa il numero medio di features per colonna\n",
    "    mean_features = pipeline.named_steps['tfidf'].mean_features_per_column()\n",
    "    print(\"\\nNumero medio di features per ciascuna colonna:\")\n",
    "    for col, mean_count in mean_features.items():\n",
    "        print(f\"Colonna '{col}': {mean_count:.2f} features\")\n",
    "    \n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### CUSTOM TRANSFORMER CON MAX FEATURES ########################\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "class CustomTfidfTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_features_per_column=None):\n",
    "        \"\"\"\n",
    "        max_features_per_column è un dizionario che mappa i nomi delle colonne a un valore di max_features.\n",
    "        Se non viene fornito, ogni colonna avrà un numero illimitato di features (comportamento di default).\n",
    "        \"\"\"\n",
    "        self.max_features_per_column = max_features_per_column if max_features_per_column is not None else {}\n",
    "        self.tfidf_column_vectorizers = {}\n",
    "        self.column_feature_counts = {}  # Dizionario per accumulare le features di ogni colonna\n",
    "\n",
    "    def fit(self, X, y=None):        \n",
    "        for col in X.columns:\n",
    "            # Recupera il numero massimo di features per questa colonna, se definito\n",
    "            max_features = self.max_features_per_column.get(col, None)  # Imposta max_features se definito\n",
    "            \n",
    "            # Prendiamo i dati della colonna e applichiamo il TfidfVectorizer\n",
    "            col_data = X[col].fillna('').astype(str)\n",
    "            vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "            self.tfidf_column_vectorizers[col] = vectorizer\n",
    "            self.tfidf_column_vectorizers[col].fit(col_data)\n",
    "\n",
    "            # Salva il numero di features effettivo per questa colonna\n",
    "            num_features_col = len(vectorizer.get_feature_names_out())\n",
    "            if col in self.column_feature_counts:\n",
    "                self.column_feature_counts[col].append(num_features_col)\n",
    "            else:\n",
    "                self.column_feature_counts[col] = [num_features_col]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        transformed_columns = []\n",
    "        \n",
    "        # Per ogni colonna, trasformiamo i dati usando il rispettivo TfidfVectorizer\n",
    "        for col in X.columns:\n",
    "            col_data = X[col].fillna('').astype(str)\n",
    "            transformed_columns.append(self.tfidf_column_vectorizers[col].transform(col_data))\n",
    "        \n",
    "        # Combiniamo le trasformazioni di tutte le colonne\n",
    "        X_combined = hstack(transformed_columns)  # Uniamo le matrici sparse in una sola\n",
    "\n",
    "        return X_combined\n",
    "    \n",
    "    def mean_features_per_column(self):\n",
    "        \"\"\"\n",
    "        Calcola e restituisce il numero medio di features per colonna.\n",
    "        \"\"\"\n",
    "        mean_features = {col: sum(counts) / len(counts) for col, counts in self.column_feature_counts.items()}\n",
    "        return mean_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### CLASSIFICATION CON MAX FEATURES ##############################\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Lista delle colonne preprocessate\n",
    "colonne_preprocessate = [f\"{colonna}_preprocessed\" for colonna in colonne_da_preprocessare]\n",
    "\n",
    "# Definisci il dizionario max_features per ogni colonna\n",
    "max_features_dict = {\n",
    "    'testo_preprocessed': 5000,  \n",
    "    'motivo_ricovero_preprocessed': 5000,   \n",
    "    'anamnesi_preprocessed': 5000,\n",
    "    'esameobiettivo_preprocessed': 5000,\n",
    "    'terapiafarmaingresso_preprocessed': 5000,\n",
    "    'decorso_preprocessed': 5000,\n",
    "    'laboratorio_preprocessed': 5000,\n",
    "    'interventi_preprocessed': 5000,\n",
    "    'followup_preprocessed': 5000,\n",
    "    'terapie2_preprocessed': 5000,\n",
    "    'terapie3_preprocessed': 5000,\n",
    "    'esami_preprocessed': 5000,\n",
    "}\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "X = merged_dataset[colonne_preprocessate]\n",
    "y = merged_dataset['positivi']\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "    \n",
    "    # Crea la pipeline con il CustomTfidfTransformer\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', CustomTfidfTransformer(max_features_per_column=max_features_dict)),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classificazione', clf)  # Sostituisci con il tuo classificatore\n",
    "    ])\n",
    "\n",
    "    # Adattiamo il CustomTfidfTransformer per accedere alle features per ogni colonna\n",
    "    pipeline.named_steps['tfidf'].fit(X)\n",
    "\n",
    "    # Stampa il numero di features per ogni colonna\n",
    "    print(\"Numero di features per colonna:\")\n",
    "    for col, vectorizer in pipeline.named_steps['tfidf'].tfidf_column_vectorizers.items():\n",
    "        num_features_col = len(vectorizer.get_feature_names_out())\n",
    "        print(f\" - {col}: {num_features_col} features\")\n",
    "\n",
    "    # Trasforma i dati e ottieni la matrice TF-IDF combinata\n",
    "    X_tfidf = pipeline.named_steps['tfidf'].transform(X)\n",
    "    num_samples, num_features = X_tfidf.shape\n",
    "    #print(f\"Dimensione della matrice TF-IDF combinata: {num_samples} campioni, {num_features} features\")\n",
    "\n",
    "    # Calcola le metriche usando la funzione\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Stampa il numero medio di features per colonna\n",
    "    mean_features = pipeline.named_steps['tfidf'].mean_features_per_column()\n",
    "    print(\"\\nNumero medio di features per ciascuna colonna:\")\n",
    "    for col, mean_count in mean_features.items():\n",
    "        print(f\"Colonna '{col}': {mean_count:.2f} features\")\n",
    "    \n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esegui la cross-validation per ogni classificatore\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "    \n",
    "    # Crea la pipeline con il CustomTfidfTransformer\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', CustomTfidfTransformer(max_features_per_column=max_features_dict)),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    # Adatta la pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Ottieni i coefficienti dal classificatore\n",
    "    if hasattr(clf, 'coef_'):  # Verifica se il classificatore ha il parametro 'coef_'\n",
    "        coef = clf.coef_.flatten()  # Estrai i coefficienti come array\n",
    "    else:\n",
    "        print(f\"Il classificatore {clf_name} non supporta i coefficienti.\")\n",
    "        continue\n",
    "\n",
    "    # Ottieni i nomi delle feature da CustomTfidfTransformer\n",
    "    feature_names = []\n",
    "    for col, vectorizer in pipeline.named_steps['tfidf'].tfidf_column_vectorizers.items():\n",
    "        column_feature_names = vectorizer.get_feature_names_out()\n",
    "        feature_names.extend([f\"{col}::{feature}\" for feature in column_feature_names])\n",
    "\n",
    "    # Associa i coefficienti alle feature\n",
    "    feature_coef = list(zip(feature_names, coef))\n",
    "\n",
    "    # Ordina le feature in base ai coefficienti (decrescente)\n",
    "    top_features = sorted(feature_coef, key=lambda x: abs(x[1]), reverse=True)[:20]\n",
    "\n",
    "    # Stampa le prime 20 features con la colonna di origine\n",
    "    print(\"\\nTop 20 features (con valori di coef più alti):\")\n",
    "    for feature, coef_value in top_features:\n",
    "        col_name, feature_name = feature.split(\"::\")\n",
    "        print(f\"Feature: {feature_name} (Colonna: {col_name}) - Coefficiente: {coef_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Inizializza una lista per tenere traccia delle importanze delle colonne\n",
    "column_importances = {}\n",
    "\n",
    "# Calcola l'importanza cumulativa per ciascuna colonna (escludendo 'reparto')\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "    \n",
    "    # Crea la pipeline con il CustomTfidfTransformer\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', CustomTfidfTransformer(max_features_per_column=max_features_dict)),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classificazione', clf)\n",
    "    ])\n",
    "\n",
    "    # Adatta la pipeline\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Ottieni i coefficienti dal classificatore\n",
    "    if hasattr(clf, 'coef_'):  # Verifica se il classificatore ha il parametro 'coef_'\n",
    "        coef = clf.coef_.flatten()  # Estrai i coefficienti come array\n",
    "    else:\n",
    "        print(f\"Il classificatore {clf_name} non supporta i coefficienti.\")\n",
    "        continue\n",
    "\n",
    "    # Ottieni i nomi delle feature da CustomTfidfTransformer\n",
    "    feature_names = []\n",
    "    for col, vectorizer in pipeline.named_steps['tfidf'].tfidf_column_vectorizers.items():\n",
    "        column_feature_names = vectorizer.get_feature_names_out()\n",
    "        feature_names.extend([f\"{col}::{feature}\" for feature in column_feature_names])\n",
    "\n",
    "    # Associa i coefficienti alle feature\n",
    "    feature_coef = list(zip(feature_names, coef))\n",
    "\n",
    "    # Somma i coefficienti per ciascuna colonna (escludendo 'reparto')\n",
    "    for col, vectorizer in pipeline.named_steps['tfidf'].tfidf_column_vectorizers.items():\n",
    "        # Escludi la colonna 'reparto'\n",
    "        if col == 'reparto_preprocessed':  # Assicurati che il nome della colonna sia corretto\n",
    "            continue\n",
    "        \n",
    "        # Trova tutte le feature che appartengono alla colonna 'col'\n",
    "        column_features = [f\"{col}::{feature}\" for feature in vectorizer.get_feature_names_out()]\n",
    "        column_coef = [coef[i] for i, feature in enumerate(feature_names) if feature in column_features]\n",
    "        \n",
    "        # Somma i coefficienti per questa colonna\n",
    "        column_importances[col] = np.sum(column_coef)\n",
    "\n",
    "# Stampa l'importanza cumulativa di ciascuna colonna (escludendo 'reparto')\n",
    "print(\"\\nImportanza cumulativa per ciascuna colonna (esclusa 'reparto'):\")\n",
    "for col, importance in column_importances.items():\n",
    "    print(f\"{col}: {importance:.4f}\")\n",
    "\n",
    "# Crea il bar plot (escludendo 'reparto')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(column_importances.keys(), column_importances.values(), color='skyblue')\n",
    "plt.xlabel('Colonne')\n",
    "plt.ylabel('Importanza cumulativa dei coefficienti')\n",
    "plt.title('Importanza cumulativa per ogni colonna (esclusa \"reparto\")')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## SMOTE + UNDERSAMPLING CON MAX FEATURES ##########################\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Lista delle colonne preprocessate\n",
    "colonne_preprocessate = [f\"{colonna}_preprocessed\" for colonna in colonne_da_preprocessare]\n",
    "\n",
    "# Definisci il dizionario max_features per ogni colonna\n",
    "max_features_dict = {\n",
    "    'testo_preprocessed': 5000,  \n",
    "    'motivo_ricovero_preprocessed': 5000,   \n",
    "    'anamnesi_preprocessed': 5000,\n",
    "    'esameobiettivo_preprocessed': 5000,\n",
    "    'terapiafarmaingresso_preprocessed': 5000,\n",
    "    'decorso_preprocessed': 5000,\n",
    "    'laboratorio_preprocessed': 5000,\n",
    "    'interventi_preprocessed': 5000,\n",
    "    'followup_preprocessed': 5000,\n",
    "    'terapie2_preprocessed': 5000,\n",
    "    'terapie3_preprocessed': 5000,\n",
    "    'esami_preprocessed': 5000,\n",
    "}\n",
    "\n",
    "# Esegui la cross-validation per ogni classificatore\n",
    "X = merged_dataset[colonne_preprocessate]\n",
    "y = merged_dataset['positivi']\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    print(f\"\\n### Modello: {clf_name} ###\")\n",
    "    \n",
    "    # Crea la pipeline con il CustomTfidfTransformer\n",
    "    pipeline = Pipeline([\n",
    "    ('tfidf', CustomTfidfTransformer(max_features_per_column=max_features_dict)),\n",
    "    ('smote', SMOTE(random_state=42, sampling_strategy=0.6)),  # Applica SMOTE fino al 60% della classe maggioritaria\n",
    "    ('undersample', RandomUnderSampler(sampling_strategy=1.0, random_state=42)),\n",
    "    ('classificazione', clf)  # Sostituisci con il tuo classificatore\n",
    "])\n",
    "\n",
    "    # Trasforma i dati e ottieni la matrice TF-IDF combinata\n",
    "    X_tfidf = pipeline.named_steps['tfidf'].fit_transform(X)\n",
    "    num_samples, num_features = X_tfidf.shape\n",
    "    print(f\"Dimensione della matrice TF-IDF combinata: {num_samples} campioni, {num_features} features\")\n",
    " \n",
    "    # Calcola le metriche usando la funzione\n",
    "    df_avg, auc_avg, f1_avg_per_class, num_features_avg = eval_cross_validation(pipeline, X, y, skf)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    print(\"Classification report mediato:\")\n",
    "    print(df_avg)\n",
    "    print(f\"AUC medio: {auc_avg:.4f}\")\n",
    "    print(f\"Numero medio di features: {num_features_avg:.2f}\")\n",
    "\n",
    "    # Stampa il numero medio di features per colonna\n",
    "    mean_features = pipeline.named_steps['tfidf'].mean_features_per_column()\n",
    "    print(\"\\nNumero medio di features per ciascuna colonna:\")\n",
    "    for col, mean_count in mean_features.items():\n",
    "        print(f\"Colonna '{col}': {mean_count:.2f} features\")\n",
    "    \n",
    "    # Previsioni su tutto il dataset usando cross_val_predict\n",
    "    y_pred = cross_val_predict(pipeline, X, y, cv=skf)\n",
    "\n",
    "    # Visualizza la matrice di confusione\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.title(f\"Matrice di Confusione - {clf_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### grid search per i parametri di mpl ####################\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Definisci i parametri da cercare\n",
    "param_grid = {\n",
    "    'classificazione__hidden_layer_sizes': [(50,), (100,), (100, 100)],\n",
    "    'classificazione__activation': ['relu', 'tanh'],\n",
    "    'classificazione__learning_rate_init': [0.001, 0.01],\n",
    "    'classificazione__alpha': [0.0001, 0.001],\n",
    "    'classificazione__max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "# Crea la pipeline con il CustomTfidfTransformer\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', CustomTfidfTransformer(max_features_per_column=max_features_dict)),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classificazione', MLPClassifier(solver='adam', random_state=42))  # Inizializza MLPClassifier\n",
    "])\n",
    "\n",
    "# Esegui GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)  # 5-fold cross-validation\n",
    "\n",
    "# Esegui la ricerca dei parametri\n",
    "grid_search.fit(X, y)  # Usa X (dati) e y (etichette) da merged_dataset\n",
    "\n",
    "# Stampa i migliori parametri\n",
    "print(f\"I migliori parametri: {grid_search.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
